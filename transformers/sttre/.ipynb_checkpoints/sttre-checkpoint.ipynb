{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc8c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "with open(\"../../_ynyt/prediction/config.env\") as f:\n",
    "    for line in f.readlines():\n",
    "        if len(line) > 2:\n",
    "            k, v = line[:-2].split('=')\n",
    "            os.environ[k] = v\n",
    "    \n",
    "from ynyt.data import Preprocessor\n",
    "from ynyt.features import BaseFeatures, FeatureCombiner\n",
    "from ynyt.utils import json_read\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43eb343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"zero_hour\": [\"2019-01-01 00:00:00\", \"%Y-%m-%d %H:%M:%S\"],\n",
    "    \"base_features\": {\n",
    "        \"rolling\":[\n",
    "            [\n",
    "                \"y\",\n",
    "                {\n",
    "                    \"ar_d\": 1, 'ar_D': 15,\n",
    "                }\n",
    "            ]\n",
    "        ],\n",
    "        \"target\": [\"target\", \"y\", 6],\n",
    "        \"time_based\": {\"features_on\": {\"weekday\": True, \"weekday_plus\": False, \"weekhours\": False}}\n",
    "        },\n",
    "    'harmonics': {'K': {'week': 6, 'year': 0, 'day': 6}},\n",
    "    \"add_features\": [\"distance\", \"duration\", \"passengers\", \"cost\", \"tips\", \"vendor\", \"dol\"], \n",
    "    \"normalizer\": {\"path\": \"../artifacts/transformers\", \n",
    "                   \"to_normalize\": [\"h\", \"distance\", \"duration\", \"passengers\", \"cost\", \"tips\", \"vendor\", \"velocity\"],\n",
    "                   \"target\": \"y\"},\n",
    "    \"combinations\": [],\n",
    "    \"D_combinations\": []\n",
    "}\n",
    "\n",
    "with open('configs/config.json', 'w') as f:\n",
    "    json.dump(config, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1509b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_preprocessed = './../_ynyt/data/preprocessed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31d8bcf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b46bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 714, 55)\n",
      "torch.Size([39, 4, 55, 1]) torch.Size([1, 660]) torch.Size([6, 55])\n"
     ]
    }
   ],
   "source": [
    "period = [datetime.datetime(2022, 4, 17), datetime.datetime(2022, 5, 31)]\n",
    "setting = {'features': True, 'D': False, 'hours': True, 'weekday': True}\n",
    "dataset = YNYT(period, mode='train', path_preprocessed=path_preprocessed, setting=setting)\n",
    "x, r, y = dataset[10]\n",
    "\n",
    "print(dataset.X.shape)\n",
    "print(x.shape, r.shape, y.shape)\n",
    "\n",
    "assert len(dataset) == dataset.bf.data.t.max() + 1 - dataset.horizon - dataset.seq_len + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6579192",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m x, y \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m337\u001b[39m]:\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# test\n",
    "x, y = dataset[0]\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "for k in [0, 10, 337]:\n",
    "    test = dataset.bf.data[dataset.bf.data.t==dataset.seq_len+k-1]\n",
    "    x, y = dataset[k]\n",
    "    assert (test.y_1.values - x[:55, -1]).sum() == 0\n",
    "    assert (test.loc[:, dataset.bf.config['add_features'][0]].values - x[55:110, -1]).sum() == 0\n",
    "    assert (test.loc[:, dataset.bf.feature_groups['target']].values - y).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c244a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.normalize_back(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c83a8b9",
   "metadata": {},
   "source": [
    "<img src=\"../transformers/seerte.jpeg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8e40d7",
   "metadata": {},
   "source": [
    "## Dimensions\n",
    "### original paper:\n",
    "\n",
    "**temporal embedding**\n",
    "\n",
    "element embedding: $X \\in \\mathbb{R}^{b\\times m\\times l} \\to X^e \\in \\mathbb{R}^{b\\times ml \\times d}$, where $b$ - batch size, $l$ - length on the sequence, $m$ - number of zones, $d$ embedding dim\n",
    "\n",
    "positional embedding: $p \\in \\mathbb{R}^l \\to P \\in \\mathbb{R}^{b \\times ml \\times d}$\n",
    "\n",
    "$X^t = X^{e_t} + P$\n",
    "\n",
    "**spatio embedding**\n",
    "\n",
    "var embedding: $s \\in \\mathbb{R}^m \\to S \\in \\mathbb{R}^{b \\times ml \\times d}$\n",
    "\n",
    "$X^s = X^{e_s} + S$\n",
    "\n",
    "**spatio-temporal embedding**\n",
    "\n",
    "$X^{st} = X^{e_{st}} + P$\n",
    "\n",
    "\n",
    "### changes:\n",
    "\n",
    "element embedding: $X \\in \\mathbb{R}^{b\\times mq\\times l} \\to X^e \\in \\mathbb{R}^{b\\times ml \\times d}$, where $q$ - number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c26f866",
   "metadata": {},
   "source": [
    "## Self-attentions\n",
    "<img src=\"../transformers/heads.jpeg\"/>\n",
    "\n",
    "**temporal encoder**: $m$ heads\n",
    "\n",
    "**spatio encoder**: $l$ heads\n",
    "\n",
    "**spatio-temporal encoder**: $h$ heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ecff95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "from torchmetrics import MeanAbsolutePercentageError, MeanAbsoluteError, R2Score\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partialmethod\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from sttre import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f45560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_shape,\n",
    "                 embed_size, num_layers, forward_expansion, heads, device, dropout, regression_head, horizon=6):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.batch_size, self.num_features, self.seq_len, self.num_var, _ = input_shape\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.element_embedding_temporal = nn.Linear(self.seq_len*self.num_features, embed_size*self.seq_len)\n",
    "        self.element_embedding_spatial = nn.Linear(self.num_var*self.num_features, embed_size*self.num_var)\n",
    "        \n",
    "        self.pos_embedding = nn.Embedding(self.seq_len, embed_size)\n",
    "        self.variable_embedding = nn.Embedding(self.num_var, embed_size)\n",
    "\n",
    "        self.temporal = Encoder(seq_len=self.seq_len,\n",
    "                                embed_size=embed_size,\n",
    "                                num_layers=num_layers,\n",
    "                                heads=self.num_var,\n",
    "                                device=self.device,\n",
    "                                forward_expansion=forward_expansion,\n",
    "                                module='temporal',\n",
    "                                rel_emb=True)\n",
    "\n",
    "        self.spatial = Encoder(seq_len=self.num_var,\n",
    "                               embed_size=embed_size,\n",
    "                               num_layers=num_layers,\n",
    "                               heads=self.seq_len,\n",
    "                               device=self.device,\n",
    "                               forward_expansion=forward_expansion,\n",
    "                               module = 'spatial',\n",
    "                               rel_emb=True)\n",
    "\n",
    "        self.spatiotemporal = Encoder(seq_len=self.seq_len*self.num_var,\n",
    "                                      embed_size=embed_size,\n",
    "                                      num_layers=num_layers,\n",
    "                                      heads=heads,\n",
    "                                      device=self.device,\n",
    "                                      forward_expansion=forward_expansion,\n",
    "                                      module = 'spatiotemporal',\n",
    "                                      rel_emb=True)\n",
    "        \n",
    "        factor = regression_head['flatt_factor']\n",
    "        \n",
    "        self.flatter = nn.Sequential(nn.Linear(embed_size, embed_size // factor),\n",
    "                                     nn.LeakyReLU(),\n",
    "                                     nn.Dropout(dropout),\n",
    "                                     nn.Flatten()\n",
    "                                    )\n",
    "\n",
    "        # additional features\n",
    "        self.add_dim = horizon * 55 * regression_head['add_features']\n",
    "        d_out = (embed_size // factor) * self.seq_len * 3 * self.num_var + self.add_dim\n",
    "        \n",
    "        if regression_head['heads'] == 1:\n",
    "            output_size = self.num_var * horizon\n",
    "            self.head = []\n",
    "            for i, l in enumerate(regression_head['layers']):\n",
    "                l1 = self.num_var * horizon * l\n",
    "                if i == 0:\n",
    "                    self.head.append(nn.BatchNorm1d(d_out))\n",
    "                    self.head.append(nn.Linear(d_out, l1))\n",
    "                else:\n",
    "                    l0 = self.num_var * horizon * regression_head['layers'][i - 1]\n",
    "                    self.head.append(nn.BatchNorm1d(l0))\n",
    "                    self.head.append(nn.Linear(l0, l1))\n",
    "                self.head.append(nn.LeakyReLU())\n",
    "                self.head.append(nn.Dropout(regression_head['dropout_head']))\n",
    "            self.head.append(nn.BatchNorm1d(l1))\n",
    "            self.head.append(nn.Linear(l1, self.num_var * horizon))\n",
    "            self.head = nn.Sequential(*self.head)\n",
    "        \n",
    "    def forward(self, x, regressors, dropout):\n",
    "        batch_size = len(x)\n",
    "\n",
    "        #process/embed input for spatio-temporal module\n",
    "        positions = torch.arange(0, self.seq_len\n",
    "                                ).expand(batch_size, self.num_var, self.seq_len\n",
    "                                        ).reshape(batch_size, self.num_var * self.seq_len\n",
    "                                                 ).to(self.device)\n",
    "        x_spatio_temporal = x.reshape(batch_size, self.num_var, self.seq_len*self.num_features)\n",
    "        x_spatio_temporal = self.element_embedding_temporal(x_spatio_temporal\n",
    "                                    ).reshape(batch_size, self.num_var * self.seq_len, self.embed_size)\n",
    "        x_spatio_temporal = F.dropout(self.pos_embedding(positions) + x_spatio_temporal, dropout)\n",
    "        x_spatio_temporal = torch.unsqueeze(x_spatio_temporal, -1)\n",
    "\n",
    "        #process/embed input for temporal module\n",
    "        positions = torch.arange(0, self.seq_len\n",
    "                                ).expand(batch_size, self.num_var, self.seq_len\n",
    "                                        ).reshape(batch_size, self.num_var * self.seq_len\n",
    "                                                 ).to(self.device)\n",
    "        \n",
    "        x_temporal = x.view(batch_size, self.num_var, self.seq_len*self.num_features)\n",
    "        x_temporal = self.element_embedding_temporal(x_temporal\n",
    "                                    ).reshape(batch_size, self.num_var * self.seq_len, self.embed_size)\n",
    "        x_temporal = F.dropout(self.pos_embedding(positions) + x_temporal, dropout)\n",
    "        x_temporal = torch.unsqueeze(x_temporal, -1)\n",
    "        \n",
    "        #process/embed input for spatial module\n",
    "        vars = torch.arange(0, self.num_var).expand(batch_size, self.seq_len, self.num_var).reshape(batch_size, self.num_var*self.seq_len).to(self.device)\n",
    "        \n",
    "        x_spatial = x.view(batch_size, self.seq_len, self.num_features*self.num_var)\n",
    "        x_spatial = self.element_embedding_spatial(x_spatial).reshape(batch_size, self.num_var * self.seq_len, self.embed_size)\n",
    "        x_spatial = F.dropout(self.variable_embedding(vars) + x_spatial, dropout)\n",
    "        x_spatial = torch.unsqueeze(x_spatial, -1)\n",
    "        \n",
    "        out1 = self.temporal(x_temporal)\n",
    "        out2 = self.spatial(x_spatial)\n",
    "        out3 = self.spatiotemporal(x_spatio_temporal)\n",
    "        \n",
    "        out = torch.cat((out1, out2, out3), 1)\n",
    "        \n",
    "        out = self.flatter(out)\n",
    "\n",
    "        if self.add_dim > 0:\n",
    "            out = torch.cat([out.unsqueeze(1), regressors.squeeze(-1)], dim=2).squeeze(1)\n",
    "\n",
    "        out = self.head(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6892b986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d113714",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msttre\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_val\n\u001b[1;32m      3\u001b[0m regression_head \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheads\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_head\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayers\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m8\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_features\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflatt_factor\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m}\n\u001b[1;32m      4\u001b[0m data_setting \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhours\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweekday\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n",
      "File \u001b[0;32m~/andrei/ynyt/ynyt/sttre/sttre.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partialmethod\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YNYT\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSelfAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_size, heads, seq_len, module, rel_emb, device, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from sttre.sttre import train_val\n",
    "\n",
    "regression_head = {'heads': 1, 'dropout_head': 0.1, 'layers': [16, 8], 'add_features': 2, 'flatt_factor': 2}\n",
    "data_setting = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "params = {'embed_size': 64, 'heads': 4, 'num_layers': 2, 'dropout': 0.1, \n",
    "          'forward_expansion': 1, 'lr': 0.0005, 'batch_size': 128, 'seq_len': 4, 'epoches': 100,\n",
    "          'device': 'cpu',\n",
    "         }\n",
    "\n",
    "\n",
    "model = train_val(period=period, \n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50a96a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu!\n",
      "Transformer(\n",
      "  (element_embedding_temporal): Linear(in_features=196, out_features=256, bias=True)\n",
      "  (element_embedding_spatial): Linear(in_features=2695, out_features=3520, bias=True)\n",
      "  (pos_embedding): Embedding(4, 64)\n",
      "  (variable_embedding): Embedding(55, 64)\n",
      "  (temporal): Encoder(\n",
      "    (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (keys): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (queries): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatial): Encoder(\n",
      "    (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (keys): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (queries): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatiotemporal): Encoder(\n",
      "    (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=True)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=True)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=True)\n",
      "          (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatter): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.15, inplace=False)\n",
      "    (3): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): BatchNorm1d(21780, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): Linear(in_features=21780, out_features=5280, bias=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): BatchNorm1d(5280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): Linear(in_features=5280, out_features=2640, bias=True)\n",
      "    (6): LeakyReLU(negative_slope=0.01)\n",
      "    (7): Dropout(p=0.1, inplace=False)\n",
      "    (8): BatchNorm1d(2640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): Linear(in_features=2640, out_features=330, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch: 0, train loss: 0.085253, val loss: 0.047413, val mae: 0.156575, val r2: -0.188158\n",
      "epoch: 1, train loss: 0.044185, val loss: 0.039590, val mae: 0.137589, val r2: 0.004853\n",
      "epoch: 2, train loss: 0.035948, val loss: 0.033579, val mae: 0.124892, val r2: 0.155694\n",
      "epoch: 3, train loss: 0.030342, val loss: 0.029085, val mae: 0.115131, val r2: 0.269343\n",
      "epoch: 4, train loss: 0.026479, val loss: 0.025785, val mae: 0.108885, val r2: 0.353552\n",
      "epoch: 5, train loss: 0.023312, val loss: 0.022656, val mae: 0.104628, val r2: 0.430008\n",
      "epoch: 6, train loss: 0.020787, val loss: 0.019801, val mae: 0.099007, val r2: 0.503224\n",
      "epoch: 7, train loss: 0.019059, val loss: 0.018525, val mae: 0.096796, val r2: 0.536434\n",
      "epoch: 8, train loss: 0.017571, val loss: 0.017439, val mae: 0.095550, val r2: 0.562435\n",
      "epoch: 9, train loss: 0.016458, val loss: 0.016697, val mae: 0.092147, val r2: 0.580728\n",
      "epoch: 10, train loss: 0.015403, val loss: 0.016450, val mae: 0.090830, val r2: 0.586214\n",
      "epoch: 11, train loss: 0.014975, val loss: 0.014873, val mae: 0.087256, val r2: 0.626925\n",
      "epoch: 12, train loss: 0.014188, val loss: 0.013975, val mae: 0.084974, val r2: 0.650611\n",
      "epoch: 13, train loss: 0.013785, val loss: 0.013415, val mae: 0.084021, val r2: 0.663409\n",
      "epoch: 14, train loss: 0.013459, val loss: 0.013650, val mae: 0.083156, val r2: 0.656954\n",
      "epoch: 15, train loss: 0.013208, val loss: 0.013257, val mae: 0.081727, val r2: 0.667287\n",
      "epoch: 16, train loss: 0.012959, val loss: 0.011837, val mae: 0.079264, val r2: 0.705204\n",
      "epoch: 17, train loss: 0.012814, val loss: 0.011732, val mae: 0.078535, val r2: 0.707179\n",
      "epoch: 18, train loss: 0.012721, val loss: 0.012096, val mae: 0.079154, val r2: 0.696981\n",
      "epoch: 19, train loss: 0.012601, val loss: 0.013149, val mae: 0.085044, val r2: 0.670809\n",
      "epoch: 20, train loss: 0.012640, val loss: 0.013630, val mae: 0.082236, val r2: 0.658741\n",
      "epoch: 21, train loss: 0.012238, val loss: 0.014768, val mae: 0.083173, val r2: 0.631459\n",
      "epoch: 22, train loss: 0.012203, val loss: 0.014054, val mae: 0.083590, val r2: 0.650427\n",
      "epoch: 23, train loss: 0.012066, val loss: 0.011469, val mae: 0.079382, val r2: 0.714512\n",
      "epoch: 24, train loss: 0.012034, val loss: 0.012123, val mae: 0.081447, val r2: 0.698278\n",
      "epoch: 25, train loss: 0.012051, val loss: 0.011921, val mae: 0.080637, val r2: 0.703230\n",
      "epoch: 26, train loss: 0.012434, val loss: 0.012530, val mae: 0.082164, val r2: 0.690502\n",
      "epoch: 27, train loss: 0.012479, val loss: 0.012814, val mae: 0.081502, val r2: 0.683345\n",
      "epoch: 28, train loss: 0.012651, val loss: 0.012886, val mae: 0.084031, val r2: 0.679907\n",
      "epoch: 29, train loss: 0.012426, val loss: 0.013285, val mae: 0.084967, val r2: 0.668655\n",
      "epoch: 30, train loss: 0.013123, val loss: 0.012474, val mae: 0.083212, val r2: 0.691239\n",
      "epoch: 31, train loss: 0.013333, val loss: 0.012262, val mae: 0.081925, val r2: 0.696581\n",
      "epoch: 32, train loss: 0.012848, val loss: 0.011231, val mae: 0.079805, val r2: 0.722112\n",
      "epoch: 33, train loss: 0.011645, val loss: 0.011737, val mae: 0.082078, val r2: 0.709873\n",
      "epoch: 34, train loss: 0.012490, val loss: 0.013024, val mae: 0.086938, val r2: 0.677594\n",
      "epoch: 35, train loss: 0.011616, val loss: 0.011310, val mae: 0.081074, val r2: 0.720278\n",
      "epoch: 36, train loss: 0.011871, val loss: 0.014025, val mae: 0.088605, val r2: 0.651685\n",
      "epoch: 37, train loss: 0.012057, val loss: 0.013318, val mae: 0.087069, val r2: 0.669114\n",
      "epoch: 38, train loss: 0.012275, val loss: 0.014731, val mae: 0.092083, val r2: 0.635595\n",
      "epoch: 39, train loss: 0.011740, val loss: 0.013141, val mae: 0.087226, val r2: 0.674898\n",
      "epoch: 40, train loss: 0.011286, val loss: 0.012761, val mae: 0.085175, val r2: 0.685007\n",
      "epoch: 41, train loss: 0.011198, val loss: 0.012225, val mae: 0.084196, val r2: 0.695971\n",
      "epoch: 42, train loss: 0.010994, val loss: 0.010923, val mae: 0.079273, val r2: 0.728844\n",
      "epoch: 43, train loss: 0.011419, val loss: 0.010929, val mae: 0.079024, val r2: 0.729053\n",
      "epoch: 44, train loss: 0.011085, val loss: 0.011777, val mae: 0.083230, val r2: 0.710702\n",
      "epoch: 45, train loss: 0.010345, val loss: 0.011646, val mae: 0.082264, val r2: 0.714790\n",
      "epoch: 46, train loss: 0.010364, val loss: 0.012173, val mae: 0.084400, val r2: 0.702237\n",
      "epoch: 47, train loss: 0.010180, val loss: 0.011873, val mae: 0.083869, val r2: 0.709713\n",
      "epoch: 48, train loss: 0.010071, val loss: 0.011792, val mae: 0.083542, val r2: 0.711615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49, train loss: 0.010059, val loss: 0.011535, val mae: 0.082538, val r2: 0.716899\n",
      "epoch: 50, train loss: 0.009742, val loss: 0.010453, val mae: 0.077748, val r2: 0.741511\n",
      "epoch: 51, train loss: 0.009753, val loss: 0.010351, val mae: 0.076525, val r2: 0.744086\n",
      "epoch: 52, train loss: 0.010197, val loss: 0.010851, val mae: 0.078961, val r2: 0.732728\n",
      "epoch: 53, train loss: 0.009908, val loss: 0.010725, val mae: 0.078920, val r2: 0.735369\n",
      "epoch: 54, train loss: 0.010133, val loss: 0.011195, val mae: 0.080099, val r2: 0.724843\n",
      "epoch: 55, train loss: 0.010111, val loss: 0.010936, val mae: 0.079295, val r2: 0.729916\n",
      "epoch: 56, train loss: 0.010458, val loss: 0.011487, val mae: 0.081481, val r2: 0.715116\n",
      "epoch: 57, train loss: 0.010113, val loss: 0.010858, val mae: 0.078875, val r2: 0.730412\n",
      "epoch: 58, train loss: 0.010490, val loss: 0.010890, val mae: 0.079062, val r2: 0.730974\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0005\u001b[39m\n\u001b[1;32m     14\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m---> 16\u001b[0m model_3 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_expansion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregression_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msetting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhorizon\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 55\u001b[0m, in \u001b[0;36mtrain_val\u001b[0;34m(embed_size, heads, num_layers, dropout, forward_expansion, lr, batch_size, seq_len, regression_head, setting, verbose, verbose_step, horizon)\u001b[0m\n\u001b[1;32m     53\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(labels, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 55\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregressors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, labels\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     57\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[45], line 110\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, regressors, dropout)\u001b[0m\n\u001b[1;32m    108\u001b[0m out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal(x_temporal)\n\u001b[1;32m    109\u001b[0m out2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial(x_spatial)\n\u001b[0;32m--> 110\u001b[0m out3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspatiotemporal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_spatio_temporal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((out1, out2, out3), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    114\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatter(out)\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/andrei/ynyt/ynyt/sttre/sttre.py:170\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 170\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(out)\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/andrei/ynyt/ynyt/sttre/sttre.py:142\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    141\u001b[0m     z \u001b[38;5;241m=\u001b[39m x[:, :, :, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 142\u001b[0m     attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(attention \u001b[38;5;241m+\u001b[39m z)\n\u001b[1;32m    144\u001b[0m     forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(x)\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/andrei/ynyt/ynyt/sttre/sttre.py:73\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_emb:\n\u001b[0;32m---> 73\u001b[0m     QE \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     QE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_positions(QE)\n\u001b[1;32m     75\u001b[0m     S \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skew(QE)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(N, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "regression_head = {'heads': 1, 'dropout_head': 0.1, 'layers': [16, 8], 'add_features': 2, 'flatt_factor': 2}\n",
    "data_setting = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "params = {'embed_size': 64, 'heads': 4, 'num_layers': 2, 'dropout': 0.1, \n",
    "          'forward_expansion': 1, 'lr': 0.0005, 'batch_size': 128, 'seq_len': 4, 'epoches': 100}\n",
    "\n",
    "    device = torch.device(\"mps\")\n",
    "    device = 'cpu'\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "\n",
    "\n",
    "horizon = 6\n",
    "d = 64\n",
    "h = 4\n",
    "seq_len = 4\n",
    "num_layers = 2\n",
    "forward_expansion = 1\n",
    "dropout = 0.15\n",
    "lr = 0.0005\n",
    "batch_size = 128\n",
    "\n",
    "model = train_val(period, epoches, \n",
    "                  embed_size, heads, num_layers, dropout, forward_expansion, lr, batch_size, seq_len, \n",
    "                  batch_size, seq_len, regression_head, data_setting, device, \n",
    "                  verbose=True, horizon=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c90fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch: 0, train loss: 0.022740, val loss: 0.010971, val mae: 0.078174, val r2: 0.715576\n",
    "epoch: 1, train loss: 0.011180, val loss: 0.017958, val mae: 0.099584, val r2: 0.532309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca05b412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 0.022521, val loss: 0.016710, val mae: 0.102509, val r2: 0.599304\n",
      "epoch: 1, train loss: 0.013898, val loss: 0.014178, val mae: 0.092142, val r2: 0.658461\n",
      "epoch: 2, train loss: 0.010964, val loss: 0.012187, val mae: 0.084001, val r2: 0.706509\n",
      "epoch: 3, train loss: 0.009008, val loss: 0.011310, val mae: 0.080319, val r2: 0.727264\n",
      "epoch: 4, train loss: 0.007942, val loss: 0.010689, val mae: 0.077674, val r2: 0.742635\n",
      "epoch: 5, train loss: 0.007223, val loss: 0.010010, val mae: 0.074908, val r2: 0.759120\n",
      "epoch: 6, train loss: 0.006706, val loss: 0.009710, val mae: 0.073383, val r2: 0.766474\n",
      "epoch: 7, train loss: 0.006305, val loss: 0.009212, val mae: 0.071253, val r2: 0.778462\n",
      "epoch: 8, train loss: 0.005955, val loss: 0.008944, val mae: 0.069936, val r2: 0.785152\n",
      "epoch: 9, train loss: 0.005673, val loss: 0.008569, val mae: 0.068158, val r2: 0.794209\n",
      "epoch: 10, train loss: 0.005436, val loss: 0.008323, val mae: 0.067021, val r2: 0.800280\n",
      "epoch: 11, train loss: 0.005232, val loss: 0.008117, val mae: 0.065954, val r2: 0.805191\n",
      "epoch: 12, train loss: 0.005068, val loss: 0.007988, val mae: 0.065182, val r2: 0.808582\n",
      "epoch: 13, train loss: 0.004902, val loss: 0.007872, val mae: 0.064711, val r2: 0.811282\n",
      "epoch: 14, train loss: 0.004768, val loss: 0.007806, val mae: 0.064273, val r2: 0.812912\n",
      "epoch: 15, train loss: 0.004637, val loss: 0.007502, val mae: 0.062704, val r2: 0.820423\n",
      "epoch: 16, train loss: 0.004556, val loss: 0.007390, val mae: 0.062130, val r2: 0.823032\n",
      "epoch: 17, train loss: 0.004462, val loss: 0.007424, val mae: 0.062291, val r2: 0.822208\n",
      "epoch: 18, train loss: 0.004374, val loss: 0.007416, val mae: 0.062104, val r2: 0.822659\n",
      "epoch: 19, train loss: 0.004290, val loss: 0.007217, val mae: 0.061185, val r2: 0.827384\n",
      "epoch: 20, train loss: 0.004230, val loss: 0.007050, val mae: 0.060369, val r2: 0.831442\n",
      "epoch: 21, train loss: 0.004168, val loss: 0.007104, val mae: 0.060699, val r2: 0.830069\n",
      "epoch: 22, train loss: 0.004108, val loss: 0.007043, val mae: 0.060080, val r2: 0.831705\n",
      "epoch: 23, train loss: 0.004052, val loss: 0.007073, val mae: 0.060143, val r2: 0.831006\n",
      "epoch: 24, train loss: 0.004011, val loss: 0.006878, val mae: 0.059309, val r2: 0.835451\n",
      "epoch: 25, train loss: 0.003955, val loss: 0.006992, val mae: 0.059755, val r2: 0.832814\n",
      "epoch: 26, train loss: 0.003913, val loss: 0.006887, val mae: 0.059220, val r2: 0.835265\n",
      "epoch: 27, train loss: 0.003883, val loss: 0.006954, val mae: 0.059518, val r2: 0.833772\n",
      "epoch: 28, train loss: 0.003837, val loss: 0.006887, val mae: 0.059111, val r2: 0.835381\n",
      "epoch: 29, train loss: 0.003811, val loss: 0.006998, val mae: 0.059703, val r2: 0.833027\n",
      "epoch: 30, train loss: 0.003780, val loss: 0.006948, val mae: 0.059362, val r2: 0.834100\n",
      "epoch: 31, train loss: 0.003741, val loss: 0.006968, val mae: 0.059447, val r2: 0.833558\n",
      "epoch: 32, train loss: 0.003719, val loss: 0.007024, val mae: 0.059681, val r2: 0.832658\n",
      "epoch: 33, train loss: 0.003697, val loss: 0.006983, val mae: 0.059405, val r2: 0.833380\n",
      "epoch: 34, train loss: 0.003669, val loss: 0.006980, val mae: 0.059391, val r2: 0.833647\n",
      "epoch: 35, train loss: 0.003633, val loss: 0.006986, val mae: 0.059419, val r2: 0.833343\n",
      "epoch: 36, train loss: 0.003622, val loss: 0.007243, val mae: 0.060613, val r2: 0.827545\n",
      "epoch: 37, train loss: 0.003593, val loss: 0.007195, val mae: 0.060165, val r2: 0.828754\n",
      "epoch: 38, train loss: 0.003583, val loss: 0.007378, val mae: 0.061155, val r2: 0.824364\n",
      "epoch: 39, train loss: 0.003565, val loss: 0.007753, val mae: 0.062848, val r2: 0.815784\n",
      "epoch: 40, train loss: 0.003556, val loss: 0.007862, val mae: 0.063311, val r2: 0.813136\n",
      "epoch: 41, train loss: 0.003551, val loss: 0.008187, val mae: 0.064550, val r2: 0.805583\n",
      "epoch: 42, train loss: 0.003557, val loss: 0.008679, val mae: 0.066600, val r2: 0.793883\n",
      "epoch: 43, train loss: 0.003592, val loss: 0.009067, val mae: 0.068296, val r2: 0.784493\n",
      "epoch: 44, train loss: 0.003673, val loss: 0.008496, val mae: 0.066034, val r2: 0.797532\n",
      "epoch: 45, train loss: 0.003805, val loss: 0.006468, val mae: 0.056816, val r2: 0.844978\n",
      "epoch: 46, train loss: 0.003872, val loss: 0.005876, val mae: 0.054133, val r2: 0.859095\n",
      "epoch: 47, train loss: 0.003751, val loss: 0.005817, val mae: 0.053746, val r2: 0.860246\n",
      "epoch: 48, train loss: 0.003958, val loss: 0.007332, val mae: 0.061051, val r2: 0.824133\n",
      "epoch: 49, train loss: 0.003894, val loss: 0.007468, val mae: 0.061723, val r2: 0.821475\n",
      "epoch: 50, train loss: 0.003588, val loss: 0.006677, val mae: 0.057855, val r2: 0.840323\n",
      "epoch: 51, train loss: 0.003409, val loss: 0.006297, val mae: 0.055850, val r2: 0.849397\n",
      "epoch: 52, train loss: 0.003369, val loss: 0.006177, val mae: 0.055250, val r2: 0.852165\n",
      "epoch: 53, train loss: 0.003335, val loss: 0.006144, val mae: 0.055063, val r2: 0.853077\n",
      "epoch: 54, train loss: 0.003313, val loss: 0.006079, val mae: 0.054780, val r2: 0.854420\n",
      "epoch: 55, train loss: 0.003295, val loss: 0.006063, val mae: 0.054680, val r2: 0.855051\n",
      "epoch: 56, train loss: 0.003287, val loss: 0.006014, val mae: 0.054564, val r2: 0.856194\n",
      "epoch: 57, train loss: 0.003255, val loss: 0.006039, val mae: 0.054552, val r2: 0.855492\n",
      "epoch: 58, train loss: 0.003240, val loss: 0.006169, val mae: 0.055156, val r2: 0.852462\n",
      "epoch: 59, train loss: 0.003222, val loss: 0.006087, val mae: 0.054706, val r2: 0.854605\n",
      "epoch: 60, train loss: 0.003205, val loss: 0.006177, val mae: 0.055181, val r2: 0.852380\n",
      "epoch: 61, train loss: 0.003193, val loss: 0.006007, val mae: 0.054394, val r2: 0.856321\n",
      "epoch: 62, train loss: 0.003174, val loss: 0.006060, val mae: 0.054650, val r2: 0.854990\n",
      "epoch: 63, train loss: 0.003158, val loss: 0.005990, val mae: 0.054368, val r2: 0.856865\n",
      "epoch: 64, train loss: 0.003147, val loss: 0.005925, val mae: 0.054040, val r2: 0.858511\n",
      "epoch: 65, train loss: 0.003126, val loss: 0.005910, val mae: 0.053892, val r2: 0.858652\n",
      "epoch: 66, train loss: 0.003117, val loss: 0.006082, val mae: 0.054747, val r2: 0.854927\n",
      "epoch: 67, train loss: 0.003104, val loss: 0.006077, val mae: 0.054681, val r2: 0.855096\n",
      "epoch: 68, train loss: 0.003089, val loss: 0.005969, val mae: 0.054128, val r2: 0.857415\n",
      "epoch: 69, train loss: 0.003076, val loss: 0.005903, val mae: 0.053821, val r2: 0.859109\n",
      "epoch: 70, train loss: 0.003058, val loss: 0.005880, val mae: 0.053693, val r2: 0.859545\n",
      "epoch: 71, train loss: 0.003051, val loss: 0.005825, val mae: 0.053349, val r2: 0.860933\n",
      "epoch: 72, train loss: 0.003039, val loss: 0.005751, val mae: 0.052982, val r2: 0.862341\n",
      "epoch: 73, train loss: 0.003031, val loss: 0.005657, val mae: 0.052569, val r2: 0.864783\n",
      "epoch: 74, train loss: 0.003022, val loss: 0.005601, val mae: 0.052280, val r2: 0.866023\n",
      "epoch: 75, train loss: 0.003015, val loss: 0.005578, val mae: 0.052097, val r2: 0.866465\n",
      "epoch: 76, train loss: 0.003007, val loss: 0.005481, val mae: 0.051675, val r2: 0.868754\n",
      "epoch: 77, train loss: 0.003003, val loss: 0.005578, val mae: 0.052071, val r2: 0.866605\n",
      "epoch: 78, train loss: 0.003015, val loss: 0.005571, val mae: 0.052035, val r2: 0.866575\n",
      "epoch: 79, train loss: 0.003028, val loss: 0.005568, val mae: 0.052070, val r2: 0.866700\n",
      "epoch: 80, train loss: 0.003069, val loss: 0.005626, val mae: 0.052395, val r2: 0.865442\n",
      "epoch: 81, train loss: 0.003101, val loss: 0.006350, val mae: 0.056090, val r2: 0.848745\n",
      "epoch: 82, train loss: 0.003155, val loss: 0.007772, val mae: 0.062730, val r2: 0.815820\n",
      "epoch: 83, train loss: 0.003161, val loss: 0.009358, val mae: 0.069595, val r2: 0.779419\n",
      "epoch: 84, train loss: 0.003144, val loss: 0.009287, val mae: 0.069059, val r2: 0.781222\n",
      "epoch: 85, train loss: 0.003250, val loss: 0.007747, val mae: 0.062571, val r2: 0.816538\n",
      "epoch: 86, train loss: 0.003368, val loss: 0.005666, val mae: 0.052726, val r2: 0.864652\n",
      "epoch: 87, train loss: 0.003179, val loss: 0.005374, val mae: 0.051184, val r2: 0.871448\n",
      "epoch: 88, train loss: 0.003072, val loss: 0.005217, val mae: 0.050348, val r2: 0.875263\n",
      "epoch: 89, train loss: 0.002978, val loss: 0.005331, val mae: 0.051042, val r2: 0.872578\n",
      "epoch: 90, train loss: 0.002953, val loss: 0.005551, val mae: 0.052026, val r2: 0.867512\n",
      "epoch: 91, train loss: 0.002935, val loss: 0.005801, val mae: 0.053350, val r2: 0.861814\n",
      "epoch: 92, train loss: 0.002907, val loss: 0.005883, val mae: 0.053762, val r2: 0.860210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 93, train loss: 0.002907, val loss: 0.005766, val mae: 0.053165, val r2: 0.862702\n",
      "epoch: 94, train loss: 0.002899, val loss: 0.005544, val mae: 0.052021, val r2: 0.867944\n",
      "epoch: 95, train loss: 0.002891, val loss: 0.005473, val mae: 0.051588, val r2: 0.869322\n",
      "epoch: 96, train loss: 0.002874, val loss: 0.005394, val mae: 0.051219, val r2: 0.871194\n",
      "epoch: 97, train loss: 0.002888, val loss: 0.005462, val mae: 0.051709, val r2: 0.869722\n",
      "epoch: 98, train loss: 0.002895, val loss: 0.005748, val mae: 0.053049, val r2: 0.862968\n",
      "epoch: 99, train loss: 0.002895, val loss: 0.006060, val mae: 0.054606, val r2: 0.855958\n"
     ]
    }
   ],
   "source": [
    "# **\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "d = 32\n",
    "h = 4\n",
    "seq_len = 4\n",
    "num_layers = 2\n",
    "forward_expansion = 1\n",
    "dropout = 0.1\n",
    "lr = 0.00005\n",
    "batch_size = 128\n",
    "\n",
    "model_2 = train_val(d, h, num_layers, dropout, forward_expansion, lr, batch_size, seq_len, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8eaca5c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 0.019370, val loss: 0.015780, val mae: 0.098585, val r2: 0.623975\n",
      "epoch: 1, train loss: 0.013873, val loss: 0.013926, val mae: 0.090102, val r2: 0.668033\n",
      "epoch: 2, train loss: 0.009853, val loss: 0.012343, val mae: 0.084003, val r2: 0.704577\n",
      "epoch: 3, train loss: 0.007697, val loss: 0.011234, val mae: 0.079773, val r2: 0.731614\n",
      "epoch: 4, train loss: 0.006733, val loss: 0.010664, val mae: 0.077258, val r2: 0.745611\n",
      "epoch: 5, train loss: 0.006147, val loss: 0.009851, val mae: 0.073878, val r2: 0.765137\n",
      "epoch: 6, train loss: 0.005718, val loss: 0.009531, val mae: 0.072521, val r2: 0.772791\n",
      "epoch: 7, train loss: 0.005411, val loss: 0.009172, val mae: 0.070858, val r2: 0.781374\n",
      "epoch: 8, train loss: 0.005157, val loss: 0.008953, val mae: 0.069775, val r2: 0.786838\n",
      "epoch: 9, train loss: 0.004950, val loss: 0.008513, val mae: 0.067860, val r2: 0.797126\n",
      "epoch: 10, train loss: 0.004793, val loss: 0.008561, val mae: 0.067897, val r2: 0.796141\n",
      "epoch: 11, train loss: 0.004647, val loss: 0.008278, val mae: 0.066584, val r2: 0.802698\n",
      "epoch: 12, train loss: 0.004520, val loss: 0.008190, val mae: 0.066209, val r2: 0.805118\n",
      "epoch: 13, train loss: 0.004420, val loss: 0.008241, val mae: 0.066231, val r2: 0.803899\n",
      "epoch: 14, train loss: 0.004332, val loss: 0.008051, val mae: 0.065194, val r2: 0.808544\n",
      "epoch: 15, train loss: 0.004236, val loss: 0.007992, val mae: 0.065050, val r2: 0.809750\n",
      "epoch: 16, train loss: 0.004156, val loss: 0.007965, val mae: 0.064828, val r2: 0.810544\n",
      "epoch: 17, train loss: 0.004094, val loss: 0.008063, val mae: 0.065128, val r2: 0.808304\n",
      "epoch: 18, train loss: 0.004027, val loss: 0.008078, val mae: 0.065122, val r2: 0.807917\n",
      "epoch: 19, train loss: 0.003974, val loss: 0.007764, val mae: 0.063713, val r2: 0.815195\n",
      "epoch: 20, train loss: 0.003915, val loss: 0.007826, val mae: 0.064070, val r2: 0.814025\n",
      "epoch: 21, train loss: 0.003859, val loss: 0.007872, val mae: 0.064086, val r2: 0.812982\n",
      "epoch: 22, train loss: 0.003821, val loss: 0.008040, val mae: 0.064661, val r2: 0.809169\n",
      "epoch: 23, train loss: 0.003770, val loss: 0.007812, val mae: 0.063699, val r2: 0.814637\n",
      "epoch: 24, train loss: 0.003733, val loss: 0.007734, val mae: 0.063320, val r2: 0.816367\n",
      "epoch: 25, train loss: 0.003691, val loss: 0.007914, val mae: 0.064054, val r2: 0.812180\n",
      "epoch: 26, train loss: 0.003661, val loss: 0.007666, val mae: 0.062937, val r2: 0.818084\n",
      "epoch: 27, train loss: 0.003629, val loss: 0.008012, val mae: 0.064478, val r2: 0.809771\n",
      "epoch: 28, train loss: 0.003592, val loss: 0.007775, val mae: 0.063361, val r2: 0.815437\n",
      "epoch: 29, train loss: 0.003556, val loss: 0.007868, val mae: 0.063673, val r2: 0.813348\n",
      "epoch: 30, train loss: 0.003528, val loss: 0.007712, val mae: 0.063091, val r2: 0.816905\n",
      "epoch: 31, train loss: 0.003506, val loss: 0.007891, val mae: 0.063782, val r2: 0.812756\n",
      "epoch: 32, train loss: 0.003478, val loss: 0.007846, val mae: 0.063468, val r2: 0.813881\n",
      "epoch: 33, train loss: 0.003447, val loss: 0.008052, val mae: 0.064254, val r2: 0.808994\n",
      "epoch: 34, train loss: 0.003419, val loss: 0.007947, val mae: 0.063869, val r2: 0.811212\n",
      "epoch: 35, train loss: 0.003406, val loss: 0.008063, val mae: 0.064292, val r2: 0.808835\n",
      "epoch: 36, train loss: 0.003366, val loss: 0.007947, val mae: 0.063916, val r2: 0.811367\n",
      "epoch: 37, train loss: 0.003352, val loss: 0.008235, val mae: 0.065285, val r2: 0.804516\n",
      "epoch: 38, train loss: 0.003337, val loss: 0.008213, val mae: 0.065079, val r2: 0.805377\n",
      "epoch: 39, train loss: 0.003309, val loss: 0.008260, val mae: 0.065241, val r2: 0.803949\n",
      "epoch: 40, train loss: 0.003290, val loss: 0.008513, val mae: 0.066316, val r2: 0.798035\n",
      "epoch: 41, train loss: 0.003277, val loss: 0.008772, val mae: 0.067258, val r2: 0.791987\n",
      "epoch: 42, train loss: 0.003258, val loss: 0.008867, val mae: 0.067694, val r2: 0.789732\n",
      "epoch: 43, train loss: 0.003245, val loss: 0.008829, val mae: 0.067652, val r2: 0.790645\n",
      "epoch: 44, train loss: 0.003234, val loss: 0.008942, val mae: 0.068096, val r2: 0.787769\n",
      "epoch: 45, train loss: 0.003232, val loss: 0.009021, val mae: 0.068563, val r2: 0.785997\n",
      "epoch: 46, train loss: 0.003225, val loss: 0.009224, val mae: 0.069444, val r2: 0.781318\n",
      "epoch: 47, train loss: 0.003233, val loss: 0.009968, val mae: 0.072202, val r2: 0.763895\n",
      "epoch: 48, train loss: 0.003285, val loss: 0.009768, val mae: 0.071465, val r2: 0.768160\n",
      "epoch: 49, train loss: 0.003344, val loss: 0.008704, val mae: 0.067169, val r2: 0.793347\n",
      "epoch: 50, train loss: 0.003416, val loss: 0.007217, val mae: 0.060618, val r2: 0.827508\n",
      "epoch: 51, train loss: 0.003498, val loss: 0.005949, val mae: 0.054539, val r2: 0.857374\n",
      "epoch: 52, train loss: 0.003533, val loss: 0.005558, val mae: 0.053136, val r2: 0.866630\n",
      "epoch: 53, train loss: 0.003464, val loss: 0.005560, val mae: 0.052895, val r2: 0.866511\n",
      "epoch: 54, train loss: 0.003580, val loss: 0.006643, val mae: 0.057697, val r2: 0.841374\n",
      "epoch: 55, train loss: 0.003527, val loss: 0.007555, val mae: 0.062060, val r2: 0.820331\n",
      "epoch: 56, train loss: 0.003224, val loss: 0.006938, val mae: 0.059135, val r2: 0.834815\n",
      "epoch: 57, train loss: 0.003043, val loss: 0.006462, val mae: 0.056761, val r2: 0.845872\n",
      "epoch: 58, train loss: 0.003017, val loss: 0.006159, val mae: 0.055466, val r2: 0.852959\n",
      "epoch: 59, train loss: 0.003001, val loss: 0.006141, val mae: 0.055351, val r2: 0.853432\n",
      "epoch: 60, train loss: 0.002963, val loss: 0.005980, val mae: 0.054486, val r2: 0.856915\n",
      "epoch: 61, train loss: 0.002950, val loss: 0.005971, val mae: 0.054494, val r2: 0.857399\n",
      "epoch: 62, train loss: 0.002933, val loss: 0.005957, val mae: 0.054361, val r2: 0.857717\n",
      "epoch: 63, train loss: 0.002918, val loss: 0.005991, val mae: 0.054612, val r2: 0.856884\n",
      "epoch: 64, train loss: 0.002896, val loss: 0.006049, val mae: 0.054777, val r2: 0.855561\n",
      "epoch: 65, train loss: 0.002876, val loss: 0.006199, val mae: 0.055494, val r2: 0.852243\n",
      "epoch: 66, train loss: 0.002856, val loss: 0.006195, val mae: 0.055432, val r2: 0.852265\n",
      "epoch: 67, train loss: 0.002845, val loss: 0.006338, val mae: 0.056173, val r2: 0.848820\n",
      "epoch: 68, train loss: 0.002827, val loss: 0.006104, val mae: 0.055009, val r2: 0.854408\n",
      "epoch: 69, train loss: 0.002811, val loss: 0.006057, val mae: 0.054830, val r2: 0.855294\n",
      "epoch: 70, train loss: 0.002807, val loss: 0.006126, val mae: 0.055168, val r2: 0.853764\n",
      "epoch: 71, train loss: 0.002795, val loss: 0.006062, val mae: 0.054877, val r2: 0.855381\n",
      "epoch: 72, train loss: 0.002784, val loss: 0.005957, val mae: 0.054262, val r2: 0.857599\n",
      "epoch: 73, train loss: 0.002770, val loss: 0.005824, val mae: 0.053547, val r2: 0.860983\n",
      "epoch: 74, train loss: 0.002762, val loss: 0.005853, val mae: 0.053835, val r2: 0.860083\n",
      "epoch: 75, train loss: 0.002751, val loss: 0.005849, val mae: 0.053789, val r2: 0.860246\n",
      "epoch: 76, train loss: 0.002739, val loss: 0.005807, val mae: 0.053611, val r2: 0.861112\n",
      "epoch: 77, train loss: 0.002732, val loss: 0.005575, val mae: 0.052617, val r2: 0.866511\n",
      "epoch: 78, train loss: 0.002717, val loss: 0.005601, val mae: 0.052501, val r2: 0.866023\n",
      "epoch: 79, train loss: 0.002707, val loss: 0.005573, val mae: 0.052422, val r2: 0.866598\n",
      "epoch: 80, train loss: 0.002700, val loss: 0.005371, val mae: 0.051444, val r2: 0.871190\n",
      "epoch: 81, train loss: 0.002708, val loss: 0.005275, val mae: 0.050927, val r2: 0.873563\n",
      "epoch: 82, train loss: 0.002687, val loss: 0.005333, val mae: 0.051264, val r2: 0.872031\n",
      "epoch: 83, train loss: 0.002692, val loss: 0.005274, val mae: 0.050929, val r2: 0.873513\n",
      "epoch: 84, train loss: 0.002683, val loss: 0.005303, val mae: 0.051108, val r2: 0.872683\n",
      "epoch: 85, train loss: 0.002693, val loss: 0.005228, val mae: 0.050603, val r2: 0.874575\n",
      "epoch: 86, train loss: 0.002701, val loss: 0.005255, val mae: 0.050776, val r2: 0.873763\n",
      "epoch: 87, train loss: 0.002715, val loss: 0.005249, val mae: 0.050804, val r2: 0.874072\n",
      "epoch: 88, train loss: 0.002765, val loss: 0.005269, val mae: 0.050941, val r2: 0.873652\n",
      "epoch: 89, train loss: 0.002820, val loss: 0.005574, val mae: 0.052492, val r2: 0.866620\n",
      "epoch: 90, train loss: 0.002905, val loss: 0.006593, val mae: 0.057588, val r2: 0.842901\n",
      "epoch: 91, train loss: 0.002947, val loss: 0.008664, val mae: 0.067088, val r2: 0.794693\n",
      "epoch: 92, train loss: 0.002942, val loss: 0.009151, val mae: 0.069068, val r2: 0.783507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 93, train loss: 0.003005, val loss: 0.007921, val mae: 0.063606, val r2: 0.812466\n",
      "epoch: 94, train loss: 0.003074, val loss: 0.006166, val mae: 0.055542, val r2: 0.852889\n",
      "epoch: 95, train loss: 0.002945, val loss: 0.005486, val mae: 0.052181, val r2: 0.868622\n",
      "epoch: 96, train loss: 0.002779, val loss: 0.005450, val mae: 0.051982, val r2: 0.869284\n",
      "epoch: 97, train loss: 0.002677, val loss: 0.005635, val mae: 0.052778, val r2: 0.865175\n",
      "epoch: 98, train loss: 0.002650, val loss: 0.006048, val mae: 0.054723, val r2: 0.855591\n",
      "epoch: 99, train loss: 0.002625, val loss: 0.006194, val mae: 0.055484, val r2: 0.852408\n"
     ]
    }
   ],
   "source": [
    "# *\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "d = 64\n",
    "h = 4\n",
    "seq_len = 6\n",
    "num_layers = 2\n",
    "forward_expansion = 1\n",
    "dropout = 0.15\n",
    "lr = 0.00005\n",
    "batch_size = 128\n",
    "\n",
    "model_1 = train_val(d, h, num_layers, dropout, forward_expansion, lr, batch_size, seq_len, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "\n",
    "d = 64\n",
    "h = 4\n",
    "seq_len = 6\n",
    "num_layers = 2\n",
    "forward_expansion = 1\n",
    "dropout = 0.15\n",
    "lr = 0.00005\n",
    "batch_size = 256\n",
    "\n",
    "model_1 = train_val(d, h, num_layers, dropout, forward_expansion, lr, batch_size, seq_len, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256566bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "\n",
    "d = 32\n",
    "h = 4\n",
    "seq_len = 4\n",
    "num_layers = 2\n",
    "forward_expansion = 1\n",
    "dropout = 0.1\n",
    "lr = 0.00005\n",
    "batch_size = 64\n",
    "\n",
    "model_1 = train_val(d, h, num_layers, dropout, forward_expansion, lr, batch_size, seq_len, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "worse:\n",
    "    \n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "d = 64\n",
    "seq_len = 6\n",
    "batch_size = 64\n",
    "\n",
    "d = 16\n",
    "seq_len = 4\n",
    "batch_size = 128\n",
    "\n",
    "d = 96\n",
    "h = 4\n",
    "batch_size = 128\n",
    "\n",
    "d = 64\n",
    "seq_len = 4\n",
    "dropout = 0.15\n",
    "lr = 0.00005\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8251ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c8a6da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "period = [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 5, 31)]\n",
    "test_data = YNYT(period, seq_len=seq_len, mode='test')\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "def prdict(data, model, device=device):\n",
    "    device = device\n",
    "    inference_dataloader = torch.utils.data.DataLoader(data, batch_size=len(test_data))\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    res = []\n",
    "    fact = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(inference_dataloader):\n",
    "            inputs, regressors, labels = data\n",
    "            regressors = regressors.unsqueeze(-1)\n",
    "            labels = torch.flatten(labels, start_dim=1)\n",
    "            output = model(inputs.to(device), regressors.to(device), 0)                \n",
    "            loss = loss_fn(output, labels.to(device))\n",
    "            labels = labels.view(-1, 6).detach().cpu().numpy()\n",
    "            output = output.view(-1, 6).detach().cpu().numpy()\n",
    "            res.append(output)\n",
    "            fact.append(labels)\n",
    "    return res, fact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b7e8808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=1, mse: 0.005289999768137932, mae: 0.05127999931573868, r2: 0.8736\n",
      "h=2, mse: 0.00535999983549118, mae: 0.051589999347925186, r2: 0.87221\n",
      "h=3, mse: 0.0053900000639259815, mae: 0.051669999957084656, r2: 0.87162\n",
      "h=4, mse: 0.0053400001488626, mae: 0.05149000138044357, r2: 0.87252\n",
      "h=5, mse: 0.005350000225007534, mae: 0.05172999948263168, r2: 0.87213\n",
      "h=6, mse: 0.005289999768137932, mae: 0.051430001854896545, r2: 0.87361\n"
     ]
    }
   ],
   "source": [
    "pred, fact = prdict(test_data, model_2)\n",
    "\n",
    "for h in range(1, 7):\n",
    "    y = fact[0][:, h-1]\n",
    "    y_pred = pred[0][:, h-1]\n",
    "    r2 = round(r2_score(y, y_pred), 5)\n",
    "    mae = round(mean_absolute_error(y, y_pred), 5)\n",
    "    mse = round(mean_squared_error(y, y_pred), 5)\n",
    "    print(f'h={h}, mse: {mse}, mae: {mae}, r2: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21659f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c80ae84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bf56a171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=1, mse: 0.005549999885261059, mae: 0.052730001509189606, r2: 0.87241\n",
      "h=2, mse: 0.00558000011369586, mae: 0.0528700016438961, r2: 0.87173\n",
      "h=3, mse: 0.005619999952614307, mae: 0.05291999876499176, r2: 0.87082\n",
      "h=4, mse: 0.0056500001810491085, mae: 0.05307000130414963, r2: 0.87019\n",
      "h=5, mse: 0.005630000028759241, mae: 0.053040001541376114, r2: 0.87059\n",
      "h=6, mse: 0.0055599999614059925, mae: 0.05274999886751175, r2: 0.87214\n"
     ]
    }
   ],
   "source": [
    "# old \n",
    "\n",
    "pred, fact = prdict(test_data, model_3)\n",
    "\n",
    "for h in range(1, 7):\n",
    "    y = fact[0][:, h-1]\n",
    "    y_pred = pred[0][:, h-1]\n",
    "    r2 = round(r2_score(y, y_pred), 5)\n",
    "    mae = round(mean_absolute_error(y, y_pred), 5)\n",
    "    mse = round(mean_squared_error(y, y_pred), 5)\n",
    "    print(f'h={h}, mse: {mse}, mae: {mae}, r2: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbedbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.90793\n",
    "0.87887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71cbede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83e33d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
