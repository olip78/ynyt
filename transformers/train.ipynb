{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94418bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu!\n",
      "epoch: 0, train loss: 1.517988, val loss: 0.144604, val mae: 0.295849, val r2: -2.670630\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m path_preprocessed \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./../_ynyt/data/preprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m horizon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperiod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperiod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_preprocessed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_preprocessed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mregression_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregression_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_setting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_setting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhorizon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/andrei/ynyt/ynyt/transformers/sttre/sttre.py:359\u001b[0m, in \u001b[0;36mtrain_val\u001b[0;34m(period, epoches, path_preprocessed, embed_size, heads, num_layers, dropout, forward_expansion, lr, batch_size, seq_len, regression_head, data_setting, device, verbose, verbose_step, horizon)\u001b[0m\n\u001b[1;32m    357\u001b[0m output \u001b[38;5;241m=\u001b[39m model(inputs\u001b[38;5;241m.\u001b[39mto(device), regressors\u001b[38;5;241m.\u001b[39mto(device), dropout)\n\u001b[1;32m    358\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, labels\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m--> 359\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    361\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m+\u001b[39m loss\n",
      "File \u001b[0;32m~/miniforge3/envs/hard_ml/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/hard_ml/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "import sys\n",
    "\n",
    "print(sys.argv[1:]\n",
    "\n",
    "filename = 'exp_1_1.json'\n",
    "with open(f'./settings/{filename}', 'r') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18e4dfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "filename = sys.argv[0]\n",
    "with open(f'./settings/{filename}', 'r') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "path_preprocessed = './../data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "ed52ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [6], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 16,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 1,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.000005, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 4, \n",
    "                      'epoches': 600,\n",
    "                      'device': 'cuda',\n",
    "                     }\n",
    "\n",
    "filename = 'exp_final.json'\n",
    "\n",
    "with open(f'./settings/{filename}', 'w') as f:\n",
    "    json.dump(settings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a88c9821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"regression_head\": {\"heads\": 1, \"dropout_head\": 0.1, \"layers\": [6], \"add_features\": 2, \"flatt_factor\": 2}, \"data_setting\": {\"features\": true, \"D\": true, \"hours\": true, \"weekday\": true}, \"params\": {\"embed_size\": 32, \"heads\": 4, \"num_layers\": 2, \"dropout\": 0.15, \"forward_expansion\": 1, \"lr\": 1e-05, \"batch_size\": 128, \"seq_len\": 6, \"epoches\": 300, \"device\": \"cpu\"}}"
     ]
    }
   ],
   "source": [
    "!cat ./settings/exp_1_5_ok1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d47ee63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"regression_head\": {\"heads\": 1, \"dropout_head\": 0.1, \"layers\": [16, 6], \"add_features\": 2, \"flatt_factor\": 2}, \"data_setting\": {\"features\": true, \"D\": true, \"hours\": true, \"weekday\": true}, \"params\": {\"embed_size\": 32, \"heads\": 4, \"num_layers\": 2, \"dropout\": 0.15, \"forward_expansion\": 1, \"lr\": 1e-05, \"batch_size\": 128, \"seq_len\": 6, \"epoches\": 300, \"device\": \"cpu\"}}"
     ]
    }
   ],
   "source": [
    "!cat ./settings/exp_1_6_up_500.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8da821b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_1_1.json        exp_1_4.json        exp_1_6.json\r\n",
      "exp_1_2.json        exp_1_5.json        exp_1_6_up_500.json\r\n",
      "exp_1_3.json        exp_1_5_ok1.json    exp_1_7.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b87d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "                       0.04457, r2: test: 0.90793, 0.96273, train: 0.91653, 0.96253\n",
    "t=2, mse: 0.0047, mae: 0.04805, r2: test: 0.89385, 0.95456, train: 0.90326, 0.95393\n",
    "t=3, mse: 0.00506, mae: 0.04958, r2: test: 0.88563, 0.95046, train: 0.89692, 0.94974\n",
    "t=4, mse: 0.00524, mae: 0.05035, r2: test: 0.8815, 0.94829, train: 0.89251, 0.94658\n",
    "t=5, mse: 0.0053, mae: 0.0509, r2: test: 0.88013, 0.947, train: 0.88920, 0.94407\n",
    "t=6, mse: 0.00535, mae: 0.05129, r2: test: 0.87887, 0.94622, train: 0.88689, 0.94230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8d1d2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04912333333333333"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.04457 + 0.04805 + 0.04958 + 0.05035 + 0.0509 + 0.05129) / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7879631a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.887985"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.90793 + 0.89385 + 0.88563 + 0.8815 + 0.88013 + 0.87887) / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de35853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 24,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 2,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 6, \n",
    "                      'epoches': 500,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0716c42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps!\n",
      "Transformer(\n",
      "  (element_embedding_temporal): Linear(in_features=294, out_features=192, bias=True)\n",
      "  (element_embedding_spatial): Linear(in_features=2695, out_features=1760, bias=True)\n",
      "  (pos_embedding): Embedding(6, 32)\n",
      "  (variable_embedding): Embedding(55, 32)\n",
      "  (temporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatial): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatiotemporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (keys): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (queries): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatter): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=11220, out_features=2640, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.15, inplace=False)\n",
      "    (3): Linear(in_features=2640, out_features=330, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch: 0, train loss: 0.019273, val loss: 0.017495, val mae: 0.104709, val r2: 0.556603\n",
      "epoch: 1, train loss: 0.011331, val loss: 0.014474, val mae: 0.091565, val r2: 0.633875\n",
      "epoch: 2, train loss: 0.009777, val loss: 0.013014, val mae: 0.086037, val r2: 0.670564\n",
      "epoch: 3, train loss: 0.008876, val loss: 0.012072, val mae: 0.082536, val r2: 0.694333\n",
      "epoch: 4, train loss: 0.008221, val loss: 0.011522, val mae: 0.080489, val r2: 0.708260\n",
      "epoch: 5, train loss: 0.007707, val loss: 0.010989, val mae: 0.078498, val r2: 0.721857\n",
      "epoch: 6, train loss: 0.007269, val loss: 0.010563, val mae: 0.076799, val r2: 0.733099\n",
      "epoch: 7, train loss: 0.006918, val loss: 0.010388, val mae: 0.076136, val r2: 0.737778\n",
      "epoch: 8, train loss: 0.006600, val loss: 0.010138, val mae: 0.075148, val r2: 0.744443\n",
      "epoch: 9, train loss: 0.006323, val loss: 0.009797, val mae: 0.073671, val r2: 0.753257\n",
      "epoch: 10, train loss: 0.006070, val loss: 0.009650, val mae: 0.073065, val r2: 0.757344\n",
      "epoch: 11, train loss: 0.005861, val loss: 0.009471, val mae: 0.072201, val r2: 0.762060\n",
      "epoch: 12, train loss: 0.005649, val loss: 0.009205, val mae: 0.070914, val r2: 0.768983\n",
      "epoch: 13, train loss: 0.005461, val loss: 0.009064, val mae: 0.070255, val r2: 0.772726\n",
      "epoch: 14, train loss: 0.005279, val loss: 0.008892, val mae: 0.069431, val r2: 0.777258\n",
      "epoch: 15, train loss: 0.005137, val loss: 0.008747, val mae: 0.068684, val r2: 0.781118\n",
      "epoch: 16, train loss: 0.004996, val loss: 0.008579, val mae: 0.067881, val r2: 0.785530\n",
      "epoch: 17, train loss: 0.004886, val loss: 0.008494, val mae: 0.067475, val r2: 0.787701\n",
      "epoch: 18, train loss: 0.004778, val loss: 0.008385, val mae: 0.066885, val r2: 0.790595\n",
      "epoch: 19, train loss: 0.004683, val loss: 0.008387, val mae: 0.066839, val r2: 0.790712\n",
      "epoch: 20, train loss: 0.004588, val loss: 0.008309, val mae: 0.066452, val r2: 0.792854\n",
      "epoch: 21, train loss: 0.004511, val loss: 0.008161, val mae: 0.065701, val r2: 0.796521\n",
      "epoch: 22, train loss: 0.004435, val loss: 0.008344, val mae: 0.066392, val r2: 0.792160\n",
      "epoch: 23, train loss: 0.004370, val loss: 0.008116, val mae: 0.065386, val r2: 0.797884\n",
      "epoch: 24, train loss: 0.004308, val loss: 0.008177, val mae: 0.065693, val r2: 0.796479\n",
      "epoch: 25, train loss: 0.004256, val loss: 0.008107, val mae: 0.065231, val r2: 0.798267\n",
      "epoch: 26, train loss: 0.004201, val loss: 0.008265, val mae: 0.065946, val r2: 0.794510\n",
      "epoch: 27, train loss: 0.004159, val loss: 0.008077, val mae: 0.065057, val r2: 0.799163\n",
      "epoch: 28, train loss: 0.004116, val loss: 0.008154, val mae: 0.065384, val r2: 0.797314\n",
      "epoch: 29, train loss: 0.004079, val loss: 0.008254, val mae: 0.065862, val r2: 0.794866\n",
      "epoch: 30, train loss: 0.004050, val loss: 0.008109, val mae: 0.065140, val r2: 0.798516\n",
      "epoch: 31, train loss: 0.004028, val loss: 0.007994, val mae: 0.064687, val r2: 0.801181\n",
      "epoch: 32, train loss: 0.003989, val loss: 0.007979, val mae: 0.064564, val r2: 0.801613\n",
      "epoch: 33, train loss: 0.003975, val loss: 0.007996, val mae: 0.064579, val r2: 0.801361\n",
      "epoch: 34, train loss: 0.003955, val loss: 0.007827, val mae: 0.063824, val r2: 0.805372\n",
      "epoch: 35, train loss: 0.003944, val loss: 0.007824, val mae: 0.063852, val r2: 0.805445\n",
      "epoch: 36, train loss: 0.003940, val loss: 0.007542, val mae: 0.062502, val r2: 0.812403\n",
      "epoch: 37, train loss: 0.003931, val loss: 0.007269, val mae: 0.061260, val r2: 0.818864\n",
      "epoch: 38, train loss: 0.003921, val loss: 0.007241, val mae: 0.060984, val r2: 0.819565\n",
      "epoch: 39, train loss: 0.003926, val loss: 0.006917, val mae: 0.059447, val r2: 0.827542\n",
      "epoch: 40, train loss: 0.003909, val loss: 0.006604, val mae: 0.057890, val r2: 0.835108\n",
      "epoch: 41, train loss: 0.003885, val loss: 0.006384, val mae: 0.056695, val r2: 0.840396\n",
      "epoch: 42, train loss: 0.003833, val loss: 0.006239, val mae: 0.055868, val r2: 0.843918\n",
      "epoch: 43, train loss: 0.003760, val loss: 0.006167, val mae: 0.055451, val r2: 0.845610\n",
      "epoch: 44, train loss: 0.003696, val loss: 0.006154, val mae: 0.055336, val r2: 0.845895\n",
      "epoch: 45, train loss: 0.003625, val loss: 0.006168, val mae: 0.055373, val r2: 0.845647\n",
      "epoch: 46, train loss: 0.003579, val loss: 0.006126, val mae: 0.055204, val r2: 0.846665\n",
      "epoch: 47, train loss: 0.003547, val loss: 0.006125, val mae: 0.055166, val r2: 0.846687\n",
      "epoch: 48, train loss: 0.003516, val loss: 0.006108, val mae: 0.055058, val r2: 0.847158\n",
      "epoch: 49, train loss: 0.003488, val loss: 0.006100, val mae: 0.054988, val r2: 0.847372\n",
      "epoch: 50, train loss: 0.003467, val loss: 0.006079, val mae: 0.054828, val r2: 0.847943\n",
      "epoch: 51, train loss: 0.003459, val loss: 0.006070, val mae: 0.054776, val r2: 0.848209\n",
      "epoch: 52, train loss: 0.003444, val loss: 0.006039, val mae: 0.054595, val r2: 0.848951\n",
      "epoch: 53, train loss: 0.003425, val loss: 0.006039, val mae: 0.054478, val r2: 0.848941\n",
      "epoch: 54, train loss: 0.003414, val loss: 0.006029, val mae: 0.054419, val r2: 0.849080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55, train loss: 0.003408, val loss: 0.006016, val mae: 0.054342, val r2: 0.849457\n",
      "epoch: 56, train loss: 0.003397, val loss: 0.006022, val mae: 0.054361, val r2: 0.849328\n",
      "epoch: 57, train loss: 0.003379, val loss: 0.006006, val mae: 0.054237, val r2: 0.849795\n",
      "epoch: 58, train loss: 0.003367, val loss: 0.005976, val mae: 0.054117, val r2: 0.850523\n",
      "epoch: 59, train loss: 0.003367, val loss: 0.005996, val mae: 0.054095, val r2: 0.850044\n",
      "epoch: 60, train loss: 0.003362, val loss: 0.005982, val mae: 0.053954, val r2: 0.850298\n",
      "epoch: 61, train loss: 0.003366, val loss: 0.005994, val mae: 0.054038, val r2: 0.850014\n",
      "epoch: 62, train loss: 0.003381, val loss: 0.006000, val mae: 0.054014, val r2: 0.849941\n",
      "epoch: 63, train loss: 0.003404, val loss: 0.006003, val mae: 0.054098, val r2: 0.849908\n",
      "epoch: 64, train loss: 0.003440, val loss: 0.006080, val mae: 0.054499, val r2: 0.848209\n",
      "epoch: 65, train loss: 0.003476, val loss: 0.006328, val mae: 0.055898, val r2: 0.842384\n",
      "epoch: 66, train loss: 0.003462, val loss: 0.006645, val mae: 0.057525, val r2: 0.834797\n",
      "epoch: 67, train loss: 0.003419, val loss: 0.006887, val mae: 0.058712, val r2: 0.829201\n",
      "epoch: 68, train loss: 0.003363, val loss: 0.007073, val mae: 0.059583, val r2: 0.824883\n",
      "epoch: 69, train loss: 0.003371, val loss: 0.007002, val mae: 0.059256, val r2: 0.826732\n",
      "epoch: 70, train loss: 0.003469, val loss: 0.006660, val mae: 0.057619, val r2: 0.834766\n",
      "epoch: 71, train loss: 0.003595, val loss: 0.006049, val mae: 0.054510, val r2: 0.849299\n",
      "epoch: 72, train loss: 0.003664, val loss: 0.005747, val mae: 0.052701, val r2: 0.856182\n",
      "epoch: 73, train loss: 0.003581, val loss: 0.005779, val mae: 0.052689, val r2: 0.855227\n",
      "epoch: 74, train loss: 0.003389, val loss: 0.005726, val mae: 0.052494, val r2: 0.856614\n",
      "epoch: 75, train loss: 0.003236, val loss: 0.005707, val mae: 0.052533, val r2: 0.857155\n",
      "epoch: 76, train loss: 0.003153, val loss: 0.005748, val mae: 0.052804, val r2: 0.856189\n",
      "epoch: 77, train loss: 0.003134, val loss: 0.005778, val mae: 0.052903, val r2: 0.855569\n",
      "epoch: 78, train loss: 0.003121, val loss: 0.005806, val mae: 0.053057, val r2: 0.854887\n",
      "epoch: 79, train loss: 0.003108, val loss: 0.005828, val mae: 0.053178, val r2: 0.854483\n",
      "epoch: 80, train loss: 0.003100, val loss: 0.005842, val mae: 0.053186, val r2: 0.854074\n",
      "epoch: 81, train loss: 0.003088, val loss: 0.005812, val mae: 0.053073, val r2: 0.854857\n",
      "epoch: 82, train loss: 0.003078, val loss: 0.005814, val mae: 0.053122, val r2: 0.854843\n",
      "epoch: 83, train loss: 0.003071, val loss: 0.005766, val mae: 0.052777, val r2: 0.855988\n",
      "epoch: 84, train loss: 0.003057, val loss: 0.005787, val mae: 0.052870, val r2: 0.855396\n",
      "epoch: 85, train loss: 0.003043, val loss: 0.005752, val mae: 0.052701, val r2: 0.856190\n",
      "epoch: 86, train loss: 0.003023, val loss: 0.005774, val mae: 0.052827, val r2: 0.855728\n",
      "epoch: 87, train loss: 0.003010, val loss: 0.005773, val mae: 0.052806, val r2: 0.855634\n",
      "epoch: 88, train loss: 0.003001, val loss: 0.005747, val mae: 0.052734, val r2: 0.856325\n",
      "epoch: 89, train loss: 0.002990, val loss: 0.005772, val mae: 0.052837, val r2: 0.855642\n",
      "epoch: 90, train loss: 0.002972, val loss: 0.005752, val mae: 0.052722, val r2: 0.856205\n",
      "epoch: 91, train loss: 0.002959, val loss: 0.005762, val mae: 0.052740, val r2: 0.855758\n",
      "epoch: 92, train loss: 0.002952, val loss: 0.005778, val mae: 0.052880, val r2: 0.855522\n",
      "epoch: 93, train loss: 0.002950, val loss: 0.005724, val mae: 0.052510, val r2: 0.856719\n",
      "epoch: 94, train loss: 0.002926, val loss: 0.005714, val mae: 0.052523, val r2: 0.856897\n",
      "epoch: 95, train loss: 0.002926, val loss: 0.005737, val mae: 0.052657, val r2: 0.856314\n",
      "epoch: 96, train loss: 0.002922, val loss: 0.005715, val mae: 0.052469, val r2: 0.856752\n",
      "epoch: 97, train loss: 0.002911, val loss: 0.005722, val mae: 0.052520, val r2: 0.856683\n",
      "epoch: 98, train loss: 0.002905, val loss: 0.005703, val mae: 0.052439, val r2: 0.856976\n",
      "epoch: 99, train loss: 0.002899, val loss: 0.005766, val mae: 0.052855, val r2: 0.855582\n",
      "epoch: 100, train loss: 0.002889, val loss: 0.005756, val mae: 0.052701, val r2: 0.855692\n",
      "epoch: 101, train loss: 0.002887, val loss: 0.005756, val mae: 0.052807, val r2: 0.855787\n",
      "epoch: 102, train loss: 0.002887, val loss: 0.005773, val mae: 0.052808, val r2: 0.855306\n",
      "epoch: 103, train loss: 0.002884, val loss: 0.005847, val mae: 0.053284, val r2: 0.853452\n",
      "epoch: 104, train loss: 0.002885, val loss: 0.005862, val mae: 0.053314, val r2: 0.853040\n",
      "epoch: 105, train loss: 0.002889, val loss: 0.005984, val mae: 0.053965, val r2: 0.850012\n",
      "epoch: 106, train loss: 0.002893, val loss: 0.006059, val mae: 0.054400, val r2: 0.848332\n",
      "epoch: 107, train loss: 0.002894, val loss: 0.006258, val mae: 0.055391, val r2: 0.843572\n",
      "epoch: 108, train loss: 0.002913, val loss: 0.006336, val mae: 0.055818, val r2: 0.841499\n",
      "epoch: 109, train loss: 0.002941, val loss: 0.006618, val mae: 0.057063, val r2: 0.834779\n",
      "epoch: 110, train loss: 0.002978, val loss: 0.006583, val mae: 0.056899, val r2: 0.835315\n",
      "epoch: 111, train loss: 0.003042, val loss: 0.006207, val mae: 0.055123, val r2: 0.844425\n",
      "epoch: 112, train loss: 0.003136, val loss: 0.005849, val mae: 0.053267, val r2: 0.853049\n",
      "epoch: 113, train loss: 0.003148, val loss: 0.005694, val mae: 0.052549, val r2: 0.856616\n",
      "epoch: 114, train loss: 0.003169, val loss: 0.006067, val mae: 0.054663, val r2: 0.846988\n",
      "epoch: 115, train loss: 0.003289, val loss: 0.007119, val mae: 0.059717, val r2: 0.820078\n",
      "epoch: 116, train loss: 0.003358, val loss: 0.007836, val mae: 0.063210, val r2: 0.801582\n",
      "epoch: 117, train loss: 0.003386, val loss: 0.008145, val mae: 0.064714, val r2: 0.793392\n",
      "epoch: 118, train loss: 0.003359, val loss: 0.008110, val mae: 0.064675, val r2: 0.793818\n",
      "epoch: 119, train loss: 0.003319, val loss: 0.008024, val mae: 0.064368, val r2: 0.795709\n",
      "epoch: 120, train loss: 0.003256, val loss: 0.007717, val mae: 0.063043, val r2: 0.803375\n",
      "epoch: 121, train loss: 0.003223, val loss: 0.007697, val mae: 0.063021, val r2: 0.803361\n",
      "epoch: 122, train loss: 0.003164, val loss: 0.007499, val mae: 0.062102, val r2: 0.808558\n",
      "epoch: 123, train loss: 0.003133, val loss: 0.007312, val mae: 0.061231, val r2: 0.813361\n",
      "epoch: 124, train loss: 0.003101, val loss: 0.007198, val mae: 0.060733, val r2: 0.816190\n",
      "epoch: 125, train loss: 0.003057, val loss: 0.007120, val mae: 0.060332, val r2: 0.818092\n",
      "epoch: 126, train loss: 0.003038, val loss: 0.007019, val mae: 0.059843, val r2: 0.820766\n",
      "epoch: 127, train loss: 0.002993, val loss: 0.006798, val mae: 0.058749, val r2: 0.826650\n",
      "epoch: 128, train loss: 0.002958, val loss: 0.006769, val mae: 0.058679, val r2: 0.827484\n",
      "epoch: 129, train loss: 0.002916, val loss: 0.006530, val mae: 0.057431, val r2: 0.833738\n",
      "epoch: 130, train loss: 0.002882, val loss: 0.006508, val mae: 0.057305, val r2: 0.834575\n",
      "epoch: 131, train loss: 0.002843, val loss: 0.006328, val mae: 0.056388, val r2: 0.839311\n",
      "epoch: 132, train loss: 0.002814, val loss: 0.006169, val mae: 0.055662, val r2: 0.843589\n",
      "epoch: 133, train loss: 0.002786, val loss: 0.006172, val mae: 0.055695, val r2: 0.843775\n",
      "epoch: 134, train loss: 0.002753, val loss: 0.006078, val mae: 0.055148, val r2: 0.846320\n",
      "epoch: 135, train loss: 0.002730, val loss: 0.006008, val mae: 0.054796, val r2: 0.848228\n",
      "epoch: 136, train loss: 0.002707, val loss: 0.005884, val mae: 0.054170, val r2: 0.851491\n",
      "epoch: 137, train loss: 0.002704, val loss: 0.005936, val mae: 0.054444, val r2: 0.850343\n",
      "epoch: 138, train loss: 0.002678, val loss: 0.005872, val mae: 0.054059, val r2: 0.852180\n",
      "epoch: 139, train loss: 0.002657, val loss: 0.005695, val mae: 0.053149, val r2: 0.856838\n",
      "epoch: 140, train loss: 0.002640, val loss: 0.005702, val mae: 0.053108, val r2: 0.856674\n",
      "epoch: 141, train loss: 0.002632, val loss: 0.005619, val mae: 0.052683, val r2: 0.858902\n",
      "epoch: 142, train loss: 0.002619, val loss: 0.005695, val mae: 0.053134, val r2: 0.856921\n",
      "epoch: 143, train loss: 0.002609, val loss: 0.005618, val mae: 0.052747, val r2: 0.858889\n",
      "epoch: 144, train loss: 0.002593, val loss: 0.005536, val mae: 0.052272, val r2: 0.860969\n",
      "epoch: 145, train loss: 0.002585, val loss: 0.005568, val mae: 0.052401, val r2: 0.860289\n",
      "epoch: 146, train loss: 0.002572, val loss: 0.005510, val mae: 0.052089, val r2: 0.861689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 147, train loss: 0.002563, val loss: 0.005547, val mae: 0.052327, val r2: 0.860789\n",
      "epoch: 148, train loss: 0.002551, val loss: 0.005563, val mae: 0.052421, val r2: 0.860374\n",
      "epoch: 149, train loss: 0.002540, val loss: 0.005523, val mae: 0.052238, val r2: 0.861382\n",
      "epoch: 150, train loss: 0.002531, val loss: 0.005598, val mae: 0.052572, val r2: 0.859605\n",
      "epoch: 151, train loss: 0.002519, val loss: 0.005621, val mae: 0.052709, val r2: 0.859124\n",
      "epoch: 152, train loss: 0.002505, val loss: 0.005628, val mae: 0.052679, val r2: 0.858950\n",
      "epoch: 153, train loss: 0.002498, val loss: 0.005579, val mae: 0.052523, val r2: 0.860114\n",
      "epoch: 154, train loss: 0.002490, val loss: 0.005498, val mae: 0.052063, val r2: 0.862167\n",
      "epoch: 155, train loss: 0.002482, val loss: 0.005595, val mae: 0.052603, val r2: 0.859706\n",
      "epoch: 156, train loss: 0.002472, val loss: 0.005536, val mae: 0.052328, val r2: 0.861137\n",
      "epoch: 157, train loss: 0.002464, val loss: 0.005709, val mae: 0.053185, val r2: 0.856997\n",
      "epoch: 158, train loss: 0.002453, val loss: 0.005615, val mae: 0.052761, val r2: 0.859165\n",
      "epoch: 159, train loss: 0.002449, val loss: 0.005573, val mae: 0.052521, val r2: 0.860325\n",
      "epoch: 160, train loss: 0.002443, val loss: 0.005599, val mae: 0.052688, val r2: 0.859492\n",
      "epoch: 161, train loss: 0.002435, val loss: 0.005612, val mae: 0.052745, val r2: 0.859219\n",
      "epoch: 162, train loss: 0.002423, val loss: 0.005600, val mae: 0.052682, val r2: 0.859559\n",
      "epoch: 163, train loss: 0.002420, val loss: 0.005593, val mae: 0.052659, val r2: 0.859691\n",
      "epoch: 164, train loss: 0.002419, val loss: 0.005650, val mae: 0.052983, val r2: 0.858215\n",
      "epoch: 165, train loss: 0.002408, val loss: 0.005613, val mae: 0.052756, val r2: 0.858909\n",
      "epoch: 166, train loss: 0.002403, val loss: 0.005644, val mae: 0.052892, val r2: 0.858233\n",
      "epoch: 167, train loss: 0.002399, val loss: 0.005630, val mae: 0.052861, val r2: 0.858399\n",
      "epoch: 168, train loss: 0.002393, val loss: 0.005503, val mae: 0.052162, val r2: 0.861664\n",
      "epoch: 169, train loss: 0.002389, val loss: 0.005478, val mae: 0.051986, val r2: 0.862157\n",
      "epoch: 170, train loss: 0.002388, val loss: 0.005407, val mae: 0.051625, val r2: 0.863811\n",
      "epoch: 171, train loss: 0.002386, val loss: 0.005576, val mae: 0.052491, val r2: 0.859593\n",
      "epoch: 172, train loss: 0.002378, val loss: 0.005486, val mae: 0.052080, val r2: 0.861576\n",
      "epoch: 173, train loss: 0.002377, val loss: 0.005438, val mae: 0.051766, val r2: 0.862873\n",
      "epoch: 174, train loss: 0.002383, val loss: 0.005349, val mae: 0.051229, val r2: 0.865145\n",
      "epoch: 175, train loss: 0.002385, val loss: 0.005342, val mae: 0.051169, val r2: 0.865138\n",
      "epoch: 176, train loss: 0.002400, val loss: 0.005276, val mae: 0.050621, val r2: 0.866887\n",
      "epoch: 177, train loss: 0.002431, val loss: 0.005329, val mae: 0.050788, val r2: 0.865487\n",
      "epoch: 178, train loss: 0.002476, val loss: 0.005423, val mae: 0.051222, val r2: 0.862806\n",
      "epoch: 179, train loss: 0.002597, val loss: 0.005704, val mae: 0.052718, val r2: 0.855140\n",
      "epoch: 180, train loss: 0.002797, val loss: 0.006988, val mae: 0.059184, val r2: 0.821676\n",
      "epoch: 181, train loss: 0.002937, val loss: 0.009964, val mae: 0.071581, val r2: 0.746384\n",
      "epoch: 182, train loss: 0.002743, val loss: 0.008320, val mae: 0.065228, val r2: 0.787711\n",
      "epoch: 183, train loss: 0.002671, val loss: 0.006391, val mae: 0.056772, val r2: 0.837099\n",
      "epoch: 184, train loss: 0.002596, val loss: 0.006263, val mae: 0.056166, val r2: 0.840382\n",
      "epoch: 185, train loss: 0.002514, val loss: 0.006524, val mae: 0.057431, val r2: 0.833632\n",
      "epoch: 186, train loss: 0.002476, val loss: 0.006612, val mae: 0.057851, val r2: 0.831785\n",
      "epoch: 187, train loss: 0.002452, val loss: 0.006488, val mae: 0.057333, val r2: 0.835150\n",
      "epoch: 188, train loss: 0.002437, val loss: 0.006244, val mae: 0.056121, val r2: 0.841462\n",
      "epoch: 189, train loss: 0.002409, val loss: 0.006022, val mae: 0.054983, val r2: 0.847404\n",
      "epoch: 190, train loss: 0.002398, val loss: 0.005986, val mae: 0.054758, val r2: 0.848413\n",
      "epoch: 191, train loss: 0.002370, val loss: 0.005935, val mae: 0.054546, val r2: 0.849830\n",
      "epoch: 192, train loss: 0.002356, val loss: 0.005886, val mae: 0.054240, val r2: 0.851253\n",
      "epoch: 193, train loss: 0.002344, val loss: 0.005675, val mae: 0.053169, val r2: 0.856817\n",
      "epoch: 194, train loss: 0.002334, val loss: 0.005609, val mae: 0.052760, val r2: 0.858578\n",
      "epoch: 195, train loss: 0.002319, val loss: 0.005549, val mae: 0.052425, val r2: 0.860218\n",
      "epoch: 196, train loss: 0.002314, val loss: 0.005522, val mae: 0.052255, val r2: 0.860992\n",
      "epoch: 197, train loss: 0.002303, val loss: 0.005437, val mae: 0.051781, val r2: 0.863222\n",
      "epoch: 198, train loss: 0.002296, val loss: 0.005456, val mae: 0.051839, val r2: 0.862752\n",
      "epoch: 199, train loss: 0.002291, val loss: 0.005400, val mae: 0.051547, val r2: 0.864154\n",
      "epoch: 200, train loss: 0.002280, val loss: 0.005364, val mae: 0.051281, val r2: 0.865175\n",
      "epoch: 201, train loss: 0.002276, val loss: 0.005352, val mae: 0.051292, val r2: 0.865461\n",
      "epoch: 202, train loss: 0.002265, val loss: 0.005367, val mae: 0.051312, val r2: 0.865290\n",
      "epoch: 203, train loss: 0.002262, val loss: 0.005333, val mae: 0.051108, val r2: 0.866069\n",
      "epoch: 204, train loss: 0.002256, val loss: 0.005341, val mae: 0.051085, val r2: 0.865960\n",
      "epoch: 205, train loss: 0.002245, val loss: 0.005276, val mae: 0.050736, val r2: 0.867604\n",
      "epoch: 206, train loss: 0.002237, val loss: 0.005271, val mae: 0.050705, val r2: 0.867706\n",
      "epoch: 207, train loss: 0.002232, val loss: 0.005272, val mae: 0.050726, val r2: 0.867687\n",
      "epoch: 208, train loss: 0.002228, val loss: 0.005237, val mae: 0.050525, val r2: 0.868650\n",
      "epoch: 209, train loss: 0.002223, val loss: 0.005204, val mae: 0.050276, val r2: 0.869428\n",
      "epoch: 210, train loss: 0.002213, val loss: 0.005211, val mae: 0.050354, val r2: 0.869249\n",
      "epoch: 211, train loss: 0.002205, val loss: 0.005176, val mae: 0.050142, val r2: 0.870088\n",
      "epoch: 212, train loss: 0.002199, val loss: 0.005125, val mae: 0.049771, val r2: 0.871336\n",
      "epoch: 213, train loss: 0.002193, val loss: 0.005138, val mae: 0.049877, val r2: 0.870985\n",
      "epoch: 214, train loss: 0.002189, val loss: 0.005181, val mae: 0.050107, val r2: 0.869969\n",
      "epoch: 215, train loss: 0.002181, val loss: 0.005228, val mae: 0.050474, val r2: 0.868644\n",
      "epoch: 216, train loss: 0.002181, val loss: 0.005157, val mae: 0.050004, val r2: 0.870431\n",
      "epoch: 217, train loss: 0.002171, val loss: 0.005174, val mae: 0.049992, val r2: 0.869861\n",
      "epoch: 218, train loss: 0.002171, val loss: 0.005211, val mae: 0.050203, val r2: 0.868996\n",
      "epoch: 219, train loss: 0.002167, val loss: 0.005197, val mae: 0.050121, val r2: 0.869275\n",
      "epoch: 220, train loss: 0.002170, val loss: 0.005303, val mae: 0.050705, val r2: 0.866568\n",
      "epoch: 221, train loss: 0.002166, val loss: 0.005308, val mae: 0.050761, val r2: 0.866277\n",
      "epoch: 222, train loss: 0.002174, val loss: 0.005332, val mae: 0.050969, val r2: 0.865673\n",
      "epoch: 223, train loss: 0.002179, val loss: 0.005472, val mae: 0.051687, val r2: 0.862062\n",
      "epoch: 224, train loss: 0.002186, val loss: 0.005523, val mae: 0.051995, val r2: 0.860814\n",
      "epoch: 225, train loss: 0.002205, val loss: 0.005797, val mae: 0.053504, val r2: 0.853916\n",
      "epoch: 226, train loss: 0.002224, val loss: 0.006222, val mae: 0.055775, val r2: 0.843229\n",
      "epoch: 227, train loss: 0.002233, val loss: 0.006999, val mae: 0.059465, val r2: 0.823622\n",
      "epoch: 228, train loss: 0.002277, val loss: 0.007464, val mae: 0.061519, val r2: 0.811562\n",
      "epoch: 229, train loss: 0.002352, val loss: 0.006684, val mae: 0.058065, val r2: 0.830931\n",
      "epoch: 230, train loss: 0.002427, val loss: 0.005603, val mae: 0.052891, val r2: 0.858342\n",
      "epoch: 231, train loss: 0.002388, val loss: 0.005417, val mae: 0.051935, val r2: 0.862884\n",
      "epoch: 232, train loss: 0.002347, val loss: 0.005730, val mae: 0.053574, val r2: 0.854772\n",
      "epoch: 233, train loss: 0.002350, val loss: 0.006152, val mae: 0.055735, val r2: 0.843875\n",
      "epoch: 234, train loss: 0.002271, val loss: 0.006318, val mae: 0.056569, val r2: 0.839507\n",
      "epoch: 235, train loss: 0.002236, val loss: 0.005926, val mae: 0.054650, val r2: 0.849712\n",
      "epoch: 236, train loss: 0.002242, val loss: 0.005593, val mae: 0.052813, val r2: 0.858496\n",
      "epoch: 237, train loss: 0.002220, val loss: 0.005430, val mae: 0.051909, val r2: 0.862829\n",
      "epoch: 238, train loss: 0.002189, val loss: 0.005389, val mae: 0.051641, val r2: 0.864051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 239, train loss: 0.002157, val loss: 0.005418, val mae: 0.051776, val r2: 0.863338\n",
      "epoch: 240, train loss: 0.002149, val loss: 0.005376, val mae: 0.051552, val r2: 0.864610\n",
      "epoch: 241, train loss: 0.002136, val loss: 0.005246, val mae: 0.050845, val r2: 0.868063\n",
      "epoch: 242, train loss: 0.002130, val loss: 0.005196, val mae: 0.050482, val r2: 0.869352\n",
      "epoch: 243, train loss: 0.002117, val loss: 0.005161, val mae: 0.050211, val r2: 0.870366\n",
      "epoch: 244, train loss: 0.002110, val loss: 0.005126, val mae: 0.049924, val r2: 0.871359\n",
      "epoch: 245, train loss: 0.002099, val loss: 0.005094, val mae: 0.049711, val r2: 0.872156\n",
      "epoch: 246, train loss: 0.002095, val loss: 0.005101, val mae: 0.049723, val r2: 0.872081\n",
      "epoch: 247, train loss: 0.002090, val loss: 0.005111, val mae: 0.049718, val r2: 0.871782\n",
      "epoch: 248, train loss: 0.002083, val loss: 0.005095, val mae: 0.049629, val r2: 0.872216\n",
      "epoch: 249, train loss: 0.002078, val loss: 0.005074, val mae: 0.049531, val r2: 0.872708\n",
      "epoch: 250, train loss: 0.002075, val loss: 0.005070, val mae: 0.049501, val r2: 0.872789\n",
      "epoch: 251, train loss: 0.002073, val loss: 0.005086, val mae: 0.049503, val r2: 0.872444\n",
      "epoch: 252, train loss: 0.002067, val loss: 0.005067, val mae: 0.049481, val r2: 0.872850\n",
      "epoch: 253, train loss: 0.002058, val loss: 0.005101, val mae: 0.049595, val r2: 0.872002\n",
      "epoch: 254, train loss: 0.002055, val loss: 0.005078, val mae: 0.049574, val r2: 0.872659\n",
      "epoch: 255, train loss: 0.002052, val loss: 0.005085, val mae: 0.049669, val r2: 0.872440\n",
      "epoch: 256, train loss: 0.002050, val loss: 0.005099, val mae: 0.049794, val r2: 0.872096\n",
      "epoch: 257, train loss: 0.002040, val loss: 0.005167, val mae: 0.050208, val r2: 0.870434\n",
      "epoch: 258, train loss: 0.002041, val loss: 0.005170, val mae: 0.050290, val r2: 0.870309\n",
      "epoch: 259, train loss: 0.002031, val loss: 0.005160, val mae: 0.050235, val r2: 0.870469\n",
      "epoch: 260, train loss: 0.002030, val loss: 0.005202, val mae: 0.050511, val r2: 0.869435\n",
      "epoch: 261, train loss: 0.002025, val loss: 0.005194, val mae: 0.050387, val r2: 0.869624\n",
      "epoch: 262, train loss: 0.002019, val loss: 0.005159, val mae: 0.050288, val r2: 0.870480\n",
      "epoch: 263, train loss: 0.002013, val loss: 0.005177, val mae: 0.050307, val r2: 0.870030\n",
      "epoch: 264, train loss: 0.002012, val loss: 0.005189, val mae: 0.050458, val r2: 0.869635\n",
      "epoch: 265, train loss: 0.002012, val loss: 0.005150, val mae: 0.050247, val r2: 0.870592\n",
      "epoch: 266, train loss: 0.002008, val loss: 0.005179, val mae: 0.050440, val r2: 0.869937\n",
      "epoch: 267, train loss: 0.002007, val loss: 0.005146, val mae: 0.050241, val r2: 0.870783\n",
      "epoch: 268, train loss: 0.002002, val loss: 0.005189, val mae: 0.050368, val r2: 0.869715\n",
      "epoch: 269, train loss: 0.002005, val loss: 0.005176, val mae: 0.050285, val r2: 0.870033\n",
      "epoch: 270, train loss: 0.002004, val loss: 0.005265, val mae: 0.050629, val r2: 0.867844\n",
      "epoch: 271, train loss: 0.002009, val loss: 0.005261, val mae: 0.050604, val r2: 0.867944\n",
      "epoch: 272, train loss: 0.002022, val loss: 0.005398, val mae: 0.051008, val r2: 0.864360\n",
      "epoch: 273, train loss: 0.002046, val loss: 0.005420, val mae: 0.051076, val r2: 0.863640\n",
      "epoch: 274, train loss: 0.002092, val loss: 0.005348, val mae: 0.050964, val r2: 0.865189\n",
      "epoch: 275, train loss: 0.002115, val loss: 0.005531, val mae: 0.052235, val r2: 0.859927\n",
      "epoch: 276, train loss: 0.002140, val loss: 0.006164, val mae: 0.055586, val r2: 0.843078\n",
      "epoch: 277, train loss: 0.002143, val loss: 0.006565, val mae: 0.057529, val r2: 0.832807\n",
      "epoch: 278, train loss: 0.002159, val loss: 0.006444, val mae: 0.056898, val r2: 0.836110\n",
      "epoch: 279, train loss: 0.002213, val loss: 0.005769, val mae: 0.053691, val r2: 0.853935\n",
      "epoch: 280, train loss: 0.002233, val loss: 0.005385, val mae: 0.051626, val r2: 0.864295\n",
      "epoch: 281, train loss: 0.002155, val loss: 0.005262, val mae: 0.050855, val r2: 0.867517\n",
      "epoch: 282, train loss: 0.002080, val loss: 0.005300, val mae: 0.050931, val r2: 0.866389\n",
      "epoch: 283, train loss: 0.002047, val loss: 0.005368, val mae: 0.051439, val r2: 0.864480\n",
      "epoch: 284, train loss: 0.002037, val loss: 0.005385, val mae: 0.051590, val r2: 0.864132\n",
      "epoch: 285, train loss: 0.002035, val loss: 0.005326, val mae: 0.051290, val r2: 0.865760\n",
      "epoch: 286, train loss: 0.002029, val loss: 0.005208, val mae: 0.050658, val r2: 0.868939\n",
      "epoch: 287, train loss: 0.002019, val loss: 0.005119, val mae: 0.049983, val r2: 0.871355\n",
      "epoch: 288, train loss: 0.002007, val loss: 0.005126, val mae: 0.049824, val r2: 0.871243\n",
      "epoch: 289, train loss: 0.002009, val loss: 0.005078, val mae: 0.049509, val r2: 0.872538\n",
      "epoch: 290, train loss: 0.002012, val loss: 0.005104, val mae: 0.049556, val r2: 0.871847\n",
      "epoch: 291, train loss: 0.002019, val loss: 0.005123, val mae: 0.049630, val r2: 0.871261\n",
      "epoch: 292, train loss: 0.002024, val loss: 0.005139, val mae: 0.049784, val r2: 0.870884\n",
      "epoch: 293, train loss: 0.002023, val loss: 0.005162, val mae: 0.050113, val r2: 0.870351\n",
      "epoch: 294, train loss: 0.002020, val loss: 0.005170, val mae: 0.050211, val r2: 0.870135\n",
      "epoch: 295, train loss: 0.002031, val loss: 0.005157, val mae: 0.050217, val r2: 0.870467\n",
      "epoch: 296, train loss: 0.002031, val loss: 0.005165, val mae: 0.050147, val r2: 0.870226\n",
      "epoch: 297, train loss: 0.002016, val loss: 0.005266, val mae: 0.050368, val r2: 0.867636\n",
      "epoch: 298, train loss: 0.002003, val loss: 0.005322, val mae: 0.050515, val r2: 0.866049\n",
      "epoch: 299, train loss: 0.002008, val loss: 0.005370, val mae: 0.050763, val r2: 0.864657\n",
      "epoch: 300, train loss: 0.002002, val loss: 0.005368, val mae: 0.050980, val r2: 0.864833\n",
      "epoch: 301, train loss: 0.001982, val loss: 0.005392, val mae: 0.051302, val r2: 0.864212\n",
      "epoch: 302, train loss: 0.001978, val loss: 0.005448, val mae: 0.051661, val r2: 0.862527\n",
      "epoch: 303, train loss: 0.001979, val loss: 0.005493, val mae: 0.051896, val r2: 0.861175\n",
      "epoch: 304, train loss: 0.001988, val loss: 0.005502, val mae: 0.051966, val r2: 0.860850\n",
      "epoch: 305, train loss: 0.001986, val loss: 0.005497, val mae: 0.051826, val r2: 0.860891\n",
      "epoch: 306, train loss: 0.001982, val loss: 0.005544, val mae: 0.051986, val r2: 0.859583\n",
      "epoch: 307, train loss: 0.001978, val loss: 0.005661, val mae: 0.052636, val r2: 0.856426\n",
      "epoch: 308, train loss: 0.001986, val loss: 0.005795, val mae: 0.053395, val r2: 0.852805\n",
      "epoch: 309, train loss: 0.001992, val loss: 0.006077, val mae: 0.054740, val r2: 0.845438\n",
      "epoch: 310, train loss: 0.001997, val loss: 0.006117, val mae: 0.055114, val r2: 0.844416\n",
      "epoch: 311, train loss: 0.002007, val loss: 0.006199, val mae: 0.055633, val r2: 0.842244\n",
      "epoch: 312, train loss: 0.002013, val loss: 0.006077, val mae: 0.055026, val r2: 0.845219\n",
      "epoch: 313, train loss: 0.002016, val loss: 0.005902, val mae: 0.054283, val r2: 0.849692\n",
      "epoch: 314, train loss: 0.002006, val loss: 0.005723, val mae: 0.053492, val r2: 0.854406\n",
      "epoch: 315, train loss: 0.002001, val loss: 0.005566, val mae: 0.052509, val r2: 0.858657\n",
      "epoch: 316, train loss: 0.001992, val loss: 0.005382, val mae: 0.051423, val r2: 0.863852\n",
      "epoch: 317, train loss: 0.001990, val loss: 0.005279, val mae: 0.050889, val r2: 0.866666\n",
      "epoch: 318, train loss: 0.002000, val loss: 0.005257, val mae: 0.050470, val r2: 0.867531\n",
      "epoch: 319, train loss: 0.002008, val loss: 0.005297, val mae: 0.050559, val r2: 0.866681\n",
      "epoch: 320, train loss: 0.002005, val loss: 0.005430, val mae: 0.051345, val r2: 0.863307\n",
      "epoch: 321, train loss: 0.001997, val loss: 0.005440, val mae: 0.051370, val r2: 0.863142\n",
      "epoch: 322, train loss: 0.001989, val loss: 0.005396, val mae: 0.051119, val r2: 0.864128\n",
      "epoch: 323, train loss: 0.001979, val loss: 0.005471, val mae: 0.051544, val r2: 0.862070\n",
      "epoch: 324, train loss: 0.001971, val loss: 0.005514, val mae: 0.051685, val r2: 0.860997\n",
      "epoch: 325, train loss: 0.001956, val loss: 0.005724, val mae: 0.052853, val r2: 0.855635\n",
      "epoch: 326, train loss: 0.001949, val loss: 0.005870, val mae: 0.053562, val r2: 0.851767\n",
      "epoch: 327, train loss: 0.001929, val loss: 0.006178, val mae: 0.055052, val r2: 0.843825\n",
      "epoch: 328, train loss: 0.001916, val loss: 0.006166, val mae: 0.054976, val r2: 0.844002\n",
      "epoch: 329, train loss: 0.001916, val loss: 0.006384, val mae: 0.055985, val r2: 0.838030\n",
      "epoch: 330, train loss: 0.001912, val loss: 0.006554, val mae: 0.056825, val r2: 0.833301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 331, train loss: 0.001917, val loss: 0.006591, val mae: 0.056979, val r2: 0.831900\n",
      "epoch: 332, train loss: 0.001920, val loss: 0.006805, val mae: 0.058077, val r2: 0.826167\n",
      "epoch: 333, train loss: 0.001924, val loss: 0.006814, val mae: 0.058283, val r2: 0.825549\n",
      "epoch: 334, train loss: 0.001932, val loss: 0.006701, val mae: 0.057798, val r2: 0.828537\n",
      "epoch: 335, train loss: 0.001926, val loss: 0.006478, val mae: 0.056881, val r2: 0.834534\n",
      "epoch: 336, train loss: 0.001927, val loss: 0.006073, val mae: 0.054870, val r2: 0.845226\n",
      "epoch: 337, train loss: 0.001912, val loss: 0.005834, val mae: 0.053737, val r2: 0.851890\n",
      "epoch: 338, train loss: 0.001898, val loss: 0.005570, val mae: 0.052188, val r2: 0.858968\n",
      "epoch: 339, train loss: 0.001889, val loss: 0.005455, val mae: 0.051508, val r2: 0.862068\n",
      "epoch: 340, train loss: 0.001893, val loss: 0.005420, val mae: 0.051222, val r2: 0.863273\n",
      "epoch: 341, train loss: 0.001898, val loss: 0.005481, val mae: 0.051573, val r2: 0.861859\n",
      "epoch: 342, train loss: 0.001893, val loss: 0.005573, val mae: 0.051979, val r2: 0.859793\n",
      "epoch: 343, train loss: 0.001879, val loss: 0.005658, val mae: 0.052583, val r2: 0.857591\n",
      "epoch: 344, train loss: 0.001871, val loss: 0.005733, val mae: 0.052860, val r2: 0.855588\n",
      "epoch: 345, train loss: 0.001874, val loss: 0.005839, val mae: 0.053418, val r2: 0.852650\n",
      "epoch: 346, train loss: 0.001875, val loss: 0.006027, val mae: 0.054300, val r2: 0.847820\n",
      "epoch: 347, train loss: 0.001873, val loss: 0.006229, val mae: 0.055222, val r2: 0.842681\n",
      "epoch: 348, train loss: 0.001868, val loss: 0.006549, val mae: 0.056721, val r2: 0.834405\n",
      "epoch: 349, train loss: 0.001861, val loss: 0.006904, val mae: 0.058402, val r2: 0.825093\n",
      "epoch: 350, train loss: 0.001859, val loss: 0.007004, val mae: 0.059030, val r2: 0.821861\n",
      "epoch: 351, train loss: 0.001854, val loss: 0.007389, val mae: 0.060905, val r2: 0.811689\n",
      "epoch: 352, train loss: 0.001846, val loss: 0.007499, val mae: 0.061596, val r2: 0.808567\n",
      "epoch: 353, train loss: 0.001842, val loss: 0.007379, val mae: 0.061108, val r2: 0.811794\n",
      "epoch: 354, train loss: 0.001854, val loss: 0.006813, val mae: 0.058598, val r2: 0.826451\n",
      "epoch: 355, train loss: 0.001844, val loss: 0.006432, val mae: 0.056770, val r2: 0.836524\n",
      "epoch: 356, train loss: 0.001839, val loss: 0.006238, val mae: 0.055732, val r2: 0.841756\n",
      "epoch: 357, train loss: 0.001829, val loss: 0.005985, val mae: 0.054378, val r2: 0.848386\n",
      "epoch: 358, train loss: 0.001821, val loss: 0.005902, val mae: 0.054028, val r2: 0.850832\n",
      "epoch: 359, train loss: 0.001818, val loss: 0.005755, val mae: 0.053195, val r2: 0.854853\n",
      "epoch: 360, train loss: 0.001814, val loss: 0.005681, val mae: 0.052855, val r2: 0.856996\n",
      "epoch: 361, train loss: 0.001806, val loss: 0.005754, val mae: 0.053339, val r2: 0.855425\n",
      "epoch: 362, train loss: 0.001803, val loss: 0.005688, val mae: 0.052966, val r2: 0.857016\n",
      "epoch: 363, train loss: 0.001797, val loss: 0.005643, val mae: 0.052624, val r2: 0.858221\n",
      "epoch: 364, train loss: 0.001792, val loss: 0.005742, val mae: 0.053105, val r2: 0.855715\n",
      "epoch: 365, train loss: 0.001793, val loss: 0.005726, val mae: 0.053051, val r2: 0.856085\n",
      "epoch: 366, train loss: 0.001795, val loss: 0.005795, val mae: 0.053495, val r2: 0.854067\n",
      "epoch: 367, train loss: 0.001801, val loss: 0.005835, val mae: 0.053713, val r2: 0.852891\n",
      "epoch: 368, train loss: 0.001808, val loss: 0.006070, val mae: 0.054992, val r2: 0.846800\n",
      "epoch: 369, train loss: 0.001812, val loss: 0.006348, val mae: 0.056333, val r2: 0.839405\n",
      "epoch: 370, train loss: 0.001801, val loss: 0.006755, val mae: 0.058403, val r2: 0.828896\n",
      "epoch: 371, train loss: 0.001790, val loss: 0.007028, val mae: 0.059662, val r2: 0.821711\n",
      "epoch: 372, train loss: 0.001787, val loss: 0.006885, val mae: 0.059007, val r2: 0.825364\n",
      "epoch: 373, train loss: 0.001785, val loss: 0.006631, val mae: 0.057814, val r2: 0.831852\n",
      "epoch: 374, train loss: 0.001785, val loss: 0.006169, val mae: 0.055582, val r2: 0.843918\n",
      "epoch: 375, train loss: 0.001777, val loss: 0.005975, val mae: 0.054560, val r2: 0.848865\n",
      "epoch: 376, train loss: 0.001772, val loss: 0.005830, val mae: 0.053747, val r2: 0.852647\n",
      "epoch: 377, train loss: 0.001765, val loss: 0.005795, val mae: 0.053529, val r2: 0.853613\n",
      "epoch: 378, train loss: 0.001769, val loss: 0.005783, val mae: 0.053507, val r2: 0.854076\n",
      "epoch: 379, train loss: 0.001764, val loss: 0.005737, val mae: 0.053271, val r2: 0.855395\n",
      "epoch: 380, train loss: 0.001759, val loss: 0.005570, val mae: 0.052395, val r2: 0.859911\n",
      "epoch: 381, train loss: 0.001757, val loss: 0.005427, val mae: 0.051470, val r2: 0.863625\n",
      "epoch: 382, train loss: 0.001749, val loss: 0.005385, val mae: 0.051149, val r2: 0.864698\n",
      "epoch: 383, train loss: 0.001741, val loss: 0.005315, val mae: 0.050743, val r2: 0.866528\n",
      "epoch: 384, train loss: 0.001729, val loss: 0.005303, val mae: 0.050595, val r2: 0.866805\n",
      "epoch: 385, train loss: 0.001731, val loss: 0.005291, val mae: 0.050614, val r2: 0.866967\n",
      "epoch: 386, train loss: 0.001733, val loss: 0.005309, val mae: 0.050772, val r2: 0.866528\n",
      "epoch: 387, train loss: 0.001735, val loss: 0.005445, val mae: 0.051594, val r2: 0.862998\n",
      "epoch: 388, train loss: 0.001732, val loss: 0.005652, val mae: 0.052762, val r2: 0.857685\n",
      "epoch: 389, train loss: 0.001718, val loss: 0.005768, val mae: 0.053417, val r2: 0.854662\n",
      "epoch: 390, train loss: 0.001711, val loss: 0.005837, val mae: 0.053861, val r2: 0.852877\n",
      "epoch: 391, train loss: 0.001707, val loss: 0.005781, val mae: 0.053648, val r2: 0.854239\n",
      "epoch: 392, train loss: 0.001705, val loss: 0.005756, val mae: 0.053625, val r2: 0.854902\n",
      "epoch: 393, train loss: 0.001700, val loss: 0.005576, val mae: 0.052625, val r2: 0.859627\n",
      "epoch: 394, train loss: 0.001696, val loss: 0.005536, val mae: 0.052354, val r2: 0.860518\n",
      "epoch: 395, train loss: 0.001691, val loss: 0.005443, val mae: 0.051775, val r2: 0.862841\n",
      "epoch: 396, train loss: 0.001689, val loss: 0.005376, val mae: 0.051282, val r2: 0.864626\n",
      "epoch: 397, train loss: 0.001692, val loss: 0.005361, val mae: 0.051223, val r2: 0.864918\n",
      "epoch: 398, train loss: 0.001695, val loss: 0.005347, val mae: 0.051168, val r2: 0.865330\n",
      "epoch: 399, train loss: 0.001697, val loss: 0.005328, val mae: 0.051156, val r2: 0.865904\n",
      "epoch: 400, train loss: 0.001697, val loss: 0.005403, val mae: 0.051660, val r2: 0.864140\n",
      "epoch: 401, train loss: 0.001695, val loss: 0.005409, val mae: 0.051594, val r2: 0.863978\n",
      "epoch: 402, train loss: 0.001693, val loss: 0.005409, val mae: 0.051641, val r2: 0.864033\n",
      "epoch: 403, train loss: 0.001694, val loss: 0.005332, val mae: 0.051055, val r2: 0.865917\n",
      "epoch: 404, train loss: 0.001689, val loss: 0.005288, val mae: 0.050735, val r2: 0.867068\n",
      "epoch: 405, train loss: 0.001681, val loss: 0.005300, val mae: 0.050522, val r2: 0.866690\n",
      "epoch: 406, train loss: 0.001675, val loss: 0.005307, val mae: 0.050486, val r2: 0.866496\n",
      "epoch: 407, train loss: 0.001691, val loss: 0.005314, val mae: 0.050534, val r2: 0.866331\n",
      "epoch: 408, train loss: 0.001702, val loss: 0.005358, val mae: 0.050890, val r2: 0.865025\n",
      "epoch: 409, train loss: 0.001710, val loss: 0.005643, val mae: 0.052579, val r2: 0.857483\n",
      "epoch: 410, train loss: 0.001702, val loss: 0.005923, val mae: 0.054109, val r2: 0.850120\n",
      "epoch: 411, train loss: 0.001705, val loss: 0.005732, val mae: 0.053184, val r2: 0.855225\n",
      "epoch: 412, train loss: 0.001716, val loss: 0.005393, val mae: 0.051309, val r2: 0.864224\n",
      "epoch: 413, train loss: 0.001704, val loss: 0.005226, val mae: 0.050273, val r2: 0.868688\n",
      "epoch: 414, train loss: 0.001683, val loss: 0.005218, val mae: 0.049988, val r2: 0.869010\n",
      "epoch: 415, train loss: 0.001676, val loss: 0.005263, val mae: 0.050064, val r2: 0.867753\n",
      "epoch: 416, train loss: 0.001685, val loss: 0.005245, val mae: 0.050100, val r2: 0.868140\n",
      "epoch: 417, train loss: 0.001678, val loss: 0.005236, val mae: 0.050284, val r2: 0.868494\n",
      "epoch: 418, train loss: 0.001662, val loss: 0.005282, val mae: 0.050628, val r2: 0.867302\n",
      "epoch: 419, train loss: 0.001661, val loss: 0.005247, val mae: 0.050592, val r2: 0.868146\n",
      "epoch: 420, train loss: 0.001659, val loss: 0.005248, val mae: 0.050439, val r2: 0.868059\n",
      "epoch: 421, train loss: 0.001657, val loss: 0.005264, val mae: 0.050423, val r2: 0.867689\n",
      "epoch: 422, train loss: 0.001653, val loss: 0.005260, val mae: 0.050415, val r2: 0.867625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 423, train loss: 0.001653, val loss: 0.005377, val mae: 0.051200, val r2: 0.864495\n",
      "epoch: 424, train loss: 0.001644, val loss: 0.005557, val mae: 0.052327, val r2: 0.859717\n",
      "epoch: 425, train loss: 0.001641, val loss: 0.005746, val mae: 0.053458, val r2: 0.854621\n",
      "epoch: 426, train loss: 0.001636, val loss: 0.005813, val mae: 0.053921, val r2: 0.852760\n",
      "epoch: 427, train loss: 0.001640, val loss: 0.005793, val mae: 0.053729, val r2: 0.853241\n",
      "epoch: 428, train loss: 0.001640, val loss: 0.005706, val mae: 0.053332, val r2: 0.855486\n",
      "epoch: 429, train loss: 0.001644, val loss: 0.005589, val mae: 0.052580, val r2: 0.858631\n",
      "epoch: 430, train loss: 0.001648, val loss: 0.005503, val mae: 0.051908, val r2: 0.860909\n",
      "epoch: 431, train loss: 0.001642, val loss: 0.005427, val mae: 0.051421, val r2: 0.862923\n",
      "epoch: 432, train loss: 0.001637, val loss: 0.005414, val mae: 0.051345, val r2: 0.863352\n",
      "epoch: 433, train loss: 0.001637, val loss: 0.005364, val mae: 0.050994, val r2: 0.864679\n",
      "epoch: 434, train loss: 0.001635, val loss: 0.005339, val mae: 0.050923, val r2: 0.865337\n",
      "epoch: 435, train loss: 0.001629, val loss: 0.005329, val mae: 0.050886, val r2: 0.865669\n",
      "epoch: 436, train loss: 0.001623, val loss: 0.005336, val mae: 0.050992, val r2: 0.865431\n",
      "epoch: 437, train loss: 0.001620, val loss: 0.005375, val mae: 0.051095, val r2: 0.864446\n",
      "epoch: 438, train loss: 0.001617, val loss: 0.005403, val mae: 0.051213, val r2: 0.863680\n",
      "epoch: 439, train loss: 0.001611, val loss: 0.005398, val mae: 0.051147, val r2: 0.863794\n",
      "epoch: 440, train loss: 0.001606, val loss: 0.005445, val mae: 0.051415, val r2: 0.862645\n",
      "epoch: 441, train loss: 0.001602, val loss: 0.005485, val mae: 0.051676, val r2: 0.861676\n",
      "epoch: 442, train loss: 0.001605, val loss: 0.005564, val mae: 0.052322, val r2: 0.859649\n",
      "epoch: 443, train loss: 0.001596, val loss: 0.005694, val mae: 0.053115, val r2: 0.856250\n",
      "epoch: 444, train loss: 0.001594, val loss: 0.005835, val mae: 0.053987, val r2: 0.852663\n",
      "epoch: 445, train loss: 0.001594, val loss: 0.005821, val mae: 0.053943, val r2: 0.852793\n",
      "epoch: 446, train loss: 0.001594, val loss: 0.005904, val mae: 0.054416, val r2: 0.850704\n",
      "epoch: 447, train loss: 0.001594, val loss: 0.005814, val mae: 0.053898, val r2: 0.852919\n",
      "epoch: 448, train loss: 0.001592, val loss: 0.005887, val mae: 0.054225, val r2: 0.850977\n",
      "epoch: 449, train loss: 0.001591, val loss: 0.005796, val mae: 0.053787, val r2: 0.853174\n",
      "epoch: 450, train loss: 0.001592, val loss: 0.005897, val mae: 0.054339, val r2: 0.850341\n",
      "epoch: 451, train loss: 0.001594, val loss: 0.005876, val mae: 0.054110, val r2: 0.850826\n",
      "epoch: 452, train loss: 0.001594, val loss: 0.006039, val mae: 0.054959, val r2: 0.846807\n",
      "epoch: 453, train loss: 0.001588, val loss: 0.006029, val mae: 0.054649, val r2: 0.846979\n",
      "epoch: 454, train loss: 0.001584, val loss: 0.005930, val mae: 0.053944, val r2: 0.849674\n",
      "epoch: 455, train loss: 0.001580, val loss: 0.005898, val mae: 0.053739, val r2: 0.850641\n",
      "epoch: 456, train loss: 0.001582, val loss: 0.005880, val mae: 0.053599, val r2: 0.851157\n",
      "epoch: 457, train loss: 0.001584, val loss: 0.005774, val mae: 0.053035, val r2: 0.853879\n",
      "epoch: 458, train loss: 0.001581, val loss: 0.005573, val mae: 0.051935, val r2: 0.859097\n",
      "epoch: 459, train loss: 0.001574, val loss: 0.005535, val mae: 0.051794, val r2: 0.860180\n",
      "epoch: 460, train loss: 0.001566, val loss: 0.005567, val mae: 0.051922, val r2: 0.859440\n",
      "epoch: 461, train loss: 0.001560, val loss: 0.005566, val mae: 0.051963, val r2: 0.859498\n",
      "epoch: 462, train loss: 0.001562, val loss: 0.005547, val mae: 0.051960, val r2: 0.860019\n",
      "epoch: 463, train loss: 0.001561, val loss: 0.005698, val mae: 0.052923, val r2: 0.856179\n",
      "epoch: 464, train loss: 0.001555, val loss: 0.005978, val mae: 0.054445, val r2: 0.849179\n",
      "epoch: 465, train loss: 0.001552, val loss: 0.006175, val mae: 0.055624, val r2: 0.843969\n",
      "epoch: 466, train loss: 0.001549, val loss: 0.006368, val mae: 0.056444, val r2: 0.838940\n",
      "epoch: 467, train loss: 0.001560, val loss: 0.006045, val mae: 0.054775, val r2: 0.847089\n",
      "epoch: 468, train loss: 0.001556, val loss: 0.005831, val mae: 0.053748, val r2: 0.852527\n",
      "epoch: 469, train loss: 0.001551, val loss: 0.005603, val mae: 0.052443, val r2: 0.858453\n",
      "epoch: 470, train loss: 0.001551, val loss: 0.005657, val mae: 0.052641, val r2: 0.857042\n",
      "epoch: 471, train loss: 0.001555, val loss: 0.005699, val mae: 0.052751, val r2: 0.855966\n",
      "epoch: 472, train loss: 0.001557, val loss: 0.005800, val mae: 0.053323, val r2: 0.853323\n",
      "epoch: 473, train loss: 0.001550, val loss: 0.005870, val mae: 0.053526, val r2: 0.851907\n",
      "epoch: 474, train loss: 0.001549, val loss: 0.005681, val mae: 0.052589, val r2: 0.856879\n",
      "epoch: 475, train loss: 0.001553, val loss: 0.005565, val mae: 0.051882, val r2: 0.859766\n",
      "epoch: 476, train loss: 0.001548, val loss: 0.005515, val mae: 0.051703, val r2: 0.861191\n",
      "epoch: 477, train loss: 0.001546, val loss: 0.005455, val mae: 0.051349, val r2: 0.862698\n",
      "epoch: 478, train loss: 0.001539, val loss: 0.005413, val mae: 0.051074, val r2: 0.863765\n",
      "epoch: 479, train loss: 0.001536, val loss: 0.005405, val mae: 0.051048, val r2: 0.863957\n",
      "epoch: 480, train loss: 0.001535, val loss: 0.005487, val mae: 0.051629, val r2: 0.861843\n",
      "epoch: 481, train loss: 0.001538, val loss: 0.005719, val mae: 0.053063, val r2: 0.855953\n",
      "epoch: 482, train loss: 0.001539, val loss: 0.006116, val mae: 0.055079, val r2: 0.845710\n",
      "epoch: 483, train loss: 0.001538, val loss: 0.006533, val mae: 0.057066, val r2: 0.834765\n",
      "epoch: 484, train loss: 0.001540, val loss: 0.006443, val mae: 0.056582, val r2: 0.836478\n",
      "epoch: 485, train loss: 0.001548, val loss: 0.006135, val mae: 0.054986, val r2: 0.844540\n",
      "epoch: 486, train loss: 0.001546, val loss: 0.005933, val mae: 0.053933, val r2: 0.849642\n",
      "epoch: 487, train loss: 0.001543, val loss: 0.005754, val mae: 0.053053, val r2: 0.854431\n",
      "epoch: 488, train loss: 0.001542, val loss: 0.005659, val mae: 0.052464, val r2: 0.856971\n",
      "epoch: 489, train loss: 0.001539, val loss: 0.005541, val mae: 0.051858, val r2: 0.860107\n",
      "epoch: 490, train loss: 0.001533, val loss: 0.005469, val mae: 0.051387, val r2: 0.862097\n",
      "epoch: 491, train loss: 0.001530, val loss: 0.005448, val mae: 0.051433, val r2: 0.862671\n",
      "epoch: 492, train loss: 0.001523, val loss: 0.005483, val mae: 0.051784, val r2: 0.861930\n",
      "epoch: 493, train loss: 0.001518, val loss: 0.005451, val mae: 0.051654, val r2: 0.862840\n",
      "epoch: 494, train loss: 0.001513, val loss: 0.005566, val mae: 0.052349, val r2: 0.859927\n",
      "epoch: 495, train loss: 0.001523, val loss: 0.005610, val mae: 0.052739, val r2: 0.858715\n",
      "epoch: 496, train loss: 0.001519, val loss: 0.005544, val mae: 0.052257, val r2: 0.860234\n",
      "epoch: 497, train loss: 0.001515, val loss: 0.005528, val mae: 0.052221, val r2: 0.860538\n",
      "epoch: 498, train loss: 0.001521, val loss: 0.005544, val mae: 0.052274, val r2: 0.860073\n",
      "epoch: 499, train loss: 0.001523, val loss: 0.005623, val mae: 0.052784, val r2: 0.857956\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.15, \n",
    "                               'layers': [8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 32,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 2,\n",
    "                      'dropout': 0.2, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 6, \n",
    "                      'epoches': 500,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dce845ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps!\n",
      "Transformer(\n",
      "  (element_embedding_temporal): Linear(in_features=294, out_features=192, bias=True)\n",
      "  (element_embedding_spatial): Linear(in_features=2695, out_features=1760, bias=True)\n",
      "  (pos_embedding): Embedding(6, 32)\n",
      "  (variable_embedding): Embedding(55, 32)\n",
      "  (temporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatial): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatiotemporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (keys): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (queries): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatter): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=11220, out_features=2640, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=2640, out_features=330, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch: 0, train loss: 0.018325, val loss: 0.015361, val mae: 0.097832, val r2: 0.610527\n",
      "epoch: 1, train loss: 0.010644, val loss: 0.013005, val mae: 0.087330, val r2: 0.670078\n",
      "epoch: 2, train loss: 0.009166, val loss: 0.011602, val mae: 0.081370, val r2: 0.705740\n",
      "epoch: 3, train loss: 0.008244, val loss: 0.010748, val mae: 0.077754, val r2: 0.727632\n",
      "epoch: 4, train loss: 0.007602, val loss: 0.010108, val mae: 0.075016, val r2: 0.744059\n",
      "epoch: 5, train loss: 0.007075, val loss: 0.009635, val mae: 0.073002, val r2: 0.756239\n",
      "epoch: 6, train loss: 0.006656, val loss: 0.009276, val mae: 0.071437, val r2: 0.765686\n",
      "epoch: 7, train loss: 0.006297, val loss: 0.008888, val mae: 0.069648, val r2: 0.775825\n",
      "epoch: 8, train loss: 0.005978, val loss: 0.008652, val mae: 0.068457, val r2: 0.782100\n",
      "epoch: 9, train loss: 0.005710, val loss: 0.008392, val mae: 0.067209, val r2: 0.788956\n",
      "epoch: 10, train loss: 0.005471, val loss: 0.008226, val mae: 0.066366, val r2: 0.793409\n",
      "epoch: 11, train loss: 0.005276, val loss: 0.008030, val mae: 0.065336, val r2: 0.798387\n",
      "epoch: 12, train loss: 0.005088, val loss: 0.007920, val mae: 0.064751, val r2: 0.801426\n",
      "epoch: 13, train loss: 0.004922, val loss: 0.007733, val mae: 0.063779, val r2: 0.806245\n",
      "epoch: 14, train loss: 0.004777, val loss: 0.007613, val mae: 0.063103, val r2: 0.809293\n",
      "epoch: 15, train loss: 0.004647, val loss: 0.007565, val mae: 0.062808, val r2: 0.810694\n",
      "epoch: 16, train loss: 0.004529, val loss: 0.007388, val mae: 0.061983, val r2: 0.815188\n",
      "epoch: 17, train loss: 0.004438, val loss: 0.007315, val mae: 0.061589, val r2: 0.817146\n",
      "epoch: 18, train loss: 0.004341, val loss: 0.007273, val mae: 0.061301, val r2: 0.818273\n",
      "epoch: 19, train loss: 0.004259, val loss: 0.007212, val mae: 0.060987, val r2: 0.819927\n",
      "epoch: 20, train loss: 0.004193, val loss: 0.007210, val mae: 0.061007, val r2: 0.820067\n",
      "epoch: 21, train loss: 0.004126, val loss: 0.007135, val mae: 0.060588, val r2: 0.822000\n",
      "epoch: 22, train loss: 0.004078, val loss: 0.007191, val mae: 0.060914, val r2: 0.820741\n",
      "epoch: 23, train loss: 0.004026, val loss: 0.007171, val mae: 0.060748, val r2: 0.821431\n",
      "epoch: 24, train loss: 0.003985, val loss: 0.007154, val mae: 0.060686, val r2: 0.821869\n",
      "epoch: 25, train loss: 0.003942, val loss: 0.007079, val mae: 0.060317, val r2: 0.823764\n",
      "epoch: 26, train loss: 0.003902, val loss: 0.007106, val mae: 0.060492, val r2: 0.823038\n",
      "epoch: 27, train loss: 0.003880, val loss: 0.007135, val mae: 0.060654, val r2: 0.822473\n",
      "epoch: 28, train loss: 0.003860, val loss: 0.007153, val mae: 0.060753, val r2: 0.822034\n",
      "epoch: 29, train loss: 0.003826, val loss: 0.007138, val mae: 0.060629, val r2: 0.822476\n",
      "epoch: 30, train loss: 0.003803, val loss: 0.007033, val mae: 0.060139, val r2: 0.824966\n",
      "epoch: 31, train loss: 0.003798, val loss: 0.007062, val mae: 0.060290, val r2: 0.824361\n",
      "epoch: 32, train loss: 0.003796, val loss: 0.006983, val mae: 0.059876, val r2: 0.826272\n",
      "epoch: 33, train loss: 0.003798, val loss: 0.006909, val mae: 0.059521, val r2: 0.828043\n",
      "epoch: 34, train loss: 0.003821, val loss: 0.006651, val mae: 0.058200, val r2: 0.834272\n",
      "epoch: 35, train loss: 0.003837, val loss: 0.006402, val mae: 0.056840, val r2: 0.840391\n",
      "epoch: 36, train loss: 0.003873, val loss: 0.006164, val mae: 0.055497, val r2: 0.845970\n",
      "epoch: 37, train loss: 0.003893, val loss: 0.006006, val mae: 0.054488, val r2: 0.849678\n",
      "epoch: 38, train loss: 0.003878, val loss: 0.005989, val mae: 0.054166, val r2: 0.849881\n",
      "epoch: 39, train loss: 0.003827, val loss: 0.006036, val mae: 0.054278, val r2: 0.848597\n",
      "epoch: 40, train loss: 0.003711, val loss: 0.006080, val mae: 0.054410, val r2: 0.847485\n",
      "epoch: 41, train loss: 0.003593, val loss: 0.005983, val mae: 0.054017, val r2: 0.849913\n",
      "epoch: 42, train loss: 0.003513, val loss: 0.005956, val mae: 0.053846, val r2: 0.850672\n",
      "epoch: 43, train loss: 0.003457, val loss: 0.005923, val mae: 0.053684, val r2: 0.851520\n",
      "epoch: 44, train loss: 0.003453, val loss: 0.005916, val mae: 0.053641, val r2: 0.851598\n",
      "epoch: 45, train loss: 0.003448, val loss: 0.005899, val mae: 0.053535, val r2: 0.852070\n",
      "epoch: 46, train loss: 0.003457, val loss: 0.005893, val mae: 0.053501, val r2: 0.852189\n",
      "epoch: 47, train loss: 0.003458, val loss: 0.005865, val mae: 0.053381, val r2: 0.852909\n",
      "epoch: 48, train loss: 0.003477, val loss: 0.005870, val mae: 0.053349, val r2: 0.852762\n",
      "epoch: 49, train loss: 0.003480, val loss: 0.005880, val mae: 0.053447, val r2: 0.852560\n",
      "epoch: 50, train loss: 0.003489, val loss: 0.005882, val mae: 0.053496, val r2: 0.852629\n",
      "epoch: 51, train loss: 0.003482, val loss: 0.005871, val mae: 0.053494, val r2: 0.853045\n",
      "epoch: 52, train loss: 0.003479, val loss: 0.005901, val mae: 0.053739, val r2: 0.852463\n",
      "epoch: 53, train loss: 0.003451, val loss: 0.005956, val mae: 0.054051, val r2: 0.851222\n",
      "epoch: 54, train loss: 0.003397, val loss: 0.005978, val mae: 0.054224, val r2: 0.850832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55, train loss: 0.003336, val loss: 0.006024, val mae: 0.054462, val r2: 0.849891\n",
      "epoch: 56, train loss: 0.003270, val loss: 0.006012, val mae: 0.054412, val r2: 0.850198\n",
      "epoch: 57, train loss: 0.003229, val loss: 0.005988, val mae: 0.054287, val r2: 0.850940\n",
      "epoch: 58, train loss: 0.003210, val loss: 0.006049, val mae: 0.054672, val r2: 0.849517\n",
      "epoch: 59, train loss: 0.003212, val loss: 0.006000, val mae: 0.054475, val r2: 0.850741\n",
      "epoch: 60, train loss: 0.003234, val loss: 0.006004, val mae: 0.054554, val r2: 0.850686\n",
      "epoch: 61, train loss: 0.003251, val loss: 0.005935, val mae: 0.054205, val r2: 0.852278\n",
      "epoch: 62, train loss: 0.003279, val loss: 0.005838, val mae: 0.053679, val r2: 0.854509\n",
      "epoch: 63, train loss: 0.003313, val loss: 0.005694, val mae: 0.052769, val r2: 0.857908\n",
      "epoch: 64, train loss: 0.003330, val loss: 0.005629, val mae: 0.052305, val r2: 0.859224\n",
      "epoch: 65, train loss: 0.003344, val loss: 0.005657, val mae: 0.052223, val r2: 0.858181\n",
      "epoch: 66, train loss: 0.003296, val loss: 0.005758, val mae: 0.052599, val r2: 0.855529\n",
      "epoch: 67, train loss: 0.003205, val loss: 0.005789, val mae: 0.052675, val r2: 0.854741\n",
      "epoch: 68, train loss: 0.003102, val loss: 0.005758, val mae: 0.052570, val r2: 0.855481\n",
      "epoch: 69, train loss: 0.003043, val loss: 0.005721, val mae: 0.052414, val r2: 0.856429\n",
      "epoch: 70, train loss: 0.003048, val loss: 0.005688, val mae: 0.052313, val r2: 0.857237\n",
      "epoch: 71, train loss: 0.003070, val loss: 0.005694, val mae: 0.052305, val r2: 0.857078\n",
      "epoch: 72, train loss: 0.003092, val loss: 0.005680, val mae: 0.052249, val r2: 0.857422\n",
      "epoch: 73, train loss: 0.003101, val loss: 0.005641, val mae: 0.052080, val r2: 0.858440\n",
      "epoch: 74, train loss: 0.003103, val loss: 0.005644, val mae: 0.052167, val r2: 0.858433\n",
      "epoch: 75, train loss: 0.003078, val loss: 0.005629, val mae: 0.052119, val r2: 0.858974\n",
      "epoch: 76, train loss: 0.003037, val loss: 0.005639, val mae: 0.052246, val r2: 0.858956\n",
      "epoch: 77, train loss: 0.003002, val loss: 0.005644, val mae: 0.052341, val r2: 0.858913\n",
      "epoch: 78, train loss: 0.002964, val loss: 0.005650, val mae: 0.052350, val r2: 0.858722\n",
      "epoch: 79, train loss: 0.002958, val loss: 0.005622, val mae: 0.052256, val r2: 0.859623\n",
      "epoch: 80, train loss: 0.002971, val loss: 0.005617, val mae: 0.052288, val r2: 0.859834\n",
      "epoch: 81, train loss: 0.002998, val loss: 0.005620, val mae: 0.052271, val r2: 0.859760\n",
      "epoch: 82, train loss: 0.003031, val loss: 0.005567, val mae: 0.051992, val r2: 0.860957\n",
      "epoch: 83, train loss: 0.003064, val loss: 0.005508, val mae: 0.051577, val r2: 0.862238\n",
      "epoch: 84, train loss: 0.003076, val loss: 0.005498, val mae: 0.051339, val r2: 0.862179\n",
      "epoch: 85, train loss: 0.003066, val loss: 0.005637, val mae: 0.051872, val r2: 0.858475\n",
      "epoch: 86, train loss: 0.003004, val loss: 0.005723, val mae: 0.052228, val r2: 0.856184\n",
      "epoch: 87, train loss: 0.002934, val loss: 0.005747, val mae: 0.052351, val r2: 0.855594\n",
      "epoch: 88, train loss: 0.002894, val loss: 0.005707, val mae: 0.052210, val r2: 0.856474\n",
      "epoch: 89, train loss: 0.002894, val loss: 0.005680, val mae: 0.052203, val r2: 0.857108\n",
      "epoch: 90, train loss: 0.002912, val loss: 0.005666, val mae: 0.052179, val r2: 0.857446\n",
      "epoch: 91, train loss: 0.002905, val loss: 0.005647, val mae: 0.052130, val r2: 0.858001\n",
      "epoch: 92, train loss: 0.002882, val loss: 0.005618, val mae: 0.052105, val r2: 0.858955\n",
      "epoch: 93, train loss: 0.002854, val loss: 0.005620, val mae: 0.052083, val r2: 0.859011\n",
      "epoch: 94, train loss: 0.002836, val loss: 0.005613, val mae: 0.052073, val r2: 0.859314\n",
      "epoch: 95, train loss: 0.002829, val loss: 0.005602, val mae: 0.051998, val r2: 0.859531\n",
      "epoch: 96, train loss: 0.002838, val loss: 0.005574, val mae: 0.051879, val r2: 0.860307\n",
      "epoch: 97, train loss: 0.002854, val loss: 0.005561, val mae: 0.051731, val r2: 0.860462\n",
      "epoch: 98, train loss: 0.002866, val loss: 0.005570, val mae: 0.051753, val r2: 0.860115\n",
      "epoch: 99, train loss: 0.002863, val loss: 0.005599, val mae: 0.051798, val r2: 0.859266\n",
      "epoch: 100, train loss: 0.002854, val loss: 0.005620, val mae: 0.051872, val r2: 0.858680\n",
      "epoch: 101, train loss: 0.002843, val loss: 0.005713, val mae: 0.052363, val r2: 0.856148\n",
      "epoch: 102, train loss: 0.002846, val loss: 0.005736, val mae: 0.052583, val r2: 0.855458\n",
      "epoch: 103, train loss: 0.002865, val loss: 0.005782, val mae: 0.052897, val r2: 0.854154\n",
      "epoch: 104, train loss: 0.002876, val loss: 0.005861, val mae: 0.053432, val r2: 0.852185\n",
      "epoch: 105, train loss: 0.002881, val loss: 0.005992, val mae: 0.054159, val r2: 0.848888\n",
      "epoch: 106, train loss: 0.002860, val loss: 0.006114, val mae: 0.054854, val r2: 0.845746\n",
      "epoch: 107, train loss: 0.002863, val loss: 0.006117, val mae: 0.054903, val r2: 0.845641\n",
      "epoch: 108, train loss: 0.002854, val loss: 0.006190, val mae: 0.055282, val r2: 0.843620\n",
      "epoch: 109, train loss: 0.002878, val loss: 0.006281, val mae: 0.055811, val r2: 0.841085\n",
      "epoch: 110, train loss: 0.002906, val loss: 0.006390, val mae: 0.056438, val r2: 0.837975\n",
      "epoch: 111, train loss: 0.002933, val loss: 0.006573, val mae: 0.057447, val r2: 0.833032\n",
      "epoch: 112, train loss: 0.002961, val loss: 0.006980, val mae: 0.059530, val r2: 0.821953\n",
      "epoch: 113, train loss: 0.002992, val loss: 0.007337, val mae: 0.061400, val r2: 0.812582\n",
      "epoch: 114, train loss: 0.003011, val loss: 0.007823, val mae: 0.063713, val r2: 0.799449\n",
      "epoch: 115, train loss: 0.003056, val loss: 0.008390, val mae: 0.066366, val r2: 0.784414\n",
      "epoch: 116, train loss: 0.003073, val loss: 0.008196, val mae: 0.065549, val r2: 0.788884\n",
      "epoch: 117, train loss: 0.003127, val loss: 0.008323, val mae: 0.066140, val r2: 0.785205\n",
      "epoch: 118, train loss: 0.003194, val loss: 0.008227, val mae: 0.065767, val r2: 0.787628\n",
      "epoch: 119, train loss: 0.003272, val loss: 0.007961, val mae: 0.064605, val r2: 0.794702\n",
      "epoch: 120, train loss: 0.003327, val loss: 0.007469, val mae: 0.062207, val r2: 0.807681\n",
      "epoch: 121, train loss: 0.003320, val loss: 0.006695, val mae: 0.058346, val r2: 0.828650\n",
      "epoch: 122, train loss: 0.003235, val loss: 0.006137, val mae: 0.055415, val r2: 0.843951\n",
      "epoch: 123, train loss: 0.003121, val loss: 0.005807, val mae: 0.053569, val r2: 0.852948\n",
      "epoch: 124, train loss: 0.002980, val loss: 0.005630, val mae: 0.052577, val r2: 0.857847\n",
      "epoch: 125, train loss: 0.002894, val loss: 0.005524, val mae: 0.052033, val r2: 0.860822\n",
      "epoch: 126, train loss: 0.002831, val loss: 0.005431, val mae: 0.051507, val r2: 0.863366\n",
      "epoch: 127, train loss: 0.002795, val loss: 0.005394, val mae: 0.051333, val r2: 0.864460\n",
      "epoch: 128, train loss: 0.002785, val loss: 0.005362, val mae: 0.051209, val r2: 0.865407\n",
      "epoch: 129, train loss: 0.002777, val loss: 0.005302, val mae: 0.050874, val r2: 0.866958\n",
      "epoch: 130, train loss: 0.002757, val loss: 0.005257, val mae: 0.050607, val r2: 0.868200\n",
      "epoch: 131, train loss: 0.002735, val loss: 0.005254, val mae: 0.050594, val r2: 0.868316\n",
      "epoch: 132, train loss: 0.002724, val loss: 0.005214, val mae: 0.050354, val r2: 0.869357\n",
      "epoch: 133, train loss: 0.002699, val loss: 0.005230, val mae: 0.050455, val r2: 0.868973\n",
      "epoch: 134, train loss: 0.002670, val loss: 0.005168, val mae: 0.050055, val r2: 0.870556\n",
      "epoch: 135, train loss: 0.002637, val loss: 0.005166, val mae: 0.050033, val r2: 0.870607\n",
      "epoch: 136, train loss: 0.002606, val loss: 0.005161, val mae: 0.050042, val r2: 0.870734\n",
      "epoch: 137, train loss: 0.002578, val loss: 0.005167, val mae: 0.050134, val r2: 0.870597\n",
      "epoch: 138, train loss: 0.002553, val loss: 0.005147, val mae: 0.049991, val r2: 0.871131\n",
      "epoch: 139, train loss: 0.002522, val loss: 0.005145, val mae: 0.049954, val r2: 0.871131\n",
      "epoch: 140, train loss: 0.002498, val loss: 0.005130, val mae: 0.049944, val r2: 0.871467\n",
      "epoch: 141, train loss: 0.002478, val loss: 0.005150, val mae: 0.050023, val r2: 0.870976\n",
      "epoch: 142, train loss: 0.002454, val loss: 0.005148, val mae: 0.050021, val r2: 0.870960\n",
      "epoch: 143, train loss: 0.002437, val loss: 0.005188, val mae: 0.050323, val r2: 0.869975\n",
      "epoch: 144, train loss: 0.002424, val loss: 0.005197, val mae: 0.050393, val r2: 0.869627\n",
      "epoch: 145, train loss: 0.002407, val loss: 0.005214, val mae: 0.050472, val r2: 0.869185\n",
      "epoch: 146, train loss: 0.002400, val loss: 0.005241, val mae: 0.050684, val r2: 0.868447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 147, train loss: 0.002398, val loss: 0.005262, val mae: 0.050812, val r2: 0.867769\n",
      "epoch: 148, train loss: 0.002393, val loss: 0.005343, val mae: 0.051247, val r2: 0.865662\n",
      "epoch: 149, train loss: 0.002391, val loss: 0.005439, val mae: 0.051876, val r2: 0.863046\n",
      "epoch: 150, train loss: 0.002393, val loss: 0.005474, val mae: 0.052048, val r2: 0.862061\n",
      "epoch: 151, train loss: 0.002396, val loss: 0.005516, val mae: 0.052228, val r2: 0.860785\n",
      "epoch: 152, train loss: 0.002396, val loss: 0.005678, val mae: 0.053181, val r2: 0.856388\n",
      "epoch: 153, train loss: 0.002395, val loss: 0.005697, val mae: 0.053226, val r2: 0.855836\n",
      "epoch: 154, train loss: 0.002405, val loss: 0.005864, val mae: 0.054150, val r2: 0.851194\n",
      "epoch: 155, train loss: 0.002418, val loss: 0.005931, val mae: 0.054472, val r2: 0.849187\n",
      "epoch: 156, train loss: 0.002438, val loss: 0.005995, val mae: 0.054729, val r2: 0.847423\n",
      "epoch: 157, train loss: 0.002458, val loss: 0.006305, val mae: 0.056372, val r2: 0.838923\n",
      "epoch: 158, train loss: 0.002481, val loss: 0.006675, val mae: 0.058172, val r2: 0.828858\n",
      "epoch: 159, train loss: 0.002507, val loss: 0.007468, val mae: 0.061876, val r2: 0.807833\n",
      "epoch: 160, train loss: 0.002525, val loss: 0.008289, val mae: 0.065492, val r2: 0.786026\n",
      "epoch: 161, train loss: 0.002526, val loss: 0.008730, val mae: 0.067387, val r2: 0.774535\n",
      "epoch: 162, train loss: 0.002525, val loss: 0.008504, val mae: 0.066337, val r2: 0.780377\n",
      "epoch: 163, train loss: 0.002550, val loss: 0.007025, val mae: 0.059817, val r2: 0.819265\n",
      "epoch: 164, train loss: 0.002564, val loss: 0.006503, val mae: 0.057421, val r2: 0.833015\n",
      "epoch: 165, train loss: 0.002535, val loss: 0.006534, val mae: 0.057655, val r2: 0.832454\n",
      "epoch: 166, train loss: 0.002529, val loss: 0.006633, val mae: 0.058108, val r2: 0.830142\n",
      "epoch: 167, train loss: 0.002499, val loss: 0.006134, val mae: 0.055640, val r2: 0.843521\n",
      "epoch: 168, train loss: 0.002475, val loss: 0.005646, val mae: 0.052987, val r2: 0.856837\n",
      "epoch: 169, train loss: 0.002456, val loss: 0.005309, val mae: 0.051050, val r2: 0.866165\n",
      "epoch: 170, train loss: 0.002440, val loss: 0.005192, val mae: 0.050226, val r2: 0.869447\n",
      "epoch: 171, train loss: 0.002442, val loss: 0.005120, val mae: 0.049716, val r2: 0.871451\n",
      "epoch: 172, train loss: 0.002449, val loss: 0.005097, val mae: 0.049486, val r2: 0.872053\n",
      "epoch: 173, train loss: 0.002457, val loss: 0.005095, val mae: 0.049342, val r2: 0.872036\n",
      "epoch: 174, train loss: 0.002457, val loss: 0.005124, val mae: 0.049514, val r2: 0.871261\n",
      "epoch: 175, train loss: 0.002460, val loss: 0.005130, val mae: 0.049539, val r2: 0.871128\n",
      "epoch: 176, train loss: 0.002485, val loss: 0.005135, val mae: 0.049688, val r2: 0.871024\n",
      "epoch: 177, train loss: 0.002489, val loss: 0.005111, val mae: 0.049580, val r2: 0.871723\n",
      "epoch: 178, train loss: 0.002481, val loss: 0.005127, val mae: 0.049625, val r2: 0.871272\n",
      "epoch: 179, train loss: 0.002472, val loss: 0.005126, val mae: 0.049633, val r2: 0.871312\n",
      "epoch: 180, train loss: 0.002454, val loss: 0.005130, val mae: 0.049719, val r2: 0.871306\n",
      "epoch: 181, train loss: 0.002436, val loss: 0.005138, val mae: 0.049786, val r2: 0.871088\n",
      "epoch: 182, train loss: 0.002398, val loss: 0.005146, val mae: 0.049897, val r2: 0.870936\n",
      "epoch: 183, train loss: 0.002355, val loss: 0.005174, val mae: 0.050066, val r2: 0.870246\n",
      "epoch: 184, train loss: 0.002312, val loss: 0.005175, val mae: 0.050068, val r2: 0.870195\n",
      "epoch: 185, train loss: 0.002266, val loss: 0.005181, val mae: 0.050184, val r2: 0.869918\n",
      "epoch: 186, train loss: 0.002235, val loss: 0.005211, val mae: 0.050416, val r2: 0.869112\n",
      "epoch: 187, train loss: 0.002213, val loss: 0.005295, val mae: 0.050913, val r2: 0.866827\n",
      "epoch: 188, train loss: 0.002208, val loss: 0.005353, val mae: 0.051312, val r2: 0.865223\n",
      "epoch: 189, train loss: 0.002207, val loss: 0.005426, val mae: 0.051741, val r2: 0.863087\n",
      "epoch: 190, train loss: 0.002205, val loss: 0.005570, val mae: 0.052535, val r2: 0.859281\n",
      "epoch: 191, train loss: 0.002210, val loss: 0.005664, val mae: 0.053024, val r2: 0.856797\n",
      "epoch: 192, train loss: 0.002211, val loss: 0.005773, val mae: 0.053624, val r2: 0.853616\n",
      "epoch: 193, train loss: 0.002206, val loss: 0.005947, val mae: 0.054515, val r2: 0.848992\n",
      "epoch: 194, train loss: 0.002204, val loss: 0.005971, val mae: 0.054680, val r2: 0.848072\n",
      "epoch: 195, train loss: 0.002203, val loss: 0.006064, val mae: 0.055134, val r2: 0.845470\n",
      "epoch: 196, train loss: 0.002195, val loss: 0.006111, val mae: 0.055363, val r2: 0.844253\n",
      "epoch: 197, train loss: 0.002190, val loss: 0.006161, val mae: 0.055583, val r2: 0.842714\n",
      "epoch: 198, train loss: 0.002183, val loss: 0.006304, val mae: 0.056324, val r2: 0.838909\n",
      "epoch: 199, train loss: 0.002177, val loss: 0.006365, val mae: 0.056606, val r2: 0.837258\n",
      "epoch: 200, train loss: 0.002168, val loss: 0.006363, val mae: 0.056673, val r2: 0.837499\n",
      "epoch: 201, train loss: 0.002163, val loss: 0.006338, val mae: 0.056527, val r2: 0.838422\n",
      "epoch: 202, train loss: 0.002165, val loss: 0.006040, val mae: 0.055073, val r2: 0.846509\n",
      "epoch: 203, train loss: 0.002163, val loss: 0.005750, val mae: 0.053515, val r2: 0.854452\n",
      "epoch: 204, train loss: 0.002175, val loss: 0.005446, val mae: 0.051812, val r2: 0.862736\n",
      "epoch: 205, train loss: 0.002186, val loss: 0.005187, val mae: 0.050177, val r2: 0.869623\n",
      "epoch: 206, train loss: 0.002194, val loss: 0.005126, val mae: 0.049579, val r2: 0.871306\n",
      "epoch: 207, train loss: 0.002189, val loss: 0.005166, val mae: 0.049500, val r2: 0.870305\n",
      "epoch: 208, train loss: 0.002188, val loss: 0.005238, val mae: 0.049821, val r2: 0.868430\n",
      "epoch: 209, train loss: 0.002188, val loss: 0.005256, val mae: 0.049895, val r2: 0.867890\n",
      "epoch: 210, train loss: 0.002185, val loss: 0.005196, val mae: 0.049680, val r2: 0.869435\n",
      "epoch: 211, train loss: 0.002171, val loss: 0.005178, val mae: 0.049712, val r2: 0.869918\n",
      "epoch: 212, train loss: 0.002152, val loss: 0.005163, val mae: 0.049729, val r2: 0.870455\n",
      "epoch: 213, train loss: 0.002135, val loss: 0.005138, val mae: 0.049686, val r2: 0.871091\n",
      "epoch: 214, train loss: 0.002115, val loss: 0.005134, val mae: 0.049632, val r2: 0.871263\n",
      "epoch: 215, train loss: 0.002103, val loss: 0.005163, val mae: 0.049800, val r2: 0.870536\n",
      "epoch: 216, train loss: 0.002086, val loss: 0.005153, val mae: 0.049813, val r2: 0.870763\n",
      "epoch: 217, train loss: 0.002069, val loss: 0.005199, val mae: 0.049978, val r2: 0.869498\n",
      "epoch: 218, train loss: 0.002059, val loss: 0.005222, val mae: 0.050078, val r2: 0.868864\n",
      "epoch: 219, train loss: 0.002060, val loss: 0.005234, val mae: 0.050145, val r2: 0.868417\n",
      "epoch: 220, train loss: 0.002061, val loss: 0.005285, val mae: 0.050607, val r2: 0.866978\n",
      "epoch: 221, train loss: 0.002057, val loss: 0.005473, val mae: 0.051738, val r2: 0.861947\n",
      "epoch: 222, train loss: 0.002052, val loss: 0.005714, val mae: 0.053206, val r2: 0.855541\n",
      "epoch: 223, train loss: 0.002047, val loss: 0.006033, val mae: 0.054913, val r2: 0.846969\n",
      "epoch: 224, train loss: 0.002058, val loss: 0.006464, val mae: 0.057069, val r2: 0.835339\n",
      "epoch: 225, train loss: 0.002064, val loss: 0.006561, val mae: 0.057436, val r2: 0.832569\n",
      "epoch: 226, train loss: 0.002080, val loss: 0.006388, val mae: 0.056627, val r2: 0.837413\n",
      "epoch: 227, train loss: 0.002092, val loss: 0.006071, val mae: 0.055053, val r2: 0.845756\n",
      "epoch: 228, train loss: 0.002092, val loss: 0.005707, val mae: 0.053123, val r2: 0.855398\n",
      "epoch: 229, train loss: 0.002075, val loss: 0.005467, val mae: 0.051794, val r2: 0.862079\n",
      "epoch: 230, train loss: 0.002065, val loss: 0.005352, val mae: 0.051031, val r2: 0.865255\n",
      "epoch: 231, train loss: 0.002074, val loss: 0.005393, val mae: 0.051191, val r2: 0.864116\n",
      "epoch: 232, train loss: 0.002089, val loss: 0.005393, val mae: 0.051185, val r2: 0.864155\n",
      "epoch: 233, train loss: 0.002081, val loss: 0.005329, val mae: 0.050897, val r2: 0.866016\n",
      "epoch: 234, train loss: 0.002065, val loss: 0.005248, val mae: 0.050520, val r2: 0.868184\n",
      "epoch: 235, train loss: 0.002040, val loss: 0.005175, val mae: 0.049940, val r2: 0.870211\n",
      "epoch: 236, train loss: 0.002018, val loss: 0.005198, val mae: 0.049761, val r2: 0.869621\n",
      "epoch: 237, train loss: 0.002007, val loss: 0.005236, val mae: 0.049804, val r2: 0.868676\n",
      "epoch: 238, train loss: 0.002012, val loss: 0.005211, val mae: 0.049874, val r2: 0.869323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 239, train loss: 0.002004, val loss: 0.005234, val mae: 0.050269, val r2: 0.868639\n",
      "epoch: 240, train loss: 0.001968, val loss: 0.005287, val mae: 0.050736, val r2: 0.867191\n",
      "epoch: 241, train loss: 0.001937, val loss: 0.005343, val mae: 0.051121, val r2: 0.865735\n",
      "epoch: 242, train loss: 0.001933, val loss: 0.005416, val mae: 0.051577, val r2: 0.863677\n",
      "epoch: 243, train loss: 0.001930, val loss: 0.005396, val mae: 0.051552, val r2: 0.864001\n",
      "epoch: 244, train loss: 0.001926, val loss: 0.005343, val mae: 0.051187, val r2: 0.865363\n",
      "epoch: 245, train loss: 0.001915, val loss: 0.005320, val mae: 0.051041, val r2: 0.865891\n",
      "epoch: 246, train loss: 0.001906, val loss: 0.005315, val mae: 0.050978, val r2: 0.865995\n",
      "epoch: 247, train loss: 0.001908, val loss: 0.005362, val mae: 0.051121, val r2: 0.864721\n",
      "epoch: 248, train loss: 0.001913, val loss: 0.005424, val mae: 0.051469, val r2: 0.863115\n",
      "epoch: 249, train loss: 0.001911, val loss: 0.005435, val mae: 0.051641, val r2: 0.862915\n",
      "epoch: 250, train loss: 0.001920, val loss: 0.005511, val mae: 0.052201, val r2: 0.860858\n",
      "epoch: 251, train loss: 0.001921, val loss: 0.005407, val mae: 0.051627, val r2: 0.863783\n",
      "epoch: 252, train loss: 0.001922, val loss: 0.005286, val mae: 0.050854, val r2: 0.867145\n",
      "epoch: 253, train loss: 0.001919, val loss: 0.005222, val mae: 0.050385, val r2: 0.869080\n",
      "epoch: 254, train loss: 0.001905, val loss: 0.005249, val mae: 0.050096, val r2: 0.868501\n",
      "epoch: 255, train loss: 0.001891, val loss: 0.005363, val mae: 0.050304, val r2: 0.865591\n",
      "epoch: 256, train loss: 0.001919, val loss: 0.005347, val mae: 0.050329, val r2: 0.865773\n",
      "epoch: 257, train loss: 0.001944, val loss: 0.005339, val mae: 0.050577, val r2: 0.865897\n",
      "epoch: 258, train loss: 0.001911, val loss: 0.005336, val mae: 0.050756, val r2: 0.865980\n",
      "epoch: 259, train loss: 0.001866, val loss: 0.005361, val mae: 0.051113, val r2: 0.865398\n",
      "epoch: 260, train loss: 0.001862, val loss: 0.005383, val mae: 0.051374, val r2: 0.864705\n",
      "epoch: 261, train loss: 0.001873, val loss: 0.005280, val mae: 0.050741, val r2: 0.867378\n",
      "epoch: 262, train loss: 0.001857, val loss: 0.005206, val mae: 0.050251, val r2: 0.869385\n",
      "epoch: 263, train loss: 0.001829, val loss: 0.005209, val mae: 0.050112, val r2: 0.869278\n",
      "epoch: 264, train loss: 0.001824, val loss: 0.005283, val mae: 0.050495, val r2: 0.867318\n",
      "epoch: 265, train loss: 0.001822, val loss: 0.005364, val mae: 0.050933, val r2: 0.865086\n",
      "epoch: 266, train loss: 0.001809, val loss: 0.005391, val mae: 0.051157, val r2: 0.864326\n",
      "epoch: 267, train loss: 0.001805, val loss: 0.005441, val mae: 0.051517, val r2: 0.862966\n",
      "epoch: 268, train loss: 0.001804, val loss: 0.005399, val mae: 0.051465, val r2: 0.864032\n",
      "epoch: 269, train loss: 0.001809, val loss: 0.005393, val mae: 0.051478, val r2: 0.864166\n",
      "epoch: 270, train loss: 0.001807, val loss: 0.005391, val mae: 0.051501, val r2: 0.864414\n",
      "epoch: 271, train loss: 0.001808, val loss: 0.005299, val mae: 0.050958, val r2: 0.866829\n",
      "epoch: 272, train loss: 0.001798, val loss: 0.005286, val mae: 0.050603, val r2: 0.867364\n",
      "epoch: 273, train loss: 0.001790, val loss: 0.005361, val mae: 0.050863, val r2: 0.865381\n",
      "epoch: 274, train loss: 0.001786, val loss: 0.005412, val mae: 0.050993, val r2: 0.864005\n",
      "epoch: 275, train loss: 0.001788, val loss: 0.005408, val mae: 0.051032, val r2: 0.864009\n",
      "epoch: 276, train loss: 0.001795, val loss: 0.005371, val mae: 0.050853, val r2: 0.864952\n",
      "epoch: 277, train loss: 0.001786, val loss: 0.005384, val mae: 0.051168, val r2: 0.864625\n",
      "epoch: 278, train loss: 0.001779, val loss: 0.005445, val mae: 0.051633, val r2: 0.863056\n",
      "epoch: 279, train loss: 0.001768, val loss: 0.005562, val mae: 0.052360, val r2: 0.860118\n",
      "epoch: 280, train loss: 0.001774, val loss: 0.005476, val mae: 0.051917, val r2: 0.862257\n",
      "epoch: 281, train loss: 0.001783, val loss: 0.005343, val mae: 0.051147, val r2: 0.865635\n",
      "epoch: 282, train loss: 0.001779, val loss: 0.005262, val mae: 0.050625, val r2: 0.867975\n",
      "epoch: 283, train loss: 0.001759, val loss: 0.005319, val mae: 0.050642, val r2: 0.866592\n",
      "epoch: 284, train loss: 0.001740, val loss: 0.005351, val mae: 0.050618, val r2: 0.865765\n",
      "epoch: 285, train loss: 0.001743, val loss: 0.005362, val mae: 0.050696, val r2: 0.865373\n",
      "epoch: 286, train loss: 0.001761, val loss: 0.005403, val mae: 0.051094, val r2: 0.864312\n",
      "epoch: 287, train loss: 0.001738, val loss: 0.005464, val mae: 0.051593, val r2: 0.862809\n",
      "epoch: 288, train loss: 0.001716, val loss: 0.005564, val mae: 0.052257, val r2: 0.860125\n",
      "epoch: 289, train loss: 0.001709, val loss: 0.005606, val mae: 0.052490, val r2: 0.859069\n",
      "epoch: 290, train loss: 0.001709, val loss: 0.005688, val mae: 0.052969, val r2: 0.856868\n",
      "epoch: 291, train loss: 0.001719, val loss: 0.005528, val mae: 0.052104, val r2: 0.860833\n",
      "epoch: 292, train loss: 0.001720, val loss: 0.005430, val mae: 0.051492, val r2: 0.863498\n",
      "epoch: 293, train loss: 0.001713, val loss: 0.005326, val mae: 0.050828, val r2: 0.866217\n",
      "epoch: 294, train loss: 0.001707, val loss: 0.005309, val mae: 0.050574, val r2: 0.866798\n",
      "epoch: 295, train loss: 0.001701, val loss: 0.005308, val mae: 0.050651, val r2: 0.866774\n",
      "epoch: 296, train loss: 0.001710, val loss: 0.005393, val mae: 0.051192, val r2: 0.864407\n",
      "epoch: 297, train loss: 0.001706, val loss: 0.005498, val mae: 0.051988, val r2: 0.861699\n",
      "epoch: 298, train loss: 0.001701, val loss: 0.005689, val mae: 0.053151, val r2: 0.856640\n",
      "epoch: 299, train loss: 0.001703, val loss: 0.005882, val mae: 0.054135, val r2: 0.851519\n",
      "epoch: 300, train loss: 0.001706, val loss: 0.005964, val mae: 0.054567, val r2: 0.849600\n",
      "epoch: 301, train loss: 0.001705, val loss: 0.006045, val mae: 0.054873, val r2: 0.847651\n",
      "epoch: 302, train loss: 0.001709, val loss: 0.005916, val mae: 0.054132, val r2: 0.850995\n",
      "epoch: 303, train loss: 0.001716, val loss: 0.005609, val mae: 0.052656, val r2: 0.858851\n",
      "epoch: 304, train loss: 0.001712, val loss: 0.005472, val mae: 0.051865, val r2: 0.862431\n",
      "epoch: 305, train loss: 0.001698, val loss: 0.005449, val mae: 0.051506, val r2: 0.863007\n",
      "epoch: 306, train loss: 0.001688, val loss: 0.005483, val mae: 0.051683, val r2: 0.862105\n",
      "epoch: 307, train loss: 0.001708, val loss: 0.005601, val mae: 0.052277, val r2: 0.858978\n",
      "epoch: 308, train loss: 0.001713, val loss: 0.005823, val mae: 0.053477, val r2: 0.853383\n",
      "epoch: 309, train loss: 0.001703, val loss: 0.006056, val mae: 0.054584, val r2: 0.847636\n",
      "epoch: 310, train loss: 0.001694, val loss: 0.006083, val mae: 0.054900, val r2: 0.847135\n",
      "epoch: 311, train loss: 0.001706, val loss: 0.005920, val mae: 0.054225, val r2: 0.851066\n",
      "epoch: 312, train loss: 0.001724, val loss: 0.005812, val mae: 0.053709, val r2: 0.853712\n",
      "epoch: 313, train loss: 0.001722, val loss: 0.005660, val mae: 0.052749, val r2: 0.857386\n",
      "epoch: 314, train loss: 0.001700, val loss: 0.005855, val mae: 0.053584, val r2: 0.852240\n",
      "epoch: 315, train loss: 0.001678, val loss: 0.006128, val mae: 0.054887, val r2: 0.845294\n",
      "epoch: 316, train loss: 0.001677, val loss: 0.006260, val mae: 0.055528, val r2: 0.842103\n",
      "epoch: 317, train loss: 0.001669, val loss: 0.006336, val mae: 0.056007, val r2: 0.840186\n",
      "epoch: 318, train loss: 0.001661, val loss: 0.006430, val mae: 0.056387, val r2: 0.837898\n",
      "epoch: 319, train loss: 0.001658, val loss: 0.006453, val mae: 0.056571, val r2: 0.837137\n",
      "epoch: 320, train loss: 0.001662, val loss: 0.006360, val mae: 0.056203, val r2: 0.839197\n",
      "epoch: 321, train loss: 0.001660, val loss: 0.006255, val mae: 0.055598, val r2: 0.841825\n",
      "epoch: 322, train loss: 0.001654, val loss: 0.006169, val mae: 0.055267, val r2: 0.843861\n",
      "epoch: 323, train loss: 0.001648, val loss: 0.006327, val mae: 0.055994, val r2: 0.839836\n",
      "epoch: 324, train loss: 0.001640, val loss: 0.006433, val mae: 0.056681, val r2: 0.837224\n",
      "epoch: 325, train loss: 0.001635, val loss: 0.006471, val mae: 0.056920, val r2: 0.836230\n",
      "epoch: 326, train loss: 0.001634, val loss: 0.006500, val mae: 0.057126, val r2: 0.835443\n",
      "epoch: 327, train loss: 0.001637, val loss: 0.006347, val mae: 0.056396, val r2: 0.839289\n",
      "epoch: 328, train loss: 0.001638, val loss: 0.006147, val mae: 0.055310, val r2: 0.844686\n",
      "epoch: 329, train loss: 0.001641, val loss: 0.005949, val mae: 0.054212, val r2: 0.849957\n",
      "epoch: 330, train loss: 0.001642, val loss: 0.005895, val mae: 0.053928, val r2: 0.851510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 331, train loss: 0.001643, val loss: 0.005741, val mae: 0.053123, val r2: 0.855561\n",
      "epoch: 332, train loss: 0.001641, val loss: 0.005835, val mae: 0.053581, val r2: 0.853361\n",
      "epoch: 333, train loss: 0.001641, val loss: 0.005839, val mae: 0.053565, val r2: 0.853365\n",
      "epoch: 334, train loss: 0.001643, val loss: 0.005839, val mae: 0.053498, val r2: 0.853661\n",
      "epoch: 335, train loss: 0.001644, val loss: 0.005865, val mae: 0.053499, val r2: 0.853113\n",
      "epoch: 336, train loss: 0.001651, val loss: 0.005871, val mae: 0.053439, val r2: 0.852823\n",
      "epoch: 337, train loss: 0.001652, val loss: 0.005756, val mae: 0.052737, val r2: 0.855632\n",
      "epoch: 338, train loss: 0.001661, val loss: 0.005750, val mae: 0.052545, val r2: 0.855603\n",
      "epoch: 339, train loss: 0.001679, val loss: 0.005872, val mae: 0.053152, val r2: 0.852445\n",
      "epoch: 340, train loss: 0.001696, val loss: 0.006116, val mae: 0.054454, val r2: 0.846251\n",
      "epoch: 341, train loss: 0.001696, val loss: 0.006470, val mae: 0.056064, val r2: 0.837531\n",
      "epoch: 342, train loss: 0.001667, val loss: 0.006816, val mae: 0.057909, val r2: 0.828632\n",
      "epoch: 343, train loss: 0.001642, val loss: 0.006694, val mae: 0.057404, val r2: 0.831477\n",
      "epoch: 344, train loss: 0.001657, val loss: 0.006136, val mae: 0.054737, val r2: 0.845096\n",
      "epoch: 345, train loss: 0.001648, val loss: 0.005875, val mae: 0.053393, val r2: 0.851895\n",
      "epoch: 346, train loss: 0.001637, val loss: 0.005822, val mae: 0.053161, val r2: 0.853081\n",
      "epoch: 347, train loss: 0.001635, val loss: 0.005935, val mae: 0.053790, val r2: 0.850419\n",
      "epoch: 348, train loss: 0.001628, val loss: 0.006149, val mae: 0.055095, val r2: 0.845128\n",
      "epoch: 349, train loss: 0.001609, val loss: 0.005974, val mae: 0.054183, val r2: 0.849560\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 32,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 2,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 6, \n",
    "                      'epoches': 350,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42f8c067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps!\n",
      "Transformer(\n",
      "  (element_embedding_temporal): Linear(in_features=294, out_features=192, bias=True)\n",
      "  (element_embedding_spatial): Linear(in_features=2695, out_features=1760, bias=True)\n",
      "  (pos_embedding): Embedding(6, 32)\n",
      "  (variable_embedding): Embedding(55, 32)\n",
      "  (temporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatial): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatiotemporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (keys): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (queries): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatter): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=11220, out_features=2640, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=2640, out_features=330, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch: 0, train loss: 0.017911, val loss: 0.015112, val mae: 0.096152, val r2: 0.617302\n",
      "epoch: 1, train loss: 0.010686, val loss: 0.012697, val mae: 0.085737, val r2: 0.678556\n",
      "epoch: 2, train loss: 0.009175, val loss: 0.011499, val mae: 0.080711, val r2: 0.708718\n",
      "epoch: 3, train loss: 0.008268, val loss: 0.010584, val mae: 0.076956, val r2: 0.731921\n",
      "epoch: 4, train loss: 0.007606, val loss: 0.010008, val mae: 0.074543, val r2: 0.746769\n",
      "epoch: 5, train loss: 0.007088, val loss: 0.009604, val mae: 0.072879, val r2: 0.757185\n",
      "epoch: 6, train loss: 0.006664, val loss: 0.009239, val mae: 0.071349, val r2: 0.766784\n",
      "epoch: 7, train loss: 0.006298, val loss: 0.008875, val mae: 0.069664, val r2: 0.776186\n",
      "epoch: 8, train loss: 0.005983, val loss: 0.008589, val mae: 0.068309, val r2: 0.783666\n",
      "epoch: 9, train loss: 0.005708, val loss: 0.008379, val mae: 0.067278, val r2: 0.789146\n",
      "epoch: 10, train loss: 0.005466, val loss: 0.008117, val mae: 0.065988, val r2: 0.795941\n",
      "epoch: 11, train loss: 0.005261, val loss: 0.007967, val mae: 0.065167, val r2: 0.799887\n",
      "epoch: 12, train loss: 0.005087, val loss: 0.007847, val mae: 0.064535, val r2: 0.803009\n",
      "epoch: 13, train loss: 0.004932, val loss: 0.007709, val mae: 0.063804, val r2: 0.806625\n",
      "epoch: 14, train loss: 0.004789, val loss: 0.007620, val mae: 0.063247, val r2: 0.809024\n",
      "epoch: 15, train loss: 0.004665, val loss: 0.007473, val mae: 0.062494, val r2: 0.812870\n",
      "epoch: 16, train loss: 0.004540, val loss: 0.007412, val mae: 0.062105, val r2: 0.814380\n",
      "epoch: 17, train loss: 0.004448, val loss: 0.007324, val mae: 0.061638, val r2: 0.816810\n",
      "epoch: 18, train loss: 0.004356, val loss: 0.007276, val mae: 0.061403, val r2: 0.818012\n",
      "epoch: 19, train loss: 0.004277, val loss: 0.007280, val mae: 0.061347, val r2: 0.818129\n",
      "epoch: 20, train loss: 0.004200, val loss: 0.007276, val mae: 0.061292, val r2: 0.818377\n",
      "epoch: 21, train loss: 0.004142, val loss: 0.007207, val mae: 0.060896, val r2: 0.820178\n",
      "epoch: 22, train loss: 0.004078, val loss: 0.007201, val mae: 0.060877, val r2: 0.820439\n",
      "epoch: 23, train loss: 0.004031, val loss: 0.007276, val mae: 0.061222, val r2: 0.818743\n",
      "epoch: 24, train loss: 0.003983, val loss: 0.007194, val mae: 0.060859, val r2: 0.820827\n",
      "epoch: 25, train loss: 0.003944, val loss: 0.007160, val mae: 0.060624, val r2: 0.821624\n",
      "epoch: 26, train loss: 0.003899, val loss: 0.007145, val mae: 0.060595, val r2: 0.822037\n",
      "epoch: 27, train loss: 0.003869, val loss: 0.007173, val mae: 0.060718, val r2: 0.821454\n",
      "epoch: 28, train loss: 0.003839, val loss: 0.007080, val mae: 0.060315, val r2: 0.823693\n",
      "epoch: 29, train loss: 0.003810, val loss: 0.007116, val mae: 0.060450, val r2: 0.822865\n",
      "epoch: 30, train loss: 0.003795, val loss: 0.007164, val mae: 0.060631, val r2: 0.821822\n",
      "epoch: 31, train loss: 0.003784, val loss: 0.007123, val mae: 0.060431, val r2: 0.822739\n",
      "epoch: 32, train loss: 0.003768, val loss: 0.006998, val mae: 0.059814, val r2: 0.825792\n",
      "epoch: 33, train loss: 0.003759, val loss: 0.007004, val mae: 0.059828, val r2: 0.825611\n",
      "epoch: 34, train loss: 0.003773, val loss: 0.006901, val mae: 0.059353, val r2: 0.828174\n",
      "epoch: 35, train loss: 0.003801, val loss: 0.006741, val mae: 0.058474, val r2: 0.832050\n",
      "epoch: 36, train loss: 0.003828, val loss: 0.006457, val mae: 0.056928, val r2: 0.838910\n",
      "epoch: 37, train loss: 0.003866, val loss: 0.006179, val mae: 0.055380, val r2: 0.845513\n",
      "epoch: 38, train loss: 0.003860, val loss: 0.006084, val mae: 0.054642, val r2: 0.847685\n",
      "epoch: 39, train loss: 0.003850, val loss: 0.006087, val mae: 0.054433, val r2: 0.847434\n",
      "epoch: 40, train loss: 0.003752, val loss: 0.006104, val mae: 0.054438, val r2: 0.846893\n",
      "epoch: 41, train loss: 0.003631, val loss: 0.006138, val mae: 0.054472, val r2: 0.846064\n",
      "epoch: 42, train loss: 0.003521, val loss: 0.006050, val mae: 0.054119, val r2: 0.848286\n",
      "epoch: 43, train loss: 0.003469, val loss: 0.006023, val mae: 0.053977, val r2: 0.848928\n",
      "epoch: 44, train loss: 0.003443, val loss: 0.005983, val mae: 0.053787, val r2: 0.849984\n",
      "epoch: 45, train loss: 0.003456, val loss: 0.005979, val mae: 0.053728, val r2: 0.850046\n",
      "epoch: 46, train loss: 0.003464, val loss: 0.005957, val mae: 0.053625, val r2: 0.850575\n",
      "epoch: 47, train loss: 0.003473, val loss: 0.005948, val mae: 0.053624, val r2: 0.850748\n",
      "epoch: 48, train loss: 0.003490, val loss: 0.005937, val mae: 0.053559, val r2: 0.851102\n",
      "epoch: 49, train loss: 0.003498, val loss: 0.005939, val mae: 0.053559, val r2: 0.851050\n",
      "epoch: 50, train loss: 0.003500, val loss: 0.005925, val mae: 0.053582, val r2: 0.851532\n",
      "epoch: 51, train loss: 0.003492, val loss: 0.005941, val mae: 0.053701, val r2: 0.851286\n",
      "epoch: 52, train loss: 0.003454, val loss: 0.005980, val mae: 0.053948, val r2: 0.850420\n",
      "epoch: 53, train loss: 0.003416, val loss: 0.006041, val mae: 0.054297, val r2: 0.849203\n",
      "epoch: 54, train loss: 0.003353, val loss: 0.006079, val mae: 0.054575, val r2: 0.848334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55, train loss: 0.003293, val loss: 0.006098, val mae: 0.054631, val r2: 0.847991\n",
      "epoch: 56, train loss: 0.003239, val loss: 0.006116, val mae: 0.054744, val r2: 0.847680\n",
      "epoch: 57, train loss: 0.003217, val loss: 0.006075, val mae: 0.054557, val r2: 0.848700\n",
      "epoch: 58, train loss: 0.003224, val loss: 0.006085, val mae: 0.054615, val r2: 0.848516\n",
      "epoch: 59, train loss: 0.003254, val loss: 0.006062, val mae: 0.054551, val r2: 0.849027\n",
      "epoch: 60, train loss: 0.003292, val loss: 0.005991, val mae: 0.054198, val r2: 0.850670\n",
      "epoch: 61, train loss: 0.003330, val loss: 0.005874, val mae: 0.053469, val r2: 0.853400\n",
      "epoch: 62, train loss: 0.003369, val loss: 0.005768, val mae: 0.052755, val r2: 0.855716\n",
      "epoch: 63, train loss: 0.003354, val loss: 0.005814, val mae: 0.052691, val r2: 0.854303\n",
      "epoch: 64, train loss: 0.003302, val loss: 0.005904, val mae: 0.053040, val r2: 0.851982\n",
      "epoch: 65, train loss: 0.003193, val loss: 0.005943, val mae: 0.053235, val r2: 0.850882\n",
      "epoch: 66, train loss: 0.003103, val loss: 0.005844, val mae: 0.052788, val r2: 0.853451\n",
      "epoch: 67, train loss: 0.003065, val loss: 0.005798, val mae: 0.052563, val r2: 0.854474\n",
      "epoch: 68, train loss: 0.003072, val loss: 0.005814, val mae: 0.052615, val r2: 0.854066\n",
      "epoch: 69, train loss: 0.003092, val loss: 0.005806, val mae: 0.052599, val r2: 0.854144\n",
      "epoch: 70, train loss: 0.003111, val loss: 0.005792, val mae: 0.052503, val r2: 0.854586\n",
      "epoch: 71, train loss: 0.003104, val loss: 0.005731, val mae: 0.052332, val r2: 0.856149\n",
      "epoch: 72, train loss: 0.003096, val loss: 0.005756, val mae: 0.052443, val r2: 0.855578\n",
      "epoch: 73, train loss: 0.003064, val loss: 0.005744, val mae: 0.052475, val r2: 0.856060\n",
      "epoch: 74, train loss: 0.003026, val loss: 0.005730, val mae: 0.052466, val r2: 0.856544\n",
      "epoch: 75, train loss: 0.003002, val loss: 0.005740, val mae: 0.052502, val r2: 0.856488\n",
      "epoch: 76, train loss: 0.002976, val loss: 0.005742, val mae: 0.052541, val r2: 0.856510\n",
      "epoch: 77, train loss: 0.002967, val loss: 0.005739, val mae: 0.052612, val r2: 0.856714\n",
      "epoch: 78, train loss: 0.002988, val loss: 0.005717, val mae: 0.052534, val r2: 0.857341\n",
      "epoch: 79, train loss: 0.003036, val loss: 0.005703, val mae: 0.052448, val r2: 0.857748\n",
      "epoch: 80, train loss: 0.003084, val loss: 0.005623, val mae: 0.051869, val r2: 0.859372\n",
      "epoch: 81, train loss: 0.003129, val loss: 0.005638, val mae: 0.051814, val r2: 0.858713\n",
      "epoch: 82, train loss: 0.003104, val loss: 0.005759, val mae: 0.052172, val r2: 0.855478\n",
      "epoch: 83, train loss: 0.003048, val loss: 0.005937, val mae: 0.053000, val r2: 0.850949\n",
      "epoch: 84, train loss: 0.002978, val loss: 0.005932, val mae: 0.053013, val r2: 0.851003\n",
      "epoch: 85, train loss: 0.002941, val loss: 0.005845, val mae: 0.052684, val r2: 0.853021\n",
      "epoch: 86, train loss: 0.002961, val loss: 0.005826, val mae: 0.052689, val r2: 0.853482\n",
      "epoch: 87, train loss: 0.002971, val loss: 0.005758, val mae: 0.052436, val r2: 0.855352\n",
      "epoch: 88, train loss: 0.002944, val loss: 0.005754, val mae: 0.052488, val r2: 0.855634\n",
      "epoch: 89, train loss: 0.002914, val loss: 0.005740, val mae: 0.052455, val r2: 0.856138\n",
      "epoch: 90, train loss: 0.002876, val loss: 0.005751, val mae: 0.052519, val r2: 0.855931\n",
      "epoch: 91, train loss: 0.002867, val loss: 0.005751, val mae: 0.052472, val r2: 0.856001\n",
      "epoch: 92, train loss: 0.002875, val loss: 0.005714, val mae: 0.052271, val r2: 0.856859\n",
      "epoch: 93, train loss: 0.002893, val loss: 0.005700, val mae: 0.052077, val r2: 0.857134\n",
      "epoch: 94, train loss: 0.002905, val loss: 0.005721, val mae: 0.052132, val r2: 0.856507\n",
      "epoch: 95, train loss: 0.002886, val loss: 0.005724, val mae: 0.052147, val r2: 0.856319\n",
      "epoch: 96, train loss: 0.002877, val loss: 0.005752, val mae: 0.052293, val r2: 0.855556\n",
      "epoch: 97, train loss: 0.002875, val loss: 0.005818, val mae: 0.052702, val r2: 0.853738\n",
      "epoch: 98, train loss: 0.002896, val loss: 0.005855, val mae: 0.052970, val r2: 0.852738\n",
      "epoch: 99, train loss: 0.002904, val loss: 0.005886, val mae: 0.053223, val r2: 0.851979\n",
      "epoch: 100, train loss: 0.002897, val loss: 0.005924, val mae: 0.053558, val r2: 0.851229\n",
      "epoch: 101, train loss: 0.002877, val loss: 0.006015, val mae: 0.054099, val r2: 0.848892\n",
      "epoch: 102, train loss: 0.002867, val loss: 0.006081, val mae: 0.054481, val r2: 0.847342\n",
      "epoch: 103, train loss: 0.002842, val loss: 0.006054, val mae: 0.054405, val r2: 0.848030\n",
      "epoch: 104, train loss: 0.002848, val loss: 0.006072, val mae: 0.054542, val r2: 0.847422\n",
      "epoch: 105, train loss: 0.002852, val loss: 0.006056, val mae: 0.054522, val r2: 0.847652\n",
      "epoch: 106, train loss: 0.002864, val loss: 0.006120, val mae: 0.054900, val r2: 0.845819\n",
      "epoch: 107, train loss: 0.002876, val loss: 0.006205, val mae: 0.055405, val r2: 0.843551\n",
      "epoch: 108, train loss: 0.002882, val loss: 0.006436, val mae: 0.056658, val r2: 0.837533\n",
      "epoch: 109, train loss: 0.002892, val loss: 0.006773, val mae: 0.058419, val r2: 0.828603\n",
      "epoch: 110, train loss: 0.002894, val loss: 0.007095, val mae: 0.060155, val r2: 0.820347\n",
      "epoch: 111, train loss: 0.002889, val loss: 0.007524, val mae: 0.062198, val r2: 0.808952\n",
      "epoch: 112, train loss: 0.002903, val loss: 0.007792, val mae: 0.063536, val r2: 0.801971\n",
      "epoch: 113, train loss: 0.002937, val loss: 0.007609, val mae: 0.062757, val r2: 0.806173\n",
      "epoch: 114, train loss: 0.003031, val loss: 0.007688, val mae: 0.063257, val r2: 0.803423\n",
      "epoch: 115, train loss: 0.003143, val loss: 0.008103, val mae: 0.065165, val r2: 0.792037\n",
      "epoch: 116, train loss: 0.003251, val loss: 0.008215, val mae: 0.065698, val r2: 0.788907\n",
      "epoch: 117, train loss: 0.003331, val loss: 0.007758, val mae: 0.063468, val r2: 0.800825\n",
      "epoch: 118, train loss: 0.003349, val loss: 0.006959, val mae: 0.059632, val r2: 0.822112\n",
      "epoch: 119, train loss: 0.003291, val loss: 0.006217, val mae: 0.055769, val r2: 0.841826\n",
      "epoch: 120, train loss: 0.003173, val loss: 0.005779, val mae: 0.053374, val r2: 0.853778\n",
      "epoch: 121, train loss: 0.003020, val loss: 0.005509, val mae: 0.051875, val r2: 0.861115\n",
      "epoch: 122, train loss: 0.002926, val loss: 0.005372, val mae: 0.051009, val r2: 0.864921\n",
      "epoch: 123, train loss: 0.002851, val loss: 0.005304, val mae: 0.050682, val r2: 0.866760\n",
      "epoch: 124, train loss: 0.002817, val loss: 0.005267, val mae: 0.050481, val r2: 0.867789\n",
      "epoch: 125, train loss: 0.002800, val loss: 0.005211, val mae: 0.050105, val r2: 0.869233\n",
      "epoch: 126, train loss: 0.002781, val loss: 0.005208, val mae: 0.050074, val r2: 0.869344\n",
      "epoch: 127, train loss: 0.002771, val loss: 0.005191, val mae: 0.049989, val r2: 0.869864\n",
      "epoch: 128, train loss: 0.002749, val loss: 0.005195, val mae: 0.050009, val r2: 0.869723\n",
      "epoch: 129, train loss: 0.002722, val loss: 0.005186, val mae: 0.049992, val r2: 0.869950\n",
      "epoch: 130, train loss: 0.002700, val loss: 0.005186, val mae: 0.049999, val r2: 0.870049\n",
      "epoch: 131, train loss: 0.002665, val loss: 0.005188, val mae: 0.050028, val r2: 0.869969\n",
      "epoch: 132, train loss: 0.002632, val loss: 0.005172, val mae: 0.049968, val r2: 0.870356\n",
      "epoch: 133, train loss: 0.002601, val loss: 0.005161, val mae: 0.049955, val r2: 0.870646\n",
      "epoch: 134, train loss: 0.002568, val loss: 0.005167, val mae: 0.049969, val r2: 0.870534\n",
      "epoch: 135, train loss: 0.002540, val loss: 0.005228, val mae: 0.050361, val r2: 0.869016\n",
      "epoch: 136, train loss: 0.002512, val loss: 0.005213, val mae: 0.050303, val r2: 0.869295\n",
      "epoch: 137, train loss: 0.002482, val loss: 0.005302, val mae: 0.050854, val r2: 0.866973\n",
      "epoch: 138, train loss: 0.002464, val loss: 0.005287, val mae: 0.050870, val r2: 0.867268\n",
      "epoch: 139, train loss: 0.002449, val loss: 0.005312, val mae: 0.051029, val r2: 0.866590\n",
      "epoch: 140, train loss: 0.002446, val loss: 0.005338, val mae: 0.051189, val r2: 0.865782\n",
      "epoch: 141, train loss: 0.002439, val loss: 0.005486, val mae: 0.052044, val r2: 0.861834\n",
      "epoch: 142, train loss: 0.002440, val loss: 0.005615, val mae: 0.052807, val r2: 0.858377\n",
      "epoch: 143, train loss: 0.002442, val loss: 0.005665, val mae: 0.053083, val r2: 0.856864\n",
      "epoch: 144, train loss: 0.002446, val loss: 0.005920, val mae: 0.054450, val r2: 0.850039\n",
      "epoch: 145, train loss: 0.002464, val loss: 0.006068, val mae: 0.055290, val r2: 0.846102\n",
      "epoch: 146, train loss: 0.002465, val loss: 0.006274, val mae: 0.056320, val r2: 0.840712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 147, train loss: 0.002486, val loss: 0.006579, val mae: 0.057843, val r2: 0.832318\n",
      "epoch: 148, train loss: 0.002499, val loss: 0.006645, val mae: 0.058165, val r2: 0.830369\n",
      "epoch: 149, train loss: 0.002510, val loss: 0.006927, val mae: 0.059475, val r2: 0.823146\n",
      "epoch: 150, train loss: 0.002520, val loss: 0.007175, val mae: 0.060664, val r2: 0.816349\n",
      "epoch: 151, train loss: 0.002534, val loss: 0.007265, val mae: 0.061019, val r2: 0.814058\n",
      "epoch: 152, train loss: 0.002534, val loss: 0.007403, val mae: 0.061569, val r2: 0.810294\n",
      "epoch: 153, train loss: 0.002528, val loss: 0.007551, val mae: 0.062258, val r2: 0.806399\n",
      "epoch: 154, train loss: 0.002524, val loss: 0.007560, val mae: 0.062232, val r2: 0.806229\n",
      "epoch: 155, train loss: 0.002520, val loss: 0.007163, val mae: 0.060385, val r2: 0.816524\n",
      "epoch: 156, train loss: 0.002512, val loss: 0.006732, val mae: 0.058456, val r2: 0.827661\n",
      "epoch: 157, train loss: 0.002517, val loss: 0.006427, val mae: 0.056972, val r2: 0.835884\n",
      "epoch: 158, train loss: 0.002493, val loss: 0.006123, val mae: 0.055476, val r2: 0.844280\n",
      "epoch: 159, train loss: 0.002482, val loss: 0.005751, val mae: 0.053515, val r2: 0.854353\n",
      "epoch: 160, train loss: 0.002470, val loss: 0.005386, val mae: 0.051395, val r2: 0.864305\n",
      "epoch: 161, train loss: 0.002469, val loss: 0.005189, val mae: 0.050135, val r2: 0.869678\n",
      "epoch: 162, train loss: 0.002469, val loss: 0.005100, val mae: 0.049420, val r2: 0.872032\n",
      "epoch: 163, train loss: 0.002455, val loss: 0.005072, val mae: 0.049152, val r2: 0.872686\n",
      "epoch: 164, train loss: 0.002441, val loss: 0.005078, val mae: 0.049113, val r2: 0.872481\n",
      "epoch: 165, train loss: 0.002433, val loss: 0.005084, val mae: 0.049185, val r2: 0.872309\n",
      "epoch: 166, train loss: 0.002428, val loss: 0.005090, val mae: 0.049264, val r2: 0.872162\n",
      "epoch: 167, train loss: 0.002423, val loss: 0.005088, val mae: 0.049294, val r2: 0.872306\n",
      "epoch: 168, train loss: 0.002415, val loss: 0.005081, val mae: 0.049266, val r2: 0.872471\n",
      "epoch: 169, train loss: 0.002397, val loss: 0.005083, val mae: 0.049238, val r2: 0.872464\n",
      "epoch: 170, train loss: 0.002381, val loss: 0.005068, val mae: 0.049197, val r2: 0.872886\n",
      "epoch: 171, train loss: 0.002370, val loss: 0.005067, val mae: 0.049182, val r2: 0.872880\n",
      "epoch: 172, train loss: 0.002347, val loss: 0.005080, val mae: 0.049425, val r2: 0.872558\n",
      "epoch: 173, train loss: 0.002327, val loss: 0.005106, val mae: 0.049563, val r2: 0.871897\n",
      "epoch: 174, train loss: 0.002300, val loss: 0.005114, val mae: 0.049674, val r2: 0.871660\n",
      "epoch: 175, train loss: 0.002271, val loss: 0.005177, val mae: 0.050097, val r2: 0.870037\n",
      "epoch: 176, train loss: 0.002256, val loss: 0.005262, val mae: 0.050632, val r2: 0.867777\n",
      "epoch: 177, train loss: 0.002244, val loss: 0.005330, val mae: 0.051048, val r2: 0.865899\n",
      "epoch: 178, train loss: 0.002244, val loss: 0.005450, val mae: 0.051769, val r2: 0.862709\n",
      "epoch: 179, train loss: 0.002249, val loss: 0.005602, val mae: 0.052712, val r2: 0.858508\n",
      "epoch: 180, train loss: 0.002264, val loss: 0.005706, val mae: 0.053179, val r2: 0.855505\n",
      "epoch: 181, train loss: 0.002263, val loss: 0.005694, val mae: 0.053101, val r2: 0.855612\n",
      "epoch: 182, train loss: 0.002278, val loss: 0.005863, val mae: 0.054038, val r2: 0.851012\n",
      "epoch: 183, train loss: 0.002282, val loss: 0.006004, val mae: 0.054806, val r2: 0.847144\n",
      "epoch: 184, train loss: 0.002292, val loss: 0.006116, val mae: 0.055324, val r2: 0.844061\n",
      "epoch: 185, train loss: 0.002299, val loss: 0.006490, val mae: 0.057155, val r2: 0.834082\n",
      "epoch: 186, train loss: 0.002297, val loss: 0.006860, val mae: 0.058925, val r2: 0.824231\n",
      "epoch: 187, train loss: 0.002284, val loss: 0.007154, val mae: 0.060393, val r2: 0.816673\n",
      "epoch: 188, train loss: 0.002285, val loss: 0.006841, val mae: 0.058843, val r2: 0.825381\n",
      "epoch: 189, train loss: 0.002306, val loss: 0.005930, val mae: 0.054313, val r2: 0.849611\n",
      "epoch: 190, train loss: 0.002313, val loss: 0.005275, val mae: 0.050824, val r2: 0.867229\n",
      "epoch: 191, train loss: 0.002281, val loss: 0.005100, val mae: 0.049436, val r2: 0.872061\n",
      "epoch: 192, train loss: 0.002268, val loss: 0.005118, val mae: 0.049369, val r2: 0.871574\n",
      "epoch: 193, train loss: 0.002278, val loss: 0.005146, val mae: 0.049593, val r2: 0.870761\n",
      "epoch: 194, train loss: 0.002256, val loss: 0.005148, val mae: 0.049698, val r2: 0.870810\n",
      "epoch: 195, train loss: 0.002217, val loss: 0.005077, val mae: 0.049268, val r2: 0.872648\n",
      "epoch: 196, train loss: 0.002191, val loss: 0.005046, val mae: 0.048977, val r2: 0.873423\n",
      "epoch: 197, train loss: 0.002179, val loss: 0.005024, val mae: 0.048868, val r2: 0.874053\n",
      "epoch: 198, train loss: 0.002162, val loss: 0.005045, val mae: 0.048951, val r2: 0.873505\n",
      "epoch: 199, train loss: 0.002156, val loss: 0.005048, val mae: 0.049063, val r2: 0.873364\n",
      "epoch: 200, train loss: 0.002139, val loss: 0.005107, val mae: 0.049515, val r2: 0.871798\n",
      "epoch: 201, train loss: 0.002124, val loss: 0.005212, val mae: 0.050348, val r2: 0.869060\n",
      "epoch: 202, train loss: 0.002096, val loss: 0.005500, val mae: 0.052113, val r2: 0.861493\n",
      "epoch: 203, train loss: 0.002090, val loss: 0.005749, val mae: 0.053523, val r2: 0.854587\n",
      "epoch: 204, train loss: 0.002103, val loss: 0.005851, val mae: 0.053963, val r2: 0.851579\n",
      "epoch: 205, train loss: 0.002119, val loss: 0.005833, val mae: 0.053873, val r2: 0.851899\n",
      "epoch: 206, train loss: 0.002131, val loss: 0.005751, val mae: 0.053520, val r2: 0.853896\n",
      "epoch: 207, train loss: 0.002131, val loss: 0.005623, val mae: 0.052871, val r2: 0.857326\n",
      "epoch: 208, train loss: 0.002116, val loss: 0.005608, val mae: 0.052734, val r2: 0.857853\n",
      "epoch: 209, train loss: 0.002110, val loss: 0.005631, val mae: 0.052900, val r2: 0.857194\n",
      "epoch: 210, train loss: 0.002113, val loss: 0.005691, val mae: 0.053192, val r2: 0.855674\n",
      "epoch: 211, train loss: 0.002117, val loss: 0.005725, val mae: 0.053408, val r2: 0.854950\n",
      "epoch: 212, train loss: 0.002110, val loss: 0.005453, val mae: 0.051874, val r2: 0.862620\n",
      "epoch: 213, train loss: 0.002120, val loss: 0.005186, val mae: 0.050145, val r2: 0.869708\n",
      "epoch: 214, train loss: 0.002117, val loss: 0.005088, val mae: 0.049324, val r2: 0.872413\n",
      "epoch: 215, train loss: 0.002091, val loss: 0.005209, val mae: 0.049582, val r2: 0.869330\n",
      "epoch: 216, train loss: 0.002080, val loss: 0.005258, val mae: 0.049798, val r2: 0.867995\n",
      "epoch: 217, train loss: 0.002106, val loss: 0.005223, val mae: 0.049765, val r2: 0.868774\n",
      "epoch: 218, train loss: 0.002146, val loss: 0.005165, val mae: 0.049695, val r2: 0.870302\n",
      "epoch: 219, train loss: 0.002113, val loss: 0.005311, val mae: 0.050824, val r2: 0.866681\n",
      "epoch: 220, train loss: 0.002050, val loss: 0.005379, val mae: 0.051363, val r2: 0.864954\n",
      "epoch: 221, train loss: 0.002047, val loss: 0.005320, val mae: 0.051047, val r2: 0.866094\n",
      "epoch: 222, train loss: 0.002074, val loss: 0.005140, val mae: 0.049954, val r2: 0.870865\n",
      "epoch: 223, train loss: 0.002036, val loss: 0.005143, val mae: 0.049768, val r2: 0.870928\n",
      "epoch: 224, train loss: 0.001999, val loss: 0.005229, val mae: 0.050302, val r2: 0.868462\n",
      "epoch: 225, train loss: 0.002009, val loss: 0.005384, val mae: 0.051311, val r2: 0.864299\n",
      "epoch: 226, train loss: 0.001994, val loss: 0.005659, val mae: 0.052937, val r2: 0.857031\n",
      "epoch: 227, train loss: 0.001982, val loss: 0.005757, val mae: 0.053628, val r2: 0.854234\n",
      "epoch: 228, train loss: 0.002012, val loss: 0.005514, val mae: 0.052159, val r2: 0.860656\n",
      "epoch: 229, train loss: 0.002033, val loss: 0.005243, val mae: 0.050630, val r2: 0.868014\n",
      "epoch: 230, train loss: 0.001994, val loss: 0.005171, val mae: 0.049983, val r2: 0.870348\n",
      "epoch: 231, train loss: 0.001959, val loss: 0.005194, val mae: 0.049910, val r2: 0.869692\n",
      "epoch: 232, train loss: 0.001971, val loss: 0.005200, val mae: 0.050137, val r2: 0.869300\n",
      "epoch: 233, train loss: 0.001974, val loss: 0.005256, val mae: 0.050629, val r2: 0.867859\n",
      "epoch: 234, train loss: 0.001953, val loss: 0.005199, val mae: 0.050260, val r2: 0.869455\n",
      "epoch: 235, train loss: 0.001952, val loss: 0.005177, val mae: 0.050055, val r2: 0.870036\n",
      "epoch: 236, train loss: 0.001954, val loss: 0.005081, val mae: 0.049399, val r2: 0.872544\n",
      "epoch: 237, train loss: 0.001935, val loss: 0.005143, val mae: 0.049435, val r2: 0.871143\n",
      "epoch: 238, train loss: 0.001932, val loss: 0.005169, val mae: 0.049494, val r2: 0.870257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 239, train loss: 0.001949, val loss: 0.005159, val mae: 0.049706, val r2: 0.870396\n",
      "epoch: 240, train loss: 0.001936, val loss: 0.005231, val mae: 0.050312, val r2: 0.868488\n",
      "epoch: 241, train loss: 0.001919, val loss: 0.005386, val mae: 0.051385, val r2: 0.864499\n",
      "epoch: 242, train loss: 0.001913, val loss: 0.005508, val mae: 0.052109, val r2: 0.861113\n",
      "epoch: 243, train loss: 0.001933, val loss: 0.005439, val mae: 0.051603, val r2: 0.862734\n",
      "epoch: 244, train loss: 0.001944, val loss: 0.005303, val mae: 0.050830, val r2: 0.866378\n",
      "epoch: 245, train loss: 0.001917, val loss: 0.005236, val mae: 0.050384, val r2: 0.868404\n",
      "epoch: 246, train loss: 0.001901, val loss: 0.005164, val mae: 0.049932, val r2: 0.870205\n",
      "epoch: 247, train loss: 0.001904, val loss: 0.005240, val mae: 0.050581, val r2: 0.868156\n",
      "epoch: 248, train loss: 0.001894, val loss: 0.005345, val mae: 0.051180, val r2: 0.865328\n",
      "epoch: 249, train loss: 0.001889, val loss: 0.005444, val mae: 0.051794, val r2: 0.862680\n",
      "epoch: 250, train loss: 0.001895, val loss: 0.005525, val mae: 0.052208, val r2: 0.860827\n",
      "epoch: 251, train loss: 0.001893, val loss: 0.005422, val mae: 0.051562, val r2: 0.863523\n",
      "epoch: 252, train loss: 0.001903, val loss: 0.005183, val mae: 0.050151, val r2: 0.869787\n",
      "epoch: 253, train loss: 0.001890, val loss: 0.005148, val mae: 0.049699, val r2: 0.870927\n",
      "epoch: 254, train loss: 0.001869, val loss: 0.005197, val mae: 0.049973, val r2: 0.869648\n",
      "epoch: 255, train loss: 0.001870, val loss: 0.005201, val mae: 0.050073, val r2: 0.869372\n",
      "epoch: 256, train loss: 0.001869, val loss: 0.005291, val mae: 0.050752, val r2: 0.867043\n",
      "epoch: 257, train loss: 0.001861, val loss: 0.005429, val mae: 0.051531, val r2: 0.863645\n",
      "epoch: 258, train loss: 0.001855, val loss: 0.005391, val mae: 0.051338, val r2: 0.864623\n",
      "epoch: 259, train loss: 0.001878, val loss: 0.005243, val mae: 0.050576, val r2: 0.868221\n",
      "epoch: 260, train loss: 0.001870, val loss: 0.005137, val mae: 0.049883, val r2: 0.871081\n",
      "epoch: 261, train loss: 0.001839, val loss: 0.005174, val mae: 0.049777, val r2: 0.870199\n",
      "epoch: 262, train loss: 0.001833, val loss: 0.005216, val mae: 0.050072, val r2: 0.868957\n",
      "epoch: 263, train loss: 0.001846, val loss: 0.005330, val mae: 0.050832, val r2: 0.866087\n",
      "epoch: 264, train loss: 0.001819, val loss: 0.005470, val mae: 0.051632, val r2: 0.862559\n",
      "epoch: 265, train loss: 0.001805, val loss: 0.005548, val mae: 0.052205, val r2: 0.860318\n",
      "epoch: 266, train loss: 0.001809, val loss: 0.005557, val mae: 0.052517, val r2: 0.859995\n",
      "epoch: 267, train loss: 0.001809, val loss: 0.005417, val mae: 0.051682, val r2: 0.863444\n",
      "epoch: 268, train loss: 0.001801, val loss: 0.005293, val mae: 0.050848, val r2: 0.866700\n",
      "epoch: 269, train loss: 0.001791, val loss: 0.005264, val mae: 0.050587, val r2: 0.867473\n",
      "epoch: 270, train loss: 0.001791, val loss: 0.005347, val mae: 0.051041, val r2: 0.865386\n",
      "epoch: 271, train loss: 0.001785, val loss: 0.005508, val mae: 0.052018, val r2: 0.861120\n",
      "epoch: 272, train loss: 0.001786, val loss: 0.005593, val mae: 0.052569, val r2: 0.858932\n",
      "epoch: 273, train loss: 0.001791, val loss: 0.005693, val mae: 0.053219, val r2: 0.856435\n",
      "epoch: 274, train loss: 0.001803, val loss: 0.005558, val mae: 0.052571, val r2: 0.860022\n",
      "epoch: 275, train loss: 0.001809, val loss: 0.005385, val mae: 0.051546, val r2: 0.864457\n",
      "epoch: 276, train loss: 0.001804, val loss: 0.005285, val mae: 0.050806, val r2: 0.867039\n",
      "epoch: 277, train loss: 0.001788, val loss: 0.005353, val mae: 0.051137, val r2: 0.865290\n",
      "epoch: 278, train loss: 0.001783, val loss: 0.005409, val mae: 0.051264, val r2: 0.863956\n",
      "epoch: 279, train loss: 0.001785, val loss: 0.005541, val mae: 0.052050, val r2: 0.860609\n",
      "epoch: 280, train loss: 0.001780, val loss: 0.005674, val mae: 0.052741, val r2: 0.857383\n",
      "epoch: 281, train loss: 0.001788, val loss: 0.005755, val mae: 0.053340, val r2: 0.855406\n",
      "epoch: 282, train loss: 0.001810, val loss: 0.005633, val mae: 0.052750, val r2: 0.858192\n",
      "epoch: 283, train loss: 0.001817, val loss: 0.005459, val mae: 0.051743, val r2: 0.862463\n",
      "epoch: 284, train loss: 0.001802, val loss: 0.005464, val mae: 0.051404, val r2: 0.862339\n",
      "epoch: 285, train loss: 0.001798, val loss: 0.005594, val mae: 0.052267, val r2: 0.859040\n",
      "epoch: 286, train loss: 0.001799, val loss: 0.006011, val mae: 0.054379, val r2: 0.848529\n",
      "epoch: 287, train loss: 0.001754, val loss: 0.006370, val mae: 0.056365, val r2: 0.839574\n",
      "epoch: 288, train loss: 0.001736, val loss: 0.006401, val mae: 0.056550, val r2: 0.838134\n",
      "epoch: 289, train loss: 0.001757, val loss: 0.006087, val mae: 0.055050, val r2: 0.845707\n",
      "epoch: 290, train loss: 0.001765, val loss: 0.005860, val mae: 0.053917, val r2: 0.851569\n",
      "epoch: 291, train loss: 0.001752, val loss: 0.005954, val mae: 0.054403, val r2: 0.849073\n",
      "epoch: 292, train loss: 0.001744, val loss: 0.006193, val mae: 0.055812, val r2: 0.842973\n",
      "epoch: 293, train loss: 0.001734, val loss: 0.006397, val mae: 0.056816, val r2: 0.837669\n",
      "epoch: 294, train loss: 0.001734, val loss: 0.006048, val mae: 0.054922, val r2: 0.846998\n",
      "epoch: 295, train loss: 0.001740, val loss: 0.005660, val mae: 0.052718, val r2: 0.857116\n",
      "epoch: 296, train loss: 0.001737, val loss: 0.005387, val mae: 0.051098, val r2: 0.864326\n",
      "epoch: 297, train loss: 0.001750, val loss: 0.005325, val mae: 0.050687, val r2: 0.866218\n",
      "epoch: 298, train loss: 0.001766, val loss: 0.005478, val mae: 0.051522, val r2: 0.862523\n",
      "epoch: 299, train loss: 0.001770, val loss: 0.005567, val mae: 0.051893, val r2: 0.860527\n",
      "epoch: 300, train loss: 0.001781, val loss: 0.005613, val mae: 0.052022, val r2: 0.859589\n",
      "epoch: 301, train loss: 0.001791, val loss: 0.005521, val mae: 0.051493, val r2: 0.861643\n",
      "epoch: 302, train loss: 0.001778, val loss: 0.005362, val mae: 0.050621, val r2: 0.865524\n",
      "epoch: 303, train loss: 0.001758, val loss: 0.005369, val mae: 0.050734, val r2: 0.865224\n",
      "epoch: 304, train loss: 0.001756, val loss: 0.005666, val mae: 0.052585, val r2: 0.857762\n",
      "epoch: 305, train loss: 0.001753, val loss: 0.006146, val mae: 0.054895, val r2: 0.845726\n",
      "epoch: 306, train loss: 0.001729, val loss: 0.006531, val mae: 0.056824, val r2: 0.835572\n",
      "epoch: 307, train loss: 0.001722, val loss: 0.006677, val mae: 0.057508, val r2: 0.831049\n",
      "epoch: 308, train loss: 0.001728, val loss: 0.006576, val mae: 0.057078, val r2: 0.833253\n",
      "epoch: 309, train loss: 0.001721, val loss: 0.006356, val mae: 0.055967, val r2: 0.838420\n",
      "epoch: 310, train loss: 0.001720, val loss: 0.006284, val mae: 0.055653, val r2: 0.840455\n",
      "epoch: 311, train loss: 0.001712, val loss: 0.006243, val mae: 0.055394, val r2: 0.841603\n",
      "epoch: 312, train loss: 0.001721, val loss: 0.006154, val mae: 0.054907, val r2: 0.844045\n",
      "epoch: 313, train loss: 0.001717, val loss: 0.005911, val mae: 0.053543, val r2: 0.850513\n",
      "epoch: 314, train loss: 0.001715, val loss: 0.005715, val mae: 0.052317, val r2: 0.855802\n",
      "epoch: 315, train loss: 0.001713, val loss: 0.005648, val mae: 0.051946, val r2: 0.857573\n",
      "epoch: 316, train loss: 0.001707, val loss: 0.005591, val mae: 0.051677, val r2: 0.859354\n",
      "epoch: 317, train loss: 0.001703, val loss: 0.005622, val mae: 0.051829, val r2: 0.858715\n",
      "epoch: 318, train loss: 0.001694, val loss: 0.005613, val mae: 0.051786, val r2: 0.859160\n",
      "epoch: 319, train loss: 0.001686, val loss: 0.005609, val mae: 0.051837, val r2: 0.859381\n",
      "epoch: 320, train loss: 0.001679, val loss: 0.005720, val mae: 0.052541, val r2: 0.856490\n",
      "epoch: 321, train loss: 0.001690, val loss: 0.005723, val mae: 0.052605, val r2: 0.856294\n",
      "epoch: 322, train loss: 0.001694, val loss: 0.005817, val mae: 0.053274, val r2: 0.853652\n",
      "epoch: 323, train loss: 0.001694, val loss: 0.006054, val mae: 0.054640, val r2: 0.847450\n",
      "epoch: 324, train loss: 0.001685, val loss: 0.006293, val mae: 0.055944, val r2: 0.841236\n",
      "epoch: 325, train loss: 0.001676, val loss: 0.006511, val mae: 0.056956, val r2: 0.835545\n",
      "epoch: 326, train loss: 0.001675, val loss: 0.006298, val mae: 0.055808, val r2: 0.840536\n",
      "epoch: 327, train loss: 0.001672, val loss: 0.005852, val mae: 0.053377, val r2: 0.852298\n",
      "epoch: 328, train loss: 0.001663, val loss: 0.005580, val mae: 0.051767, val r2: 0.859532\n",
      "epoch: 329, train loss: 0.001658, val loss: 0.005476, val mae: 0.051126, val r2: 0.862309\n",
      "epoch: 330, train loss: 0.001663, val loss: 0.005543, val mae: 0.051496, val r2: 0.860655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 331, train loss: 0.001672, val loss: 0.005796, val mae: 0.052772, val r2: 0.854295\n",
      "epoch: 332, train loss: 0.001659, val loss: 0.006080, val mae: 0.054240, val r2: 0.847087\n",
      "epoch: 333, train loss: 0.001653, val loss: 0.006248, val mae: 0.055078, val r2: 0.842530\n",
      "epoch: 334, train loss: 0.001665, val loss: 0.005938, val mae: 0.053482, val r2: 0.850135\n",
      "epoch: 335, train loss: 0.001660, val loss: 0.005663, val mae: 0.052076, val r2: 0.857308\n",
      "epoch: 336, train loss: 0.001670, val loss: 0.005602, val mae: 0.051792, val r2: 0.858981\n",
      "epoch: 337, train loss: 0.001702, val loss: 0.005898, val mae: 0.053415, val r2: 0.851441\n",
      "epoch: 338, train loss: 0.001681, val loss: 0.006229, val mae: 0.055190, val r2: 0.842935\n",
      "epoch: 339, train loss: 0.001655, val loss: 0.005877, val mae: 0.053549, val r2: 0.851737\n",
      "epoch: 340, train loss: 0.001671, val loss: 0.005516, val mae: 0.051582, val r2: 0.861209\n",
      "epoch: 341, train loss: 0.001658, val loss: 0.005464, val mae: 0.051305, val r2: 0.862798\n",
      "epoch: 342, train loss: 0.001661, val loss: 0.005517, val mae: 0.051617, val r2: 0.861413\n",
      "epoch: 343, train loss: 0.001665, val loss: 0.005557, val mae: 0.052039, val r2: 0.860440\n",
      "epoch: 344, train loss: 0.001641, val loss: 0.005463, val mae: 0.051495, val r2: 0.862886\n",
      "epoch: 345, train loss: 0.001631, val loss: 0.005358, val mae: 0.050648, val r2: 0.865625\n",
      "epoch: 346, train loss: 0.001620, val loss: 0.005346, val mae: 0.050628, val r2: 0.865894\n",
      "epoch: 347, train loss: 0.001615, val loss: 0.005411, val mae: 0.051146, val r2: 0.864250\n",
      "epoch: 348, train loss: 0.001604, val loss: 0.005485, val mae: 0.051605, val r2: 0.862369\n",
      "epoch: 349, train loss: 0.001587, val loss: 0.005581, val mae: 0.052259, val r2: 0.859679\n"
     ]
    }
   ],
   "source": [
    "# *\n",
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 32,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 1,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 6, \n",
    "                      'epoches': 350,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "06a21ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps!\n",
      "Transformer(\n",
      "  (element_embedding_temporal): Linear(in_features=196, out_features=64, bias=True)\n",
      "  (element_embedding_spatial): Linear(in_features=2695, out_features=880, bias=True)\n",
      "  (pos_embedding): Embedding(4, 16)\n",
      "  (variable_embedding): Embedding(55, 16)\n",
      "  (temporal): Encoder(\n",
      "    (fc_out): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=True)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=True)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=True)\n",
      "          (fc_out): Linear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatial): Encoder(\n",
      "    (fc_out): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=True)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=True)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=True)\n",
      "          (fc_out): Linear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatiotemporal): Encoder(\n",
      "    (fc_out): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=4, out_features=4, bias=True)\n",
      "          (keys): Linear(in_features=4, out_features=4, bias=True)\n",
      "          (queries): Linear(in_features=4, out_features=4, bias=True)\n",
      "          (fc_out): Linear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatter): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=4180, out_features=1980, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=1980, out_features=330, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch: 0, train loss: 0.019293, val loss: 0.015602, val mae: 0.098518, val r2: 0.617833\n",
      "epoch: 1, train loss: 0.010033, val loss: 0.012283, val mae: 0.083691, val r2: 0.697347\n",
      "epoch: 2, train loss: 0.008420, val loss: 0.011406, val mae: 0.079906, val r2: 0.718608\n",
      "epoch: 3, train loss: 0.007701, val loss: 0.010730, val mae: 0.077219, val r2: 0.735204\n",
      "epoch: 4, train loss: 0.007156, val loss: 0.010175, val mae: 0.075062, val r2: 0.748820\n",
      "epoch: 5, train loss: 0.006704, val loss: 0.009684, val mae: 0.073016, val r2: 0.761189\n",
      "epoch: 6, train loss: 0.006312, val loss: 0.009312, val mae: 0.071478, val r2: 0.770259\n",
      "epoch: 7, train loss: 0.005970, val loss: 0.008908, val mae: 0.069715, val r2: 0.780582\n",
      "epoch: 8, train loss: 0.005684, val loss: 0.008646, val mae: 0.068604, val r2: 0.787166\n",
      "epoch: 9, train loss: 0.005431, val loss: 0.008294, val mae: 0.066994, val r2: 0.796139\n",
      "epoch: 10, train loss: 0.005213, val loss: 0.008062, val mae: 0.065860, val r2: 0.801931\n",
      "epoch: 11, train loss: 0.005025, val loss: 0.007885, val mae: 0.064946, val r2: 0.806515\n",
      "epoch: 12, train loss: 0.004863, val loss: 0.007657, val mae: 0.063800, val r2: 0.812318\n",
      "epoch: 13, train loss: 0.004715, val loss: 0.007479, val mae: 0.062938, val r2: 0.816860\n",
      "epoch: 14, train loss: 0.004586, val loss: 0.007328, val mae: 0.062095, val r2: 0.820720\n",
      "epoch: 15, train loss: 0.004480, val loss: 0.007222, val mae: 0.061486, val r2: 0.823437\n",
      "epoch: 16, train loss: 0.004387, val loss: 0.007135, val mae: 0.061052, val r2: 0.825736\n",
      "epoch: 17, train loss: 0.004292, val loss: 0.007006, val mae: 0.060367, val r2: 0.828979\n",
      "epoch: 18, train loss: 0.004220, val loss: 0.006905, val mae: 0.059808, val r2: 0.831468\n",
      "epoch: 19, train loss: 0.004144, val loss: 0.006840, val mae: 0.059454, val r2: 0.833166\n",
      "epoch: 20, train loss: 0.004072, val loss: 0.006763, val mae: 0.059014, val r2: 0.835098\n",
      "epoch: 21, train loss: 0.004012, val loss: 0.006683, val mae: 0.058592, val r2: 0.837098\n",
      "epoch: 22, train loss: 0.003956, val loss: 0.006660, val mae: 0.058477, val r2: 0.837761\n",
      "epoch: 23, train loss: 0.003898, val loss: 0.006580, val mae: 0.058075, val r2: 0.839663\n",
      "epoch: 24, train loss: 0.003855, val loss: 0.006512, val mae: 0.057674, val r2: 0.841398\n",
      "epoch: 25, train loss: 0.003815, val loss: 0.006506, val mae: 0.057667, val r2: 0.841608\n",
      "epoch: 26, train loss: 0.003772, val loss: 0.006442, val mae: 0.057338, val r2: 0.843210\n",
      "epoch: 27, train loss: 0.003729, val loss: 0.006395, val mae: 0.057059, val r2: 0.844372\n",
      "epoch: 28, train loss: 0.003695, val loss: 0.006354, val mae: 0.056811, val r2: 0.845375\n",
      "epoch: 29, train loss: 0.003649, val loss: 0.006281, val mae: 0.056503, val r2: 0.847155\n",
      "epoch: 30, train loss: 0.003620, val loss: 0.006258, val mae: 0.056314, val r2: 0.847730\n",
      "epoch: 31, train loss: 0.003585, val loss: 0.006257, val mae: 0.056350, val r2: 0.847806\n",
      "epoch: 32, train loss: 0.003551, val loss: 0.006234, val mae: 0.056224, val r2: 0.848306\n",
      "epoch: 33, train loss: 0.003523, val loss: 0.006146, val mae: 0.055772, val r2: 0.850462\n",
      "epoch: 34, train loss: 0.003491, val loss: 0.006146, val mae: 0.055813, val r2: 0.850473\n",
      "epoch: 35, train loss: 0.003463, val loss: 0.006114, val mae: 0.055612, val r2: 0.851241\n",
      "epoch: 36, train loss: 0.003440, val loss: 0.006066, val mae: 0.055332, val r2: 0.852469\n",
      "epoch: 37, train loss: 0.003413, val loss: 0.006062, val mae: 0.055317, val r2: 0.852570\n",
      "epoch: 38, train loss: 0.003389, val loss: 0.006058, val mae: 0.055324, val r2: 0.852637\n",
      "epoch: 39, train loss: 0.003368, val loss: 0.006027, val mae: 0.055146, val r2: 0.853388\n",
      "epoch: 40, train loss: 0.003349, val loss: 0.006011, val mae: 0.055077, val r2: 0.853777\n",
      "epoch: 41, train loss: 0.003326, val loss: 0.005993, val mae: 0.055021, val r2: 0.854265\n",
      "epoch: 42, train loss: 0.003298, val loss: 0.005987, val mae: 0.055012, val r2: 0.854314\n",
      "epoch: 43, train loss: 0.003287, val loss: 0.005943, val mae: 0.054734, val r2: 0.855412\n",
      "epoch: 44, train loss: 0.003259, val loss: 0.005868, val mae: 0.054307, val r2: 0.857223\n",
      "epoch: 45, train loss: 0.003240, val loss: 0.005888, val mae: 0.054426, val r2: 0.856739\n",
      "epoch: 46, train loss: 0.003221, val loss: 0.005875, val mae: 0.054338, val r2: 0.857096\n",
      "epoch: 47, train loss: 0.003203, val loss: 0.005863, val mae: 0.054311, val r2: 0.857340\n",
      "epoch: 48, train loss: 0.003185, val loss: 0.005795, val mae: 0.053914, val r2: 0.859049\n",
      "epoch: 49, train loss: 0.003174, val loss: 0.005800, val mae: 0.053912, val r2: 0.858915\n",
      "epoch: 50, train loss: 0.003154, val loss: 0.005784, val mae: 0.053844, val r2: 0.859249\n",
      "epoch: 51, train loss: 0.003140, val loss: 0.005751, val mae: 0.053662, val r2: 0.860036\n",
      "epoch: 52, train loss: 0.003127, val loss: 0.005770, val mae: 0.053807, val r2: 0.859599\n",
      "epoch: 53, train loss: 0.003106, val loss: 0.005695, val mae: 0.053313, val r2: 0.861402\n",
      "epoch: 54, train loss: 0.003095, val loss: 0.005756, val mae: 0.053667, val r2: 0.859974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55, train loss: 0.003085, val loss: 0.005700, val mae: 0.053354, val r2: 0.861271\n",
      "epoch: 56, train loss: 0.003070, val loss: 0.005707, val mae: 0.053436, val r2: 0.861173\n",
      "epoch: 57, train loss: 0.003053, val loss: 0.005672, val mae: 0.053217, val r2: 0.862019\n",
      "epoch: 58, train loss: 0.003037, val loss: 0.005593, val mae: 0.052740, val r2: 0.863940\n",
      "epoch: 59, train loss: 0.003019, val loss: 0.005609, val mae: 0.052797, val r2: 0.863562\n",
      "epoch: 60, train loss: 0.003009, val loss: 0.005525, val mae: 0.052301, val r2: 0.865641\n",
      "epoch: 61, train loss: 0.002992, val loss: 0.005545, val mae: 0.052378, val r2: 0.865116\n",
      "epoch: 62, train loss: 0.002981, val loss: 0.005532, val mae: 0.052349, val r2: 0.865400\n",
      "epoch: 63, train loss: 0.002968, val loss: 0.005513, val mae: 0.052232, val r2: 0.865905\n",
      "epoch: 64, train loss: 0.002958, val loss: 0.005503, val mae: 0.052160, val r2: 0.866142\n",
      "epoch: 65, train loss: 0.002945, val loss: 0.005441, val mae: 0.051786, val r2: 0.867630\n",
      "epoch: 66, train loss: 0.002932, val loss: 0.005443, val mae: 0.051764, val r2: 0.867601\n",
      "epoch: 67, train loss: 0.002920, val loss: 0.005440, val mae: 0.051765, val r2: 0.867682\n",
      "epoch: 68, train loss: 0.002906, val loss: 0.005388, val mae: 0.051466, val r2: 0.868900\n",
      "epoch: 69, train loss: 0.002905, val loss: 0.005369, val mae: 0.051365, val r2: 0.869416\n",
      "epoch: 70, train loss: 0.002885, val loss: 0.005354, val mae: 0.051205, val r2: 0.869834\n",
      "epoch: 71, train loss: 0.002870, val loss: 0.005284, val mae: 0.050739, val r2: 0.871512\n",
      "epoch: 72, train loss: 0.002861, val loss: 0.005285, val mae: 0.050729, val r2: 0.871492\n",
      "epoch: 73, train loss: 0.002853, val loss: 0.005268, val mae: 0.050610, val r2: 0.871897\n",
      "epoch: 74, train loss: 0.002843, val loss: 0.005269, val mae: 0.050630, val r2: 0.871864\n",
      "epoch: 75, train loss: 0.002828, val loss: 0.005265, val mae: 0.050595, val r2: 0.871996\n",
      "epoch: 76, train loss: 0.002822, val loss: 0.005237, val mae: 0.050341, val r2: 0.872672\n",
      "epoch: 77, train loss: 0.002808, val loss: 0.005226, val mae: 0.050254, val r2: 0.872945\n",
      "epoch: 78, train loss: 0.002802, val loss: 0.005229, val mae: 0.050224, val r2: 0.872849\n",
      "epoch: 79, train loss: 0.002784, val loss: 0.005228, val mae: 0.050067, val r2: 0.872865\n",
      "epoch: 80, train loss: 0.002776, val loss: 0.005213, val mae: 0.050058, val r2: 0.873227\n",
      "epoch: 81, train loss: 0.002767, val loss: 0.005267, val mae: 0.050209, val r2: 0.871914\n",
      "epoch: 82, train loss: 0.002761, val loss: 0.005262, val mae: 0.050109, val r2: 0.872030\n",
      "epoch: 83, train loss: 0.002759, val loss: 0.005287, val mae: 0.050251, val r2: 0.871429\n",
      "epoch: 84, train loss: 0.002748, val loss: 0.005374, val mae: 0.050633, val r2: 0.869218\n",
      "epoch: 85, train loss: 0.002734, val loss: 0.005413, val mae: 0.050763, val r2: 0.868256\n",
      "epoch: 86, train loss: 0.002735, val loss: 0.005570, val mae: 0.051505, val r2: 0.864370\n",
      "epoch: 87, train loss: 0.002739, val loss: 0.005727, val mae: 0.052281, val r2: 0.860547\n",
      "epoch: 88, train loss: 0.002734, val loss: 0.005855, val mae: 0.052844, val r2: 0.857448\n",
      "epoch: 89, train loss: 0.002743, val loss: 0.006065, val mae: 0.053887, val r2: 0.852238\n",
      "epoch: 90, train loss: 0.002769, val loss: 0.006459, val mae: 0.055781, val r2: 0.842559\n",
      "epoch: 91, train loss: 0.002825, val loss: 0.006688, val mae: 0.056935, val r2: 0.836956\n",
      "epoch: 92, train loss: 0.002940, val loss: 0.006306, val mae: 0.054950, val r2: 0.846178\n",
      "epoch: 93, train loss: 0.003137, val loss: 0.005344, val mae: 0.050337, val r2: 0.869850\n",
      "epoch: 94, train loss: 0.003377, val loss: 0.005793, val mae: 0.053657, val r2: 0.859665\n",
      "epoch: 95, train loss: 0.003387, val loss: 0.006404, val mae: 0.056652, val r2: 0.845324\n",
      "epoch: 96, train loss: 0.003128, val loss: 0.005490, val mae: 0.051783, val r2: 0.866958\n",
      "epoch: 97, train loss: 0.002967, val loss: 0.005183, val mae: 0.049854, val r2: 0.873997\n",
      "epoch: 98, train loss: 0.002882, val loss: 0.005175, val mae: 0.049678, val r2: 0.873986\n",
      "epoch: 99, train loss: 0.002818, val loss: 0.005170, val mae: 0.049574, val r2: 0.874098\n",
      "epoch: 100, train loss: 0.002756, val loss: 0.005163, val mae: 0.049576, val r2: 0.874248\n",
      "epoch: 101, train loss: 0.002721, val loss: 0.005163, val mae: 0.049621, val r2: 0.874277\n",
      "epoch: 102, train loss: 0.002689, val loss: 0.005161, val mae: 0.049569, val r2: 0.874367\n",
      "epoch: 103, train loss: 0.002675, val loss: 0.005134, val mae: 0.049510, val r2: 0.875045\n",
      "epoch: 104, train loss: 0.002662, val loss: 0.005154, val mae: 0.049594, val r2: 0.874571\n",
      "epoch: 105, train loss: 0.002652, val loss: 0.005148, val mae: 0.049575, val r2: 0.874716\n",
      "epoch: 106, train loss: 0.002644, val loss: 0.005160, val mae: 0.049645, val r2: 0.874461\n",
      "epoch: 107, train loss: 0.002633, val loss: 0.005145, val mae: 0.049573, val r2: 0.874820\n",
      "epoch: 108, train loss: 0.002627, val loss: 0.005149, val mae: 0.049589, val r2: 0.874702\n",
      "epoch: 109, train loss: 0.002624, val loss: 0.005143, val mae: 0.049533, val r2: 0.874829\n",
      "epoch: 110, train loss: 0.002612, val loss: 0.005145, val mae: 0.049597, val r2: 0.874827\n",
      "epoch: 111, train loss: 0.002603, val loss: 0.005135, val mae: 0.049496, val r2: 0.875029\n",
      "epoch: 112, train loss: 0.002600, val loss: 0.005117, val mae: 0.049439, val r2: 0.875472\n",
      "epoch: 113, train loss: 0.002594, val loss: 0.005143, val mae: 0.049551, val r2: 0.874841\n",
      "epoch: 114, train loss: 0.002596, val loss: 0.005131, val mae: 0.049544, val r2: 0.875128\n",
      "epoch: 115, train loss: 0.002582, val loss: 0.005127, val mae: 0.049477, val r2: 0.875262\n",
      "epoch: 116, train loss: 0.002572, val loss: 0.005137, val mae: 0.049563, val r2: 0.875024\n",
      "epoch: 117, train loss: 0.002570, val loss: 0.005102, val mae: 0.049430, val r2: 0.875954\n",
      "epoch: 118, train loss: 0.002565, val loss: 0.005113, val mae: 0.049481, val r2: 0.875673\n",
      "epoch: 119, train loss: 0.002556, val loss: 0.005112, val mae: 0.049537, val r2: 0.875753\n",
      "epoch: 120, train loss: 0.002553, val loss: 0.005119, val mae: 0.049573, val r2: 0.875579\n",
      "epoch: 121, train loss: 0.002547, val loss: 0.005130, val mae: 0.049640, val r2: 0.875357\n",
      "epoch: 122, train loss: 0.002547, val loss: 0.005154, val mae: 0.049874, val r2: 0.874893\n",
      "epoch: 123, train loss: 0.002537, val loss: 0.005148, val mae: 0.049795, val r2: 0.874973\n",
      "epoch: 124, train loss: 0.002537, val loss: 0.005147, val mae: 0.049791, val r2: 0.875034\n",
      "epoch: 125, train loss: 0.002539, val loss: 0.005175, val mae: 0.050048, val r2: 0.874403\n",
      "epoch: 126, train loss: 0.002544, val loss: 0.005175, val mae: 0.050041, val r2: 0.874395\n",
      "epoch: 127, train loss: 0.002558, val loss: 0.005208, val mae: 0.050271, val r2: 0.873659\n",
      "epoch: 128, train loss: 0.002583, val loss: 0.005181, val mae: 0.050069, val r2: 0.874252\n",
      "epoch: 129, train loss: 0.002614, val loss: 0.005110, val mae: 0.049560, val r2: 0.875767\n",
      "epoch: 130, train loss: 0.002646, val loss: 0.005110, val mae: 0.049384, val r2: 0.875540\n",
      "epoch: 131, train loss: 0.002677, val loss: 0.005259, val mae: 0.050005, val r2: 0.871855\n",
      "epoch: 132, train loss: 0.002685, val loss: 0.005576, val mae: 0.051535, val r2: 0.864145\n",
      "epoch: 133, train loss: 0.002649, val loss: 0.005501, val mae: 0.051169, val r2: 0.866129\n",
      "epoch: 134, train loss: 0.002647, val loss: 0.005202, val mae: 0.049681, val r2: 0.873272\n",
      "epoch: 135, train loss: 0.002724, val loss: 0.005023, val mae: 0.048934, val r2: 0.877556\n",
      "epoch: 136, train loss: 0.002769, val loss: 0.005115, val mae: 0.049634, val r2: 0.875421\n",
      "epoch: 137, train loss: 0.002733, val loss: 0.005403, val mae: 0.051377, val r2: 0.868609\n",
      "epoch: 138, train loss: 0.002629, val loss: 0.005393, val mae: 0.051307, val r2: 0.868834\n",
      "epoch: 139, train loss: 0.002562, val loss: 0.005287, val mae: 0.050655, val r2: 0.871386\n",
      "epoch: 140, train loss: 0.002531, val loss: 0.005131, val mae: 0.049773, val r2: 0.875064\n",
      "epoch: 141, train loss: 0.002515, val loss: 0.005081, val mae: 0.049416, val r2: 0.876149\n",
      "epoch: 142, train loss: 0.002501, val loss: 0.005081, val mae: 0.049468, val r2: 0.876107\n",
      "epoch: 143, train loss: 0.002497, val loss: 0.005085, val mae: 0.049435, val r2: 0.875910\n",
      "epoch: 144, train loss: 0.002497, val loss: 0.005078, val mae: 0.049353, val r2: 0.876061\n",
      "epoch: 145, train loss: 0.002499, val loss: 0.005109, val mae: 0.049567, val r2: 0.875148\n",
      "epoch: 146, train loss: 0.002493, val loss: 0.005135, val mae: 0.049757, val r2: 0.874376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 147, train loss: 0.002502, val loss: 0.005184, val mae: 0.050081, val r2: 0.873149\n",
      "epoch: 148, train loss: 0.002507, val loss: 0.005241, val mae: 0.050398, val r2: 0.871616\n",
      "epoch: 149, train loss: 0.002504, val loss: 0.005292, val mae: 0.050714, val r2: 0.870146\n",
      "epoch: 150, train loss: 0.002515, val loss: 0.005385, val mae: 0.051243, val r2: 0.867627\n",
      "epoch: 151, train loss: 0.002528, val loss: 0.005530, val mae: 0.052082, val r2: 0.863903\n",
      "epoch: 152, train loss: 0.002539, val loss: 0.005582, val mae: 0.052397, val r2: 0.862213\n",
      "epoch: 153, train loss: 0.002562, val loss: 0.005806, val mae: 0.053566, val r2: 0.856165\n",
      "epoch: 154, train loss: 0.002589, val loss: 0.006002, val mae: 0.054642, val r2: 0.851071\n",
      "epoch: 155, train loss: 0.002622, val loss: 0.006328, val mae: 0.056358, val r2: 0.842279\n",
      "epoch: 156, train loss: 0.002675, val loss: 0.006665, val mae: 0.058057, val r2: 0.833201\n",
      "epoch: 157, train loss: 0.002739, val loss: 0.007131, val mae: 0.060263, val r2: 0.820675\n",
      "epoch: 158, train loss: 0.002828, val loss: 0.008020, val mae: 0.064321, val r2: 0.797706\n",
      "epoch: 159, train loss: 0.002871, val loss: 0.008541, val mae: 0.066525, val r2: 0.784022\n",
      "epoch: 160, train loss: 0.002910, val loss: 0.008403, val mae: 0.065954, val r2: 0.787293\n",
      "epoch: 161, train loss: 0.002963, val loss: 0.007933, val mae: 0.063998, val r2: 0.799127\n",
      "epoch: 162, train loss: 0.003037, val loss: 0.007199, val mae: 0.060644, val r2: 0.818158\n",
      "epoch: 163, train loss: 0.003130, val loss: 0.006656, val mae: 0.058109, val r2: 0.832577\n",
      "epoch: 164, train loss: 0.003145, val loss: 0.006030, val mae: 0.054907, val r2: 0.849447\n",
      "epoch: 165, train loss: 0.003140, val loss: 0.005591, val mae: 0.052462, val r2: 0.861425\n",
      "epoch: 166, train loss: 0.003038, val loss: 0.005334, val mae: 0.050934, val r2: 0.868663\n",
      "epoch: 167, train loss: 0.002920, val loss: 0.005205, val mae: 0.050208, val r2: 0.872312\n",
      "epoch: 168, train loss: 0.002781, val loss: 0.005107, val mae: 0.049590, val r2: 0.875026\n",
      "epoch: 169, train loss: 0.002692, val loss: 0.005023, val mae: 0.049190, val r2: 0.877217\n",
      "epoch: 170, train loss: 0.002634, val loss: 0.004965, val mae: 0.048853, val r2: 0.878763\n",
      "epoch: 171, train loss: 0.002607, val loss: 0.004888, val mae: 0.048459, val r2: 0.880712\n",
      "epoch: 172, train loss: 0.002570, val loss: 0.004877, val mae: 0.048380, val r2: 0.881092\n",
      "epoch: 173, train loss: 0.002542, val loss: 0.004859, val mae: 0.048321, val r2: 0.881579\n",
      "epoch: 174, train loss: 0.002519, val loss: 0.004836, val mae: 0.048174, val r2: 0.882203\n",
      "epoch: 175, train loss: 0.002496, val loss: 0.004841, val mae: 0.048197, val r2: 0.882126\n",
      "epoch: 176, train loss: 0.002472, val loss: 0.004834, val mae: 0.048132, val r2: 0.882298\n",
      "epoch: 177, train loss: 0.002453, val loss: 0.004826, val mae: 0.048114, val r2: 0.882537\n",
      "epoch: 178, train loss: 0.002439, val loss: 0.004833, val mae: 0.048188, val r2: 0.882403\n",
      "epoch: 179, train loss: 0.002420, val loss: 0.004819, val mae: 0.048086, val r2: 0.882751\n",
      "epoch: 180, train loss: 0.002404, val loss: 0.004796, val mae: 0.047920, val r2: 0.883280\n",
      "epoch: 181, train loss: 0.002387, val loss: 0.004807, val mae: 0.048029, val r2: 0.883080\n",
      "epoch: 182, train loss: 0.002380, val loss: 0.004807, val mae: 0.048044, val r2: 0.883032\n",
      "epoch: 183, train loss: 0.002367, val loss: 0.004791, val mae: 0.047949, val r2: 0.883434\n",
      "epoch: 184, train loss: 0.002357, val loss: 0.004810, val mae: 0.048066, val r2: 0.883002\n",
      "epoch: 185, train loss: 0.002345, val loss: 0.004807, val mae: 0.048076, val r2: 0.883093\n",
      "epoch: 186, train loss: 0.002336, val loss: 0.004827, val mae: 0.048175, val r2: 0.882540\n",
      "epoch: 187, train loss: 0.002326, val loss: 0.004837, val mae: 0.048300, val r2: 0.882338\n",
      "epoch: 188, train loss: 0.002314, val loss: 0.004840, val mae: 0.048296, val r2: 0.882196\n",
      "epoch: 189, train loss: 0.002305, val loss: 0.004839, val mae: 0.048333, val r2: 0.882254\n",
      "epoch: 190, train loss: 0.002297, val loss: 0.004806, val mae: 0.048112, val r2: 0.883003\n",
      "epoch: 191, train loss: 0.002290, val loss: 0.004833, val mae: 0.048299, val r2: 0.882289\n",
      "epoch: 192, train loss: 0.002281, val loss: 0.004852, val mae: 0.048440, val r2: 0.881869\n",
      "epoch: 193, train loss: 0.002273, val loss: 0.004833, val mae: 0.048299, val r2: 0.882289\n",
      "epoch: 194, train loss: 0.002267, val loss: 0.004848, val mae: 0.048416, val r2: 0.881897\n",
      "epoch: 195, train loss: 0.002259, val loss: 0.004866, val mae: 0.048583, val r2: 0.881441\n",
      "epoch: 196, train loss: 0.002253, val loss: 0.004873, val mae: 0.048578, val r2: 0.881195\n",
      "epoch: 197, train loss: 0.002249, val loss: 0.004840, val mae: 0.048397, val r2: 0.881990\n",
      "epoch: 198, train loss: 0.002246, val loss: 0.004867, val mae: 0.048577, val r2: 0.881301\n",
      "epoch: 199, train loss: 0.002237, val loss: 0.004900, val mae: 0.048834, val r2: 0.880365\n",
      "epoch: 200, train loss: 0.002234, val loss: 0.004899, val mae: 0.048780, val r2: 0.880449\n",
      "epoch: 201, train loss: 0.002228, val loss: 0.004961, val mae: 0.049267, val r2: 0.878739\n",
      "epoch: 202, train loss: 0.002226, val loss: 0.004941, val mae: 0.049079, val r2: 0.879257\n",
      "epoch: 203, train loss: 0.002218, val loss: 0.004952, val mae: 0.049201, val r2: 0.878915\n",
      "epoch: 204, train loss: 0.002216, val loss: 0.005005, val mae: 0.049495, val r2: 0.877547\n",
      "epoch: 205, train loss: 0.002211, val loss: 0.005008, val mae: 0.049458, val r2: 0.877421\n",
      "epoch: 206, train loss: 0.002209, val loss: 0.005002, val mae: 0.049480, val r2: 0.877596\n",
      "epoch: 207, train loss: 0.002204, val loss: 0.005116, val mae: 0.050190, val r2: 0.874554\n",
      "epoch: 208, train loss: 0.002204, val loss: 0.005155, val mae: 0.050382, val r2: 0.873419\n",
      "epoch: 209, train loss: 0.002202, val loss: 0.005218, val mae: 0.050772, val r2: 0.871714\n",
      "epoch: 210, train loss: 0.002199, val loss: 0.005320, val mae: 0.051381, val r2: 0.869166\n",
      "epoch: 211, train loss: 0.002197, val loss: 0.005370, val mae: 0.051637, val r2: 0.867799\n",
      "epoch: 212, train loss: 0.002198, val loss: 0.005642, val mae: 0.053107, val r2: 0.860862\n",
      "epoch: 213, train loss: 0.002192, val loss: 0.005659, val mae: 0.053275, val r2: 0.859952\n",
      "epoch: 214, train loss: 0.002197, val loss: 0.005845, val mae: 0.054225, val r2: 0.855396\n",
      "epoch: 215, train loss: 0.002201, val loss: 0.006119, val mae: 0.055622, val r2: 0.848212\n",
      "epoch: 216, train loss: 0.002214, val loss: 0.006406, val mae: 0.057021, val r2: 0.840623\n",
      "epoch: 217, train loss: 0.002235, val loss: 0.006559, val mae: 0.057725, val r2: 0.836524\n",
      "epoch: 218, train loss: 0.002274, val loss: 0.006555, val mae: 0.057528, val r2: 0.835833\n",
      "epoch: 219, train loss: 0.002351, val loss: 0.005843, val mae: 0.053857, val r2: 0.853312\n",
      "epoch: 220, train loss: 0.002497, val loss: 0.005307, val mae: 0.050856, val r2: 0.867460\n",
      "epoch: 221, train loss: 0.002669, val loss: 0.005958, val mae: 0.054355, val r2: 0.849948\n",
      "epoch: 222, train loss: 0.002739, val loss: 0.007746, val mae: 0.063135, val r2: 0.805034\n",
      "epoch: 223, train loss: 0.002482, val loss: 0.006389, val mae: 0.056741, val r2: 0.840366\n",
      "epoch: 224, train loss: 0.002392, val loss: 0.005541, val mae: 0.052320, val r2: 0.862241\n",
      "epoch: 225, train loss: 0.002382, val loss: 0.005349, val mae: 0.051274, val r2: 0.867419\n",
      "epoch: 226, train loss: 0.002363, val loss: 0.005326, val mae: 0.051158, val r2: 0.868312\n",
      "epoch: 227, train loss: 0.002332, val loss: 0.005117, val mae: 0.050015, val r2: 0.873928\n",
      "epoch: 228, train loss: 0.002317, val loss: 0.004975, val mae: 0.049135, val r2: 0.877909\n",
      "epoch: 229, train loss: 0.002298, val loss: 0.004837, val mae: 0.048232, val r2: 0.881749\n",
      "epoch: 230, train loss: 0.002296, val loss: 0.004781, val mae: 0.047813, val r2: 0.883328\n",
      "epoch: 231, train loss: 0.002292, val loss: 0.004721, val mae: 0.047377, val r2: 0.884943\n",
      "epoch: 232, train loss: 0.002288, val loss: 0.004709, val mae: 0.047217, val r2: 0.885263\n",
      "epoch: 233, train loss: 0.002296, val loss: 0.004737, val mae: 0.047246, val r2: 0.884568\n",
      "epoch: 234, train loss: 0.002305, val loss: 0.004748, val mae: 0.047330, val r2: 0.884241\n",
      "epoch: 235, train loss: 0.002307, val loss: 0.004769, val mae: 0.047412, val r2: 0.883636\n",
      "epoch: 236, train loss: 0.002298, val loss: 0.004832, val mae: 0.047671, val r2: 0.881990\n",
      "epoch: 237, train loss: 0.002295, val loss: 0.004850, val mae: 0.047791, val r2: 0.881531\n",
      "epoch: 238, train loss: 0.002294, val loss: 0.004845, val mae: 0.047820, val r2: 0.881710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 239, train loss: 0.002290, val loss: 0.004889, val mae: 0.048042, val r2: 0.880596\n",
      "epoch: 240, train loss: 0.002289, val loss: 0.004895, val mae: 0.048114, val r2: 0.880446\n",
      "epoch: 241, train loss: 0.002286, val loss: 0.004858, val mae: 0.047953, val r2: 0.881451\n",
      "epoch: 242, train loss: 0.002275, val loss: 0.004867, val mae: 0.047986, val r2: 0.881211\n",
      "epoch: 243, train loss: 0.002277, val loss: 0.004887, val mae: 0.048115, val r2: 0.880698\n",
      "epoch: 244, train loss: 0.002268, val loss: 0.004903, val mae: 0.048170, val r2: 0.880303\n",
      "epoch: 245, train loss: 0.002259, val loss: 0.004918, val mae: 0.048277, val r2: 0.880008\n",
      "epoch: 246, train loss: 0.002252, val loss: 0.004889, val mae: 0.048141, val r2: 0.880813\n",
      "epoch: 247, train loss: 0.002243, val loss: 0.004845, val mae: 0.047935, val r2: 0.881920\n",
      "epoch: 248, train loss: 0.002229, val loss: 0.004866, val mae: 0.048035, val r2: 0.881376\n",
      "epoch: 249, train loss: 0.002214, val loss: 0.004839, val mae: 0.047903, val r2: 0.882150\n",
      "epoch: 250, train loss: 0.002202, val loss: 0.004824, val mae: 0.047844, val r2: 0.882478\n",
      "epoch: 251, train loss: 0.002188, val loss: 0.004791, val mae: 0.047717, val r2: 0.883336\n",
      "epoch: 252, train loss: 0.002177, val loss: 0.004784, val mae: 0.047726, val r2: 0.883514\n",
      "epoch: 253, train loss: 0.002164, val loss: 0.004790, val mae: 0.047864, val r2: 0.883277\n",
      "epoch: 254, train loss: 0.002153, val loss: 0.004812, val mae: 0.048073, val r2: 0.882638\n",
      "epoch: 255, train loss: 0.002145, val loss: 0.004852, val mae: 0.048365, val r2: 0.881510\n",
      "epoch: 256, train loss: 0.002140, val loss: 0.004916, val mae: 0.048801, val r2: 0.879762\n",
      "epoch: 257, train loss: 0.002134, val loss: 0.005018, val mae: 0.049462, val r2: 0.876927\n",
      "epoch: 258, train loss: 0.002132, val loss: 0.005124, val mae: 0.050110, val r2: 0.873846\n",
      "epoch: 259, train loss: 0.002140, val loss: 0.005301, val mae: 0.051072, val r2: 0.869010\n",
      "epoch: 260, train loss: 0.002141, val loss: 0.005513, val mae: 0.052227, val r2: 0.863169\n",
      "epoch: 261, train loss: 0.002147, val loss: 0.005635, val mae: 0.052803, val r2: 0.859539\n",
      "epoch: 262, train loss: 0.002160, val loss: 0.005816, val mae: 0.053712, val r2: 0.854778\n",
      "epoch: 263, train loss: 0.002170, val loss: 0.005949, val mae: 0.054290, val r2: 0.850735\n",
      "epoch: 264, train loss: 0.002187, val loss: 0.006081, val mae: 0.054936, val r2: 0.847450\n",
      "epoch: 265, train loss: 0.002200, val loss: 0.006331, val mae: 0.056119, val r2: 0.840765\n",
      "epoch: 266, train loss: 0.002217, val loss: 0.006506, val mae: 0.056958, val r2: 0.836475\n",
      "epoch: 267, train loss: 0.002231, val loss: 0.006540, val mae: 0.057087, val r2: 0.835670\n",
      "epoch: 268, train loss: 0.002239, val loss: 0.006288, val mae: 0.055935, val r2: 0.842080\n",
      "epoch: 269, train loss: 0.002258, val loss: 0.005773, val mae: 0.053433, val r2: 0.855725\n",
      "epoch: 270, train loss: 0.002273, val loss: 0.005334, val mae: 0.051209, val r2: 0.867592\n",
      "epoch: 271, train loss: 0.002262, val loss: 0.005069, val mae: 0.049812, val r2: 0.875134\n",
      "epoch: 272, train loss: 0.002240, val loss: 0.004909, val mae: 0.048939, val r2: 0.879915\n",
      "epoch: 273, train loss: 0.002229, val loss: 0.004769, val mae: 0.047876, val r2: 0.883817\n",
      "epoch: 274, train loss: 0.002225, val loss: 0.004752, val mae: 0.047485, val r2: 0.884251\n",
      "epoch: 275, train loss: 0.002221, val loss: 0.004853, val mae: 0.047832, val r2: 0.881618\n",
      "epoch: 276, train loss: 0.002222, val loss: 0.004919, val mae: 0.048113, val r2: 0.879895\n",
      "epoch: 277, train loss: 0.002206, val loss: 0.004954, val mae: 0.048296, val r2: 0.879048\n",
      "epoch: 278, train loss: 0.002190, val loss: 0.004890, val mae: 0.048000, val r2: 0.880679\n",
      "epoch: 279, train loss: 0.002181, val loss: 0.004796, val mae: 0.047587, val r2: 0.883019\n",
      "epoch: 280, train loss: 0.002160, val loss: 0.004780, val mae: 0.047546, val r2: 0.883434\n",
      "epoch: 281, train loss: 0.002147, val loss: 0.004768, val mae: 0.047502, val r2: 0.883778\n",
      "epoch: 282, train loss: 0.002135, val loss: 0.004731, val mae: 0.047338, val r2: 0.884691\n",
      "epoch: 283, train loss: 0.002120, val loss: 0.004740, val mae: 0.047394, val r2: 0.884494\n",
      "epoch: 284, train loss: 0.002114, val loss: 0.004721, val mae: 0.047350, val r2: 0.884958\n",
      "epoch: 285, train loss: 0.002095, val loss: 0.004697, val mae: 0.047252, val r2: 0.885537\n",
      "epoch: 286, train loss: 0.002086, val loss: 0.004714, val mae: 0.047372, val r2: 0.885068\n",
      "epoch: 287, train loss: 0.002073, val loss: 0.004730, val mae: 0.047559, val r2: 0.884589\n",
      "epoch: 288, train loss: 0.002064, val loss: 0.004743, val mae: 0.047754, val r2: 0.884065\n",
      "epoch: 289, train loss: 0.002060, val loss: 0.004830, val mae: 0.048316, val r2: 0.881761\n",
      "epoch: 290, train loss: 0.002056, val loss: 0.004958, val mae: 0.049209, val r2: 0.878151\n",
      "epoch: 291, train loss: 0.002057, val loss: 0.005112, val mae: 0.050116, val r2: 0.873834\n",
      "epoch: 292, train loss: 0.002060, val loss: 0.005281, val mae: 0.051090, val r2: 0.869246\n",
      "epoch: 293, train loss: 0.002063, val loss: 0.005451, val mae: 0.051981, val r2: 0.864530\n",
      "epoch: 294, train loss: 0.002074, val loss: 0.005526, val mae: 0.052390, val r2: 0.862456\n",
      "epoch: 295, train loss: 0.002083, val loss: 0.005510, val mae: 0.052312, val r2: 0.862918\n",
      "epoch: 296, train loss: 0.002084, val loss: 0.005271, val mae: 0.050957, val r2: 0.869233\n",
      "epoch: 297, train loss: 0.002082, val loss: 0.005047, val mae: 0.049766, val r2: 0.875556\n",
      "epoch: 298, train loss: 0.002085, val loss: 0.004867, val mae: 0.048682, val r2: 0.880549\n",
      "epoch: 299, train loss: 0.002071, val loss: 0.004717, val mae: 0.047585, val r2: 0.884941\n",
      "epoch: 300, train loss: 0.002075, val loss: 0.004700, val mae: 0.047250, val r2: 0.885619\n",
      "epoch: 301, train loss: 0.002082, val loss: 0.004755, val mae: 0.047337, val r2: 0.884160\n",
      "epoch: 302, train loss: 0.002085, val loss: 0.004832, val mae: 0.047733, val r2: 0.882097\n",
      "epoch: 303, train loss: 0.002082, val loss: 0.004869, val mae: 0.047900, val r2: 0.881013\n",
      "epoch: 304, train loss: 0.002073, val loss: 0.004820, val mae: 0.047703, val r2: 0.882222\n"
     ]
    }
   ],
   "source": [
    "# final\n",
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [6], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 16,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 1,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 4, \n",
    "                      'epoches': 286,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 8, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 4, 16), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90260786",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = YNYT(period['val'], seq_len=seq_len, mode='test', path_preprocessed=path_preprocessed, \n",
    "                     setting=data_setting, horizon=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "6daaba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "sns.set_theme(\"paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "50e4aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sttre.data import YNYT\n",
    "import datetime\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "seq_len = settings['params']['seq_len']\n",
    "horizon = 6\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 8, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 4, 17), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "test_data = YNYT(period['val'], seq_len=seq_len, mode='test', path_preprocessed=path_preprocessed, \n",
    "                     setting=data_setting, horizon=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c309f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=128)\n",
    "res = []\n",
    "fact = []\n",
    "device = torch.device(\"mps\")\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, regressors, labels = data\n",
    "        regressors = regressors.unsqueeze(-1)\n",
    "        labels = torch.flatten(labels, start_dim=1).detach().cpu().numpy()\n",
    "        output = model(inputs.to(device), regressors.to(device), 0).detach().cpu().numpy()\n",
    "        res.append(output)\n",
    "        fact.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "67c9c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['t', 'zone_id', 'datetime'] + [f'target_{h}' for h in range(1, 7)]\n",
    "df = test_data.bf.data.loc[:, columns]\n",
    "\n",
    "columns = [f'pred_{h}' for h in range(1, 7)]\n",
    "for c in columns:\n",
    "    df[c] = 0\n",
    "\n",
    "t0 = 0\n",
    "shift = seq_len - 1\n",
    "for i, y in enumerate(res):\n",
    "    l = len(y)\n",
    "    t1 = t0 + l\n",
    "    index = df[(df.t >= t0 + shift ) & (df.t < t1 + shift)].index\n",
    "    df.loc[index, columns] = np.concatenate(y.reshape(l, 6, 55).T, axis=1).T\n",
    "    t0 = t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "d0055366",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df[(df.t>=3) & (df.datetime < datetime.datetime(2022, 6, 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "27b6adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_mae = []\n",
    "transf_r2 = []\n",
    "for h in range(1, 7):\n",
    "    mae = mean_absolute_error(df.loc[:, f'target_{h}'].values, df.loc[:, f'pred_{h}'].values)\n",
    "    r2 = r2_score(df.loc[:, f'target_{h}'].values, df.loc[:, f'pred_{h}'].values)\n",
    "    transf_mae.append(mae)\n",
    "    transf_r2.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "0f99627a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9045719909978178,\n",
       " 0.8992152489264963,\n",
       " 0.8951301902335514,\n",
       " 0.8915348731480675,\n",
       " 0.8876007547609097,\n",
       " 0.8847107877771501]"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transf_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "a72f31b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_mae = [0.04457, 0.04805, 0.04958, 0.05035, 0.0509, 0.05129]\n",
    "ridge_r2 = [0.90793, 0.89385, 0.88563, 0.8815, 0.88013, 0.87887]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "508812f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.axis.XTick at 0x339e26cb0>,\n",
       " <matplotlib.axis.XTick at 0x339e26c80>,\n",
       " <matplotlib.axis.XTick at 0x339e268f0>,\n",
       " <matplotlib.axis.XTick at 0x339e97cd0>,\n",
       " <matplotlib.axis.XTick at 0x339ec0670>,\n",
       " <matplotlib.axis.XTick at 0x339ec08b0>]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_palette(\"Paired\")\n",
    "x = np.arange(1, 7)\n",
    "horizon = [f'h{h}' for h in x]\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "for values, label in zip([ridge_mae, transf_mae], ['Ridge', 'Transformers']):\n",
    "    offset = width * multiplier\n",
    "    rects = ax[0].bar(x + offset, values, width, label=label)\n",
    "    multiplier += 1\n",
    "ax[0].legend(loc='upper left', ncols=2, bbox_to_anchor=(0, -0.15))\n",
    "ax[0].set_title('MAE')\n",
    "ax[0].set_xticks(x + width - 0.1, horizon)\n",
    "\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "for values, label in zip([ridge_r2, transf_r2], ['Ridge', 'Transformers']):\n",
    "    offset = width * multiplier\n",
    "    rects = ax[1].bar(x + offset, values, width, label=label)\n",
    "    multiplier += 1\n",
    "ax[1].set_title('R2')\n",
    "ax[1].set_xticks(x + width, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "876f8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_id = []\n",
    "mae = []\n",
    "for g in df.groupby('zone_id'):\n",
    "    zone_id.append(g[0])\n",
    "    mae.append(mean_absolute_error(g[1].iloc[:, -12:-6].unstack().values, g[1].iloc[:, -6:].unstack().values))\n",
    "\n",
    "mae = pd.Series(mae, index=zone_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "4f5347af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 55 artists>"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(55), mae[zones])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "203f3d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_month 17.49.52.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../_ynyt/data/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "671cc15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ridge = pd.read_csv('../../_ynyt/data/results/results_month 17.49.52.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "ed84b6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>datetime</th>\n",
       "      <th>zone_id</th>\n",
       "      <th>h</th>\n",
       "      <th>pred</th>\n",
       "      <th>fact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29184</td>\n",
       "      <td>1651363200000</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>4.133311</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29185</td>\n",
       "      <td>1651366800000</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>3.035502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29186</td>\n",
       "      <td>1651370400000</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2.263616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29187</td>\n",
       "      <td>1651374000000</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>4.082356</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29188</td>\n",
       "      <td>1651377600000</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>3.665957</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243535</th>\n",
       "      <td>29917</td>\n",
       "      <td>1654002000000</td>\n",
       "      <td>264</td>\n",
       "      <td>6</td>\n",
       "      <td>55.932773</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243536</th>\n",
       "      <td>29918</td>\n",
       "      <td>1654005600000</td>\n",
       "      <td>264</td>\n",
       "      <td>6</td>\n",
       "      <td>56.023477</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243537</th>\n",
       "      <td>29919</td>\n",
       "      <td>1654009200000</td>\n",
       "      <td>264</td>\n",
       "      <td>6</td>\n",
       "      <td>61.934255</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243538</th>\n",
       "      <td>29920</td>\n",
       "      <td>1654012800000</td>\n",
       "      <td>264</td>\n",
       "      <td>6</td>\n",
       "      <td>69.127348</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243539</th>\n",
       "      <td>29921</td>\n",
       "      <td>1654016400000</td>\n",
       "      <td>264</td>\n",
       "      <td>6</td>\n",
       "      <td>49.334978</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243540 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            t       datetime  zone_id  h       pred  fact\n",
       "0       29184  1651363200000       13  1   4.133311     4\n",
       "1       29185  1651366800000       13  1   3.035502     0\n",
       "2       29186  1651370400000       13  1   2.263616     1\n",
       "3       29187  1651374000000       13  1   4.082356     2\n",
       "4       29188  1651377600000       13  1   3.665957     0\n",
       "...       ...            ...      ... ..        ...   ...\n",
       "243535  29917  1654002000000      264  6  55.932773    66\n",
       "243536  29918  1654005600000      264  6  56.023477    83\n",
       "243537  29919  1654009200000      264  6  61.934255    84\n",
       "243538  29920  1654012800000      264  6  69.127348    73\n",
       "243539  29921  1654016400000      264  6  49.334978    45\n",
       "\n",
       "[243540 rows x 6 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1617cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = df_ridge.groupby('zone_id').fact.mean().sort_values().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92819eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "d4ee6a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('http://158.160.109.15:5001', 'http://158.160.109.15:9000')"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, Tuple, List\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import datetime\n",
    "import mlflow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from ynyt.features import BaseFeatures, FeatureCombiner\n",
    "\n",
    "with open(\"../../_ynyt/prediction/config.env\") as f:\n",
    "    for line in f.readlines():\n",
    "        if len(line) > 2:\n",
    "            k, v = line.split('=')\n",
    "            os.environ[k] = v[:-1]\n",
    "            \n",
    "AWS_ACCESS_KEY_ID = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "AWS_SECRET_ACCESS_KEY = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "S3_ENDPOINT_URL = os.environ[\"MLFLOW_S3_ENDPOINT_URL\"]\n",
    "MLFLOW_TRACKING_URI = os.environ[\"MLFLOW_TRACKING_URI\"]\n",
    "ARTIFACT_URI = os.environ[\"ARTIFACT_URI\"]\n",
    "\n",
    "MLFLOW_TRACKING_URI, S3_ENDPOINT_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "4bfbec75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-23 13:19:42.761769: predictions are updated, [datetime.datetime(2022, 4, 17, 1, 0), datetime.datetime(2022, 5, 31, 23, 0)]\n"
     ]
    }
   ],
   "source": [
    "def update_predictions(period: str, mode: str, model_paths: dict) -> pd.DataFrame:\n",
    "    path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "    inference_base_features = BaseFeatures(period, \n",
    "                                           config, \n",
    "                                           path_preprocessed, s3=False).fit(mode=mode)\n",
    "\n",
    "    def back_normalization(y, bf=inference_base_features):\n",
    "        return bf.target_normalizer.transform_back(bf.data, y)\n",
    "\n",
    "    fc_inference = FeatureCombiner(config, inference_base_features).fit()\n",
    "\n",
    "    df_output = []\n",
    "    for horizon in range(1, 7):\n",
    "        model = mlflow.sklearn.load_model(model_paths[horizon])\n",
    "        X, y = fc_inference.transform(horizon=horizon, mode=mode)\n",
    "        y_pred = model.predict(X)\n",
    "        df = inference_base_features.data.loc[:, ['t', 'datetime', 'zone_id']]\n",
    "        df['h'] = horizon\n",
    "        df['pred'] = y_pred\n",
    "        df['fact'] = y\n",
    "        df_output.append(df)\n",
    "    df_output = pd.concat(df_output)\n",
    "    print(f'{datetime.datetime.now()}: predictions are updated, {period}')\n",
    "    return df_output\n",
    "\n",
    "# mlflow client\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "config = mlflow.artifacts.load_dict(ARTIFACT_URI + \"/configs/features_conf.json\")\n",
    "\n",
    "model_paths = dict()\n",
    "for horizon in range(1, 7):\n",
    "    model_paths[horizon ] = f'models:/fourie_plus_cartesian_ar_h{horizon}/Staging'\n",
    "\n",
    "year, month = 2022, 5\n",
    "period = [datetime.datetime(year, month, 1, 1, 0) - datetime.timedelta(days=14), \n",
    "          datetime.datetime(year, month, 31, 23, 0)]\n",
    "results_month = update_predictions(period, 'validation', model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "40e0a068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>datetime</th>\n",
       "      <th>zone_id</th>\n",
       "      <th>h</th>\n",
       "      <th>pred</th>\n",
       "      <th>fact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>29184</td>\n",
       "      <td>2022-05-01 00:00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045421</td>\n",
       "      <td>0.043956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>29185</td>\n",
       "      <td>2022-05-01 01:00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.033357</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>29186</td>\n",
       "      <td>2022-05-01 02:00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.024875</td>\n",
       "      <td>0.010989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>29187</td>\n",
       "      <td>2022-05-01 03:00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.044861</td>\n",
       "      <td>0.021978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>29188</td>\n",
       "      <td>2022-05-01 04:00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.040285</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59389</th>\n",
       "      <td>29917</td>\n",
       "      <td>2022-05-31 13:00:00</td>\n",
       "      <td>264</td>\n",
       "      <td>6</td>\n",
       "      <td>0.408268</td>\n",
       "      <td>0.481752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59390</th>\n",
       "      <td>29918</td>\n",
       "      <td>2022-05-31 14:00:00</td>\n",
       "      <td>264</td>\n",
       "      <td>6</td>\n",
       "      <td>0.408930</td>\n",
       "      <td>0.605839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59391</th>\n",
       "      <td>29919</td>\n",
       "      <td>2022-05-31 15:00:00</td>\n",
       "      <td>264</td>\n",
       "      <td>6</td>\n",
       "      <td>0.452075</td>\n",
       "      <td>0.613139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59392</th>\n",
       "      <td>29920</td>\n",
       "      <td>2022-05-31 16:00:00</td>\n",
       "      <td>264</td>\n",
       "      <td>6</td>\n",
       "      <td>0.504579</td>\n",
       "      <td>0.532847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59393</th>\n",
       "      <td>29921</td>\n",
       "      <td>2022-05-31 17:00:00</td>\n",
       "      <td>264</td>\n",
       "      <td>6</td>\n",
       "      <td>0.360109</td>\n",
       "      <td>0.328467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243540 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           t            datetime  zone_id  h      pred      fact\n",
       "336    29184 2022-05-01 00:00:00       13  1  0.045421  0.043956\n",
       "337    29185 2022-05-01 01:00:00       13  1  0.033357  0.000000\n",
       "338    29186 2022-05-01 02:00:00       13  1  0.024875  0.010989\n",
       "339    29187 2022-05-01 03:00:00       13  1  0.044861  0.021978\n",
       "340    29188 2022-05-01 04:00:00       13  1  0.040285  0.000000\n",
       "...      ...                 ...      ... ..       ...       ...\n",
       "59389  29917 2022-05-31 13:00:00      264  6  0.408268  0.481752\n",
       "59390  29918 2022-05-31 14:00:00      264  6  0.408930  0.605839\n",
       "59391  29919 2022-05-31 15:00:00      264  6  0.452075  0.613139\n",
       "59392  29920 2022-05-31 16:00:00      264  6  0.504579  0.532847\n",
       "59393  29921 2022-05-31 17:00:00      264  6  0.360109  0.328467\n",
       "\n",
       "[243540 rows x 6 columns]"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "70e44ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 55 artists>"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGdCAYAAAA1/PiZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjgElEQVR4nO3df3BU1f3/8dfuhmXz6zMJCcOHVklCB2WqVkJQQAc6E2QYoVSlYrVYLa0gxfIrtT+EkVJHZuynKAxoseKvmdrpVFutFgdMyzDiTKNIBG1rRQwk0OKXmrChkOyyZPd+/6BZ2PzczZ5k9+w+HzMZ3XvP7j28793d15579q7LcRxHAAAAFnCnugMAAADxIrgAAABrEFwAAIA1CC4AAMAaBBcAAGANggsAALAGwQUAAFiD4AIAAKxBcAEAANYguAAAAGvkpLoDg+E//wkoHI4M6jaKi/Pl97cN6jayBbU0gzqaQR3NoI7mZEMtPR63/ud/cuNqm5HBJRyOqKNj8IKLy3VhO/zSU3KopRnU0QzqaAZ1NIdadsepIgAAYA2CCwAAsAbBBQAAWIPgAgAArEFwAQAA1iC4AAAAaxBcAACANQguAADAGgQXAABgDYILAACwBsEFAABYg+ACAACsQXABAADWyMhfhwYAAMnp/GXqrlL9K9UEFwAAEKN4RL5yPD2flOkIR+Q/2TbEPbqA4AIAAKJcLinH49Yf/vapzoUjMeuGedy6+crRcrlSN/JCcAEAAN2cC0fUEemaTiI9th1KTM4FAADWILgAAABrEFwAAIA1CC4AAMAaBBcAAGANggsAALAGwQUAAFjDaHCpr6/X3LlzNWHCBC1cuFDNzc3d2nz22WdauHChKisrNWfOHO3fv79bm23btmnNmjUmuwYAADKAseASDAa1fPlyLV++XHv37lVZWZkeeeSRbu0efPBBjR8/Xu+8844WL16smpoahcNhSVIoFNKmTZv06KOPmuoWAADIIMaCS11dnUaNGqWZM2fK6/Vq5cqVeuONN9Te3h5tc+bMGb311ltaunSpvF6vbrrpJhUWFurtt9+WJD388MP68MMPdfvtt5vqFgAAyCDGgktTU5PKy8ujt4uKipSXl6ejR49Glx09elTFxcUqLCyMLisvL1dDQ4MkadmyZXrqqadUUlJiqlsAACCDGPutovb2dg0fPjxmWW5uroLBYJ9tfD5ftM3IkSNNdafXn+M2+diDuY1sQS3NoI5mUEczqKM5qahlPNtK5b41Flxyc3MVCoVilgUCAeXl5cW0OXv2bEybYDAY08aE4uJ8o4/Xm5KSwv4bIS7U0gzqaAZ1NIM6mpNutUxlf4wFl4qKCm3fvj16u7W1VW1tbRozZkx0WVlZmVpbW3XmzBkVFBRIko4cOWJ8Tovf36ZwePB+wdLlOr/TWlpOp+xnvTMFtTSDOppBHc2gjuakopad2+yL6f54PO64Bx2MBZcpU6Zo9erV2rFjh2bMmKFNmzapurpaPp8v2qagoEDXX3+9Nm/erPvvv187d+5Ua2urJk2aZKobUUOxgx1naLaTDailGdTRDOpoBnU0J91qmcr+GJuc6/P5tHXrVj355JOaPHmyjh07pnXr1un48eOqrKzU8ePHJZ3/5lBjY6OmTp2qp59+Wk888YS8Xq+pbgAAgAzmcpx0ynBm+P1t6ugY3FNFpaWFam5mGDRZ1NIM6mgGdTSDOpqTilp2bvOl9/+ljkjsRnPcLs2/+vPG+5OTE/+pIi75DwAArGFsjgsA83r7yiGfYgFkK4ILkKaKR+Qrx9PzoGhHOCL/ybYh7hEApB7BBUhDLpeU43HrD3/7VOe6fLV/mMetm68cLZeLkRcA2YfgAqSxc+FIt8lx0uBNPAeAdMfkXAAAYA2CCwAAsAbBBQAAWIPgAgAArEFwAQAA1iC4AAAAaxBcAACANQguAADAGgQXAABgDYILAOj8zyx0/qjlxf8PIL1wyX8AWa/rD1qWlBRK4scsgXREcAGQ1Xr7QUt+zBJITwQXAFBPP2jJj1kC6Yg5LgAAwBoEFwAAYA2CCwAAsAbBBQAAWIPgAgAArEFwAQAA1iC4AAAAaxBcAACANQguAADAGgQXAABgDYILAACwBsEFAABYg+ACAACsQXABAADWILgAAABr5KS6Axh6LlfPyx1naPsBAECiCC5ZpnhEvnI8PQ+0dYQj8p9sG+IeAQAQP4JLFnG5pByPW3/426c6F47ErBvmcevmK0fL5WLkBQCQvgguWehcOKKOSNd0EumxLQAA6YTJuQAAwBqMuCAlOicId/0vp6kAAH0huGDI9TRBuKSkUBIThAEAfSO4YEgxQRgAkAyCC1KCCcIAgIEguAAAMgoX2cxsBBcAQMbgIpuZj+ACAMgIzKHLDgQXAEBGYQ5dZuMCdAAAwBoEFwAAYA2CCwAAsAbBBQAAWIPgAgAArEFwAQAA1iC4AAAAaxBcAACANQguAADAGgQXAABgDYILAACwBsEFAABYgx9ZBABEuVzdl/FrykgnBBcAgCSpeES+cjzdB+I7whH5T7aloEdAdwQXAIBcLinH49Yf/vapzoUj0eXDPG7dfOVouVyMvGSankbXbEBwAQBEnQtH1BG5OKFEem2L9NbXab/eRtek8yNs6YzgAgCwCvNw+tfXab9Wf1uPo2vShRG2dEZwAQBYo783ZMR32k/qaXRNsmGEjeACALBCvG/IOC9TT/sRXADLuVwXhs47/8uwOTJZpr4hIz5GL0BXX1+vuXPnasKECVq4cKGam5u7tfnss8+0cOFCVVZWas6cOdq/f39C94cdOt9Mu/7BrOIR+SotLVRJSaEkqaSkUKWlhSoekZ/ingHA4DAWXILBoJYvX67ly5dr7969Kisr0yOPPNKt3YMPPqjx48frnXfe0eLFi1VTU6NwOBz3/ZH+Ot9Me/orKuYN1ZSLh81fev9f0b8//O1T5XjcGRUUCcIAOhk7VVRXV6dRo0Zp5syZkqSVK1dq2rRpeuihh5SXlydJOnPmjN566y39/Oc/l9fr1U033aRnnnlGb7/9tkKhUL/3R/rr7Ry0ZMdsdRtl+rB5f1/b5MJoQHYxFlyamppUXl4evV1UVKS8vDwdPXpU48ePlyQdPXpUxcXFKiwsjLYrLy9XQ0ODIpFIv/dPxGB9Grv4k57bfWEuQed/e9tuvOt7a5Ps+osN87jV9c1t2H/fGLrOlUi0D/HWvb8+pLJOtu2rrm1M7MuL26SyTp1B+PUP/5/Odfn2wzC3S3O++L9y9zFunMgxO5A6XryNVNXJVB9MHk/JvEb2tY14+5js60s6vf50reVQ1am/9akc8TQWXNrb2zV8+PCYZbm5uQoGg3228fl8CgaD6ujo6Pf+8SoepNMR4Ygjj/vC3hoxojBmnaSY9V3v2996j9vVbRum1ne2CUecXkc9whFHJSWFRvrQ1zYk9bm+qLgg5XWS0n9fjRhR2GubZPdlZxspPY7pOV/83163Yep4GUgdO9tIqauT6T6YPJ4SfY1Mdl/19ZzobNPf8WKqj8nsy57WXVzLvtqZqpPU9+t057y6VDAWXHJzcxUKhWKWBQKBmNM8ubm5Onv2bEybYDCovLw8nTt3rt/7x8vvb1PY8JX/XK7zEx/7OwWSzPqTJ09rxIju20h2/cVtWlpO9/pv7EzqPf074+1DS8vpPkdeevo0MWJEoU6ePB1d11+dh6JOUvrvq06dnyAvrmMy+zKROpy/wmbPO9txHLW0nBnU40lK/ni5eBuJ1DGROvW3fiiO2f7W91XroTqeOh+j6whb5+haf33slMzxEm8fh+q5P5DndrJ1SmTkyRSPxx33oIOx4FJRUaHt27dHb7e2tqqtrU1jxoyJLisrK1Nra6vOnDmjgoICSdKRI0d0++23KxQK9Xv/RAzW10H7u2BPMus7+9zbnIWBr49tE09tkulDPNvo+sSIRGKHOIeiTv2xZV9dXLfOOvb974i/D/3p64Jffe9PM8eTqePl4jcEKd46XniMZNebOF5M9CGeY24w+xiJnJ+31NMIW0c4osh/Hybe15eu4r3wWjo99wf63E7kdXig61PFWHCZMmWKVq9erR07dmjGjBnatGmTqqur5fP5om0KCgp0/fXXa/Pmzbr//vu1c+dOtba2atKkSYpEIv3eHzDBcc6/CPY2DNoRjvQ6GTTb9FendH1hg738J9u4pD/6ZOzV2efzaevWrXryySc1efJkHTt2TOvWrdPx48dVWVmp48ePS5IefvhhNTY2aurUqXr66af1xBNPyOv19nr/bDTM41aO2xX9G8abqHH+k21qbj7d4x+XDb+g1d97nfg2D3rS9fVrIK9hF48YxDvyiOxh9Mq5V199tV599dVuyy++yNzIkSP11FNPJXR/m/Q3S7svfY0E8OnWvP6Gk3Eexx3iwUjm0OvtW0OZjkv+G2TiSdvXMClvqBiIbH1x64o6DL7O16/OyaOdk0Q7lZam7psomSTbP+QSXAzq7xRDvE/aTD/oMDSy/cWtE3UYWhfXs6dJpjAjm+cCEVwM4/SDGcmccsMFjOCdl80v8shc2Xr8ElyQVuI5T56tT9aBol7nZUIdkrnSKZApCC5IO719Opayb6QAkOIL9FLvVzplYiwyCcEFaSkTPh1LTAiFOf0FeqnvUM/EWGQKggvQC77ajnST7BVjgUxAcAG6MDXPhomxAGAewQXogal5NoysIBGcWgT6R3DJQHyzwAxCB4YKpxaB+BFcMghfJY7Fp1fYhGvNZBc+YA4cwSXDxPPNg0zHp9fskylvAjYcm5nwgSCVxwsfMJNHcMlAHPR8es0WvAkMnUz4QJAuPwTJB8zkEFyQsXgByA68CQydTPhA0NfxIsV/vZtkR55sqlm6IbgAsB5vAkMnE2qdzPVuMmHkyXYEFyCFMmVuRjZIdl+xrzMH12hKLYILkALpNDeDN9S+mZgXkS77Guawz1KH4AKkSKrnZqRTeEp38cyL6CsAtvrber0vNU4/hPn0RnABkmD7BL2hCk+Z8BXavupBAMwMhHk7EFyAAcikCXqD2ddMqlNfUj16BnPYl+mP4AIMUCZ8NXQoZMtERvZ75mBfpjeCCwYkE4b+TeAFLj7UKT0wdwOZgOCChGTL0D+QSZi7gUxCcEHCOEUC2Ie5G8gUBBcMCC90gH143iITcHITAABYg+ACAACswakiAMCQ4ZtNSBbBBQAw6PhmE0whuAAAhgTfbIIJBBcAwJAhoCBZBBf0iCvjAgDSEcEFMbgyLgAgnRFc0A1XxsVQY4QPQLwILugRIQVDgRE+AIkiuABIKUb4ACSC4AIg5QgpAOLFiWQAAGANggsAALAGwQUAAFiD4AIAAKzB5FxYi2t/AED2IbjAOlz7AwCyF8EFVuLaHwCQnQgusBYhBQCyD5MCAACANQguAADAGgQXAABgDYILAACwBsEFAABYg+ACAACsQXABAADWILgAAABrEFwAAIA1CC4AAMAaBBcAAGANggsAALAGwQUAAFiD4AIAAKxBcAEAANYguAAAAGsQXAAAgDVyUt2BbDPM45YU6WEZAADoD8FliDiO1BGO6OYrR/e4viMckeMMcacAALAMwWUI+U+2yeXqeR2hBQCA/hFchhgBBQCAgTM2uaKhoUG33XabJkyYoPnz5+vw4cM9tmtvb9eKFSs0ceJEVVdXa9euXd3a7NixQwsXLjTVNQAAhtQwj1s5blf0j7mM5hgZcXEcR6tWrdLXvvY1vfDCC3ruuee0Zs0a/eY3v+nW9rHHHpPb7VZdXZ3ee+89rVixQrt27VJhYaEcx9ELL7yg//u//1NVVZWJrgEAMGT6ms/IXEYzjETAQ4cO6cSJE7rrrrvk9Xq1aNEiNTQ0qLGxsVvb7du3a8mSJRo+fLimTp2qqqoq7dixQ5K0bds2/fGPf9R3vvMdE90CAGDI+U+2qbn5dLc//8m2VHctIxgZcWlqalJ5eblc/5156na7dckll6ihoUHl5eXRdqdOnZLf71dFRUV0WXl5uRoaGiRJt9xyixYvXqyXX35ZBw4cSKpPvU2CHczHM73NRB9/sLc/GDr7bGPf0wl1NIM6mkEdezaQelDL7hIKLnv27NGiRYu6LR8zZoxGj44dFsvNzVUwGIxZFggE5HK55PV6o8t8Pp9aWlokSSNHjkykO70qLs438jiJKikpTMl202X7ybC57+mEOppBHc2gjuZQywsSCi7Tp0/XwYMHuy2vra3Vs88+G7MsEAgoLy8vZpnP55PjOAqFQtHwEgwGlZ9vNmj4/W0KhyP9N0yAy9X/gdPScnpQz1/214fB3v5g6Pw32dj3dEIdzaCOZlBHc7Kllh6PO+5BByOniioqKtTU1CTHceRyuRSJRHTs2DGNHTs2pl1RUZGKi4vV1NSkcePGSZKOHDmiGTNmmOhGjFTsYMdJ7dedU739ZNjc93RCHc2gjmZQR3Oo5QVGJueOGzdOpaWlev755xUKhbRt2zZdeumlKisr69Z29uzZ2rJliwKBgOrq6lRfX6/q6moT3QAAABnO2BfLN2/erNraWk2ePFm7d+/Wxo0bo+sqKyu1b98+SVJNTY28Xq+mT5+utWvXasOGDSopKTHVDQAAkMGMXTm3oqKix+u2SNL+/fuj/19QUKANGzb0+Vjz5s3TvHnzTHUNAABkCC7lBwAArEFwAQAA1iC4AAAAaxBcAACANQguAADAGgQXAABgDYILAACwBsEFAABYg+ACAACsQXABAADWILgAAABrGPutIgydYR63pEiX2wAAZD6Ci0UcR+oIR3TzlaO7resIR+Q4KegUAABDiOBiGf/JNrlc3ZcTWgAA2YDgYiFCCgAgWzE5AgAAWIPgAgAArEFwAQAA1iC4AAAAaxBcAACANQguAADAGgQXAABgDYILAACwBsEFAABYg+ACAACsQXABAADWILgAAABrEFwAAIA1CC4AAMAaBBcAAGANggsAALAGwQUAAFiD4AIAAKxBcAEAANYguAAAAGsQXAAAgDUILgAAwBoEFwAAYA2CCwAAsAbBBQAAWIPgAgAArEFwAQAA1iC4AAAAaxBcAACANQguAADAGgQXAABgDYILAACwBsEFAABYg+ACAACsQXABAADWILgAAABrEFwAAIA1CC4AAMAaBBcAAGANggsAALAGwQUAAFiD4AIAAKxBcAEAANYguAAAAGsQXAAAgDUILgAAwBoEFwAAYA2CCwAAsAbBBQAAWIPgAgAArEFwAQAA1jAWXBoaGnTbbbdpwoQJmj9/vg4fPtxju/b2dq1YsUITJ05UdXW1du3aFV1XV1enm266SRMnTtQtt9yiffv2meoeAADIAEaCi+M4WrVqlebMmaO9e/fqhhtu0Jo1a3ps+9hjj8ntdquurk7r16/XAw88oNOnT6u1tVUrVqxQTU2N9u3bp29/+9u677771N7ebqKLAAAgAxgJLocOHdKJEyd01113yev1atGiRWpoaFBjY2O3ttu3b9eSJUs0fPhwTZ06VVVVVdqxY4eOHz+uG2+8UV/+8pfldrs1d+5cSdLRo0dNdBEAAGSAHBMP0tTUpPLycrlcLkmS2+3WJZdcooaGBpWXl0fbnTp1Sn6/XxUVFdFl5eXl0dNMP/3pT6PLP/jgA509e1ZjxowZUJ/+2xVj4nk809vMBp01o3bJoY5mUEczqKM51LK7hILLnj17tGjRom7Lx4wZo9GjR8csy83NVTAYjFkWCATkcrnk9Xqjy3w+n1paWmLaHT9+XCtWrNCqVauUl5eXSBclScXF+Qnfx4SSksKUbDcTUDszqKMZ1NEM6mgOtbwgoeAyffp0HTx4sNvy2tpaPfvsszHLAoFAt9Dh8/nkOI5CoVA0vASDQeXnXwgaH330kRYtWqT58+fr7rvvTqR7UX5/m8LhyIDu2xuXq/8Dp6XltBzH6GYzXmddqV1yqKMZ1NEM6mhOttTS43HHPehg5FRRRUWFmpqa5DiOXC6XIpGIjh07prFjx8a0KyoqUnFxsZqamjRu3DhJ0pEjRzRjxgxJUn19vZYsWaKVK1dqwYIFSfUpFTvYcVKz3UxA7cygjmZQRzOooznU8gIjk3PHjRun0tJSPf/88wqFQtq2bZsuvfRSlZWVdWs7e/ZsbdmyRYFAQHV1daqvr1d1dbWam5u1dOlSrV69OunQAgAAMpOx67hs3rxZtbW1mjx5snbv3q2NGzdG11VWVkavyVJTUyOv16vp06dr7dq12rBhg0pKSvTKK6+otbVVDz30kCorK6N/+/fvN9VFAABgOZfjZN7gk9/fpo4O83NcSksL9dL7/1JHJLZkOW6X5l/9eTU3Z/Y5yMHQWVdqlxzqaAZ1NIM6mpMttczJiX+OC5f8BwAA1iC4AAAAaxBcAACANQguAADAGgQXAABgDYILAACwBsEFAABYg+ACAACsQXABAADWILgAAABrEFwAAIA1CC4AAMAaBBcAAGANggsAALAGwQUAAFiD4AIAAKxBcAEAANYguAAAAGsQXAAAgDUILgAAwBoEFwAAYA2CCwAAsAbBBQAAWIPgAgAArEFwAQAA1iC4AAAAaxBcAACANQguAADAGgQXAABgDYILAACwBsEFAABYg+ACAACsQXABAADWILgAAABrEFwAAIA1CC4AAMAaBBcAAGANggsAALAGwQUAAFiD4AIAAKxBcAEAANYguAAAAGsQXAAAgDUILgAAwBoEFwAAYA2CCwAAsAbBBQAAWIPgAgAArEFwAQAA1iC4AAAAaxBcAACANQguAADAGgQXAABgDYILAACwBsEFAABYg+ACAACsQXABAADWILgAAABrEFwAAIA1CC4AAMAaBBcAAGANggsAALAGwQUAAFiD4AIAAKxBcAEAANYwFlwaGhp02223acKECZo/f74OHz7cY7v29natWLFCEydOVHV1tXbt2hVd9+abb2r27NmqrKzUvHnzdODAAVPdAwAAGcBIcHEcR6tWrdKcOXO0d+9e3XDDDVqzZk2PbR977DG53W7V1dVp/fr1euCBB3T69GmdOXNGq1at0rp167R//37dcccdqqmpMdE9AACQIYwEl0OHDunEiRO666675PV6tWjRIjU0NKixsbFb2+3bt2vJkiUaPny4pk6dqqqqKu3YsUMFBQV66623dO211yoUCunUqVMqKioy0T0AAJAhckw8SFNTk8rLy+VyuSRJbrdbl1xyiRoaGlReXh5td+rUKfn9flVUVESXlZeXq6GhQZKUn5+vY8eOadasWXK73dq6dauJ7gEAgAyRUHDZs2ePFi1a1G35mDFjNHr06Jhlubm5CgaDMcsCgYBcLpe8Xm90mc/nU0tLS/T26NGjdeDAAdXW1mrFihX685//rBEjRiTSTUnSfzOUMfE8nultZoPOmlG75FBHM6ijGdTRHGrZXULBZfr06Tp48GC35bW1tXr22WdjlgUCAeXl5cUs8/l8chxHoVAoGl6CwaDy8/MvdCjnfJe+8pWv6JlnntG7776rWbNmJdJNFRfn999oEJSUFKZku5mA2plBHc2gjmZQR3Oo5QVGThVVVFSoqalJjuPI5XIpEono2LFjGjt2bEy7oqIiFRcXq6mpSePGjZMkHTlyRDNmzNBHH32ktWvX6sUXX4y2D4VCKixMfGf5/W0KhyPJ/aO6cLn6P3BaWk7LcYxuNuN11pXaJYc6mkEdzaCO5mRLLT0ed9yDDkaCy7hx41RaWqrnn39eCxYs0HPPPadLL71UZWVl3drOnj1bW7Zs0c9+9jMdOHBA9fX1Wr9+vQoLC/Xvf/9bL730kubNm6eXX35ZgUBAEydOHFCfUrGDHSc1280E1M4M6mgGdTSDOppDLS8wdh2XzZs3q7a2VpMnT9bu3bu1cePG6LrKykrt27dPklRTUyOv16vp06dr7dq12rBhg0pKSuT1evWLX/xCL774oq699lq9+uqr2rZtm3w+n6kuAgAAy7kcJ/MynN/fpo4O86eKSksL9dL7/1JHJLZkOW6X5l/9eTU3Z/ZQ3mDorCu1Sw51NIM6mkEdzcmWWubkxH+qiEv+AwAAaxBcAACANQguAADAGgQXAABgDYILAACwBsEFAABYg+ACAACsQXABAADWILgAAABrEFwAAIA1CC4AAMAaBBcAAGCNnFR3wDbDPG5JkR6WAQCAwUZwiZPjSB3hiG6+cnSP6zvCkYz+5U4AANIBwSUB/pNtcrnO/8x4SUmhWlou/Mw4oQUAgMFHcEnQxQHFcQgsAAAMJSZnAAAAaxBcAACANQguAADAGgQXAABgDYILAACwBsEFAABYg+ACAACsQXABAADWILgAAABrEFwAAIA1CC4AAMAaBBcAAGCNjPyRRY9naPLYUG0nG1BLM6ijGdTRDOpoTqbXMpF/n8tx+H1jAABgh8yOcAAAIKMQXAAAgDUILgAAwBoEFwAAYA2CCwAAsAbBBQAAWIPgAgAArEFwAQAA1iC4AAAAaxBcAACANQguAADAGgSXBNXX12vu3LmaMGGCFi5cqObm5lR3yTrbtm3TmjVrord/+9vfatq0aaqqqtK6desUDodT2Lv09/rrr2vWrFmqqqrSggUL9Mknn0iijgPx8ssvq7q6WpWVlfrmN7+pI0eOSKKWA/Xuu+9q/Pjx0dvUMTFr167VVVddpcrKSlVWVurWW2+VRB27cRC3QCDgXHfddU5tba1z9uxZ5yc/+Ynz/e9/P9XdssbZs2edjRs3OpdffrmzevVqx3Ec569//atz3XXXOZ988onT0tLi3Hrrrc6LL76Y4p6mr08++cS55pprnA8++MDp6OhwfvnLXzqzZs2ijgNw+PBh55prrnEOHjzohMNhZ9OmTc6dd95JLQcoEAg4s2bNci677DLHcXhuD8TXv/515y9/+UvMMurYHSMuCairq9OoUaM0c+ZMeb1erVy5Um+88Yba29tT3TUrPPzww/rwww91++23R5e9/vrrmjt3rr7whS9oxIgRWrx4sX7/+9+nsJfp7fjx47rzzjt11VVXyePxaMGCBTpy5Ihee+016pigiooK7d69W5dddpmCwaDOnDmj4uJijskB2rRpk6ZNmxa9TR0T4ziOPv74Y11++eUxy6ljdwSXBDQ1Nam8vDx6u6ioSHl5eTp69GjqOmWRZcuW6amnnlJJSUl0WWNjY0xNy8rKdPjw4RT0zg7Tpk3T8uXLo7fffPNNfe5zn9OxY8eo4wDk5+frnXfeUVVVlV555RV997vf5ZgcgAMHDui9997Tt771regy6piYf/7znzp37px++MMfasqUKbr77rvV0NBAHXtAcElAe3u7hg8fHrMsNzdXwWAwRT2yy8iRI7stCwQC8vl80du5ubkKBAJD2S1r/eMf/9C6deu0evVq6piEyspKvf/++7r33nu1ZMkStbW1UcsEhEIhrV27Vg899JA8Hk90OcdkYv7zn/9o0qRJqqmp0Z49e3TNNddo6dKl1LEHBJcE5ObmKhQKxSwLBALKy8tLUY/s5/P5dPbs2eht6hmfuro63X333frBD36gmTNnUsckeL1eeb1e3XPPPQoGg8rLy6OWCdiyZYuqq6tjJuVKPLcTdcUVV+i5557TF7/4RXm9Xt13331qbm6W2+2mjl0QXBJQUVGhxsbG6O3W1la1tbVpzJgxqeuU5brWtLGxUWPHjk1dhyzwxhtv6Hvf+57Wr1+v+fPnS6KOA/Hmm29q2bJl0duRSETnzp2Tx+Ohlgn405/+pF/96leaNGmS5syZI0maNGmSiouLqWMC9u3bp9/97nfR25FIROFwWAUFBdSxC4JLAqZMmaJPP/1UO3bsUCgU0qZNm1RdXR0zjIfE3HjjjXrttdf08ccfy+/3a9u2bdEXP3R36NAh/fjHP9bjjz+umTNnRpdTx8RdccUVevvtt7Vnzx6dO3dOjz/+uMaNG6d7772XWiZg586dqq+v1759+/T6669LOv8mfMcdd1DHBHg8Hj3yyCP6+9//rlAopEcffVSXX3657rnnHurYRU6qO2ATn8+nrVu3au3atVq9erUmTpyon//856nultW+9KUvaeXKlbr33nvV1tamr371q/rGN76R6m6lrV//+tcKBoNaunRpzPKdO3dSxwSVlpZq8+bNWr9+vU6cOKGqqipt3rxZo0aNopYG8NxOTGVlpX70ox9p2bJl8vv9mjhxojZt2qTRo0dTxy5cjuM4qe4EAABAPDhVBAAArEFwAQAA1iC4AAAAaxBcAACANQguAADAGgQXAABgDYILAACwBsEFAABYg+ACAACsQXABAADWILgAAABrEFwAAIA1/j8gojOf+Yi9cgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "zone_id = []\n",
    "mae_ridge = []\n",
    "for g in results_month.groupby('zone_id'):\n",
    "    zone_id.append(g[0])\n",
    "    mae_ridge.append(mean_absolute_error(g[1].loc[:, 'fact'].values, g[1].loc[:, 'pred'].values))\n",
    "\n",
    "mae_ridge = pd.Series(mae_ridge, index=zone_id)\n",
    "\n",
    "\n",
    "zone_id = []\n",
    "mae = []\n",
    "for g in df.groupby('zone_id'):\n",
    "    zone_id.append(g[0])\n",
    "    mae.append(mean_absolute_error(g[1].iloc[:, -12:-6].unstack().values, g[1].iloc[:, -6:].unstack().values))\n",
    "\n",
    "mae = pd.Series(mae, index=zone_id)\n",
    "\n",
    "plt.bar(range(55), mae[zones] - mae_ridge[zones])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58e4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_id = []\n",
    "mae = []\n",
    "for g in df.groupby('zone_id'):\n",
    "    zone_id.append(g[0])\n",
    "    mae.append(mean_absolute_error(g[1].iloc[:, -12:-6].unstack().values, g[1].iloc[:, -6:].unstack().values))\n",
    "\n",
    "mae = pd.Series(mae, index=zone_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hard_ml",
   "language": "python",
   "name": "hard_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
