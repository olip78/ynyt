{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94418bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu!\n",
      "epoch: 0, train loss: 1.517988, val loss: 0.144604, val mae: 0.295849, val r2: -2.670630\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m path_preprocessed \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./../_ynyt/data/preprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m horizon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperiod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperiod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_preprocessed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_preprocessed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mregression_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregression_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_setting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_setting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhorizon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/andrei/ynyt/ynyt/transformers/sttre/sttre.py:359\u001b[0m, in \u001b[0;36mtrain_val\u001b[0;34m(period, epoches, path_preprocessed, embed_size, heads, num_layers, dropout, forward_expansion, lr, batch_size, seq_len, regression_head, data_setting, device, verbose, verbose_step, horizon)\u001b[0m\n\u001b[1;32m    357\u001b[0m output \u001b[38;5;241m=\u001b[39m model(inputs\u001b[38;5;241m.\u001b[39mto(device), regressors\u001b[38;5;241m.\u001b[39mto(device), dropout)\n\u001b[1;32m    358\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, labels\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m--> 359\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    361\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m+\u001b[39m loss\n",
      "File \u001b[0;32m~/miniforge3/envs/hard_ml/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/hard_ml/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "import sys\n",
    "\n",
    "print(sys.argv[1:]\n",
    "\n",
    "filename = 'exp_1_1.json'\n",
    "with open(f'./settings/{filename}', 'r') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3886e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "filename = sys.argv[0]\n",
    "with open(f'./settings/{filename}', 'r') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "path_preprocessed = './../data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed52ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 24,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 2,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 6, \n",
    "                      'epoches': 500,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "filename = 'exp_1_8.json'\n",
    "\n",
    "with open(f'./settings/{filename}', 'w') as f:\n",
    "    json.dump(settings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15e8c62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"regression_head\": {\"heads\": 1, \"dropout_head\": 0.1, \"layers\": [6], \"add_features\": 2, \"flatt_factor\": 2}, \"data_setting\": {\"features\": true, \"D\": true, \"hours\": true, \"weekday\": true}, \"params\": {\"embed_size\": 32, \"heads\": 4, \"num_layers\": 2, \"dropout\": 0.15, \"forward_expansion\": 1, \"lr\": 1e-05, \"batch_size\": 128, \"seq_len\": 6, \"epoches\": 300, \"device\": \"cpu\"}}"
     ]
    }
   ],
   "source": [
    "!cat ./settings/exp_1_5_ok1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80ea82af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"regression_head\": {\"heads\": 1, \"dropout_head\": 0.1, \"layers\": [16, 6], \"add_features\": 2, \"flatt_factor\": 2}, \"data_setting\": {\"features\": true, \"D\": true, \"hours\": true, \"weekday\": true}, \"params\": {\"embed_size\": 32, \"heads\": 4, \"num_layers\": 2, \"dropout\": 0.15, \"forward_expansion\": 1, \"lr\": 1e-05, \"batch_size\": 128, \"seq_len\": 6, \"epoches\": 300, \"device\": \"cpu\"}}"
     ]
    }
   ],
   "source": [
    "!cat ./settings/exp_1_6_up_500.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10b3fd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_1_1.json        exp_1_4.json        exp_1_6.json\r\n",
      "exp_1_2.json        exp_1_5.json        exp_1_6_up_500.json\r\n",
      "exp_1_3.json        exp_1_5_ok1.json    exp_1_7.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "                       0.04457, r2: test: 0.90793, 0.96273, train: 0.91653, 0.96253\n",
    "t=2, mse: 0.0047, mae: 0.04805, r2: test: 0.89385, 0.95456, train: 0.90326, 0.95393\n",
    "t=3, mse: 0.00506, mae: 0.04958, r2: test: 0.88563, 0.95046, train: 0.89692, 0.94974\n",
    "t=4, mse: 0.00524, mae: 0.05035, r2: test: 0.8815, 0.94829, train: 0.89251, 0.94658\n",
    "t=5, mse: 0.0053, mae: 0.0509, r2: test: 0.88013, 0.947, train: 0.88920, 0.94407\n",
    "t=6, mse: 0.00535, mae: 0.05129, r2: test: 0.87887, 0.94622, train: 0.88689, 0.94230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18745f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04912333333333333"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.04457 + 0.04805 + 0.04958 + 0.05035 + 0.0509 + 0.05129) / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa2e04bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.887985"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.90793 + 0.89385 + 0.88563 + 0.8815 + 0.88013 + 0.87887) / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf3597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 24,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 2,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 6, \n",
    "                      'epoches': 500,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80f30d20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps!\n",
      "Transformer(\n",
      "  (element_embedding_temporal): Linear(in_features=294, out_features=192, bias=True)\n",
      "  (element_embedding_spatial): Linear(in_features=2695, out_features=1760, bias=True)\n",
      "  (pos_embedding): Embedding(6, 32)\n",
      "  (variable_embedding): Embedding(55, 32)\n",
      "  (temporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatial): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatiotemporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (keys): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (queries): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatter): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=11220, out_features=2640, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.15, inplace=False)\n",
      "    (3): Linear(in_features=2640, out_features=330, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch: 0, train loss: 0.019273, val loss: 0.017495, val mae: 0.104709, val r2: 0.556603\n",
      "epoch: 1, train loss: 0.011331, val loss: 0.014474, val mae: 0.091565, val r2: 0.633875\n",
      "epoch: 2, train loss: 0.009777, val loss: 0.013014, val mae: 0.086037, val r2: 0.670564\n",
      "epoch: 3, train loss: 0.008876, val loss: 0.012072, val mae: 0.082536, val r2: 0.694333\n",
      "epoch: 4, train loss: 0.008221, val loss: 0.011522, val mae: 0.080489, val r2: 0.708260\n",
      "epoch: 5, train loss: 0.007707, val loss: 0.010989, val mae: 0.078498, val r2: 0.721857\n",
      "epoch: 6, train loss: 0.007269, val loss: 0.010563, val mae: 0.076799, val r2: 0.733099\n",
      "epoch: 7, train loss: 0.006918, val loss: 0.010388, val mae: 0.076136, val r2: 0.737778\n",
      "epoch: 8, train loss: 0.006600, val loss: 0.010138, val mae: 0.075148, val r2: 0.744443\n",
      "epoch: 9, train loss: 0.006323, val loss: 0.009797, val mae: 0.073671, val r2: 0.753257\n",
      "epoch: 10, train loss: 0.006070, val loss: 0.009650, val mae: 0.073065, val r2: 0.757344\n",
      "epoch: 11, train loss: 0.005861, val loss: 0.009471, val mae: 0.072201, val r2: 0.762060\n",
      "epoch: 12, train loss: 0.005649, val loss: 0.009205, val mae: 0.070914, val r2: 0.768983\n",
      "epoch: 13, train loss: 0.005461, val loss: 0.009064, val mae: 0.070255, val r2: 0.772726\n",
      "epoch: 14, train loss: 0.005279, val loss: 0.008892, val mae: 0.069431, val r2: 0.777258\n",
      "epoch: 15, train loss: 0.005137, val loss: 0.008747, val mae: 0.068684, val r2: 0.781118\n",
      "epoch: 16, train loss: 0.004996, val loss: 0.008579, val mae: 0.067881, val r2: 0.785530\n",
      "epoch: 17, train loss: 0.004886, val loss: 0.008494, val mae: 0.067475, val r2: 0.787701\n",
      "epoch: 18, train loss: 0.004778, val loss: 0.008385, val mae: 0.066885, val r2: 0.790595\n",
      "epoch: 19, train loss: 0.004683, val loss: 0.008387, val mae: 0.066839, val r2: 0.790712\n",
      "epoch: 20, train loss: 0.004588, val loss: 0.008309, val mae: 0.066452, val r2: 0.792854\n",
      "epoch: 21, train loss: 0.004511, val loss: 0.008161, val mae: 0.065701, val r2: 0.796521\n",
      "epoch: 22, train loss: 0.004435, val loss: 0.008344, val mae: 0.066392, val r2: 0.792160\n",
      "epoch: 23, train loss: 0.004370, val loss: 0.008116, val mae: 0.065386, val r2: 0.797884\n",
      "epoch: 24, train loss: 0.004308, val loss: 0.008177, val mae: 0.065693, val r2: 0.796479\n",
      "epoch: 25, train loss: 0.004256, val loss: 0.008107, val mae: 0.065231, val r2: 0.798267\n",
      "epoch: 26, train loss: 0.004201, val loss: 0.008265, val mae: 0.065946, val r2: 0.794510\n",
      "epoch: 27, train loss: 0.004159, val loss: 0.008077, val mae: 0.065057, val r2: 0.799163\n",
      "epoch: 28, train loss: 0.004116, val loss: 0.008154, val mae: 0.065384, val r2: 0.797314\n",
      "epoch: 29, train loss: 0.004079, val loss: 0.008254, val mae: 0.065862, val r2: 0.794866\n",
      "epoch: 30, train loss: 0.004050, val loss: 0.008109, val mae: 0.065140, val r2: 0.798516\n",
      "epoch: 31, train loss: 0.004028, val loss: 0.007994, val mae: 0.064687, val r2: 0.801181\n",
      "epoch: 32, train loss: 0.003989, val loss: 0.007979, val mae: 0.064564, val r2: 0.801613\n",
      "epoch: 33, train loss: 0.003975, val loss: 0.007996, val mae: 0.064579, val r2: 0.801361\n",
      "epoch: 34, train loss: 0.003955, val loss: 0.007827, val mae: 0.063824, val r2: 0.805372\n",
      "epoch: 35, train loss: 0.003944, val loss: 0.007824, val mae: 0.063852, val r2: 0.805445\n",
      "epoch: 36, train loss: 0.003940, val loss: 0.007542, val mae: 0.062502, val r2: 0.812403\n",
      "epoch: 37, train loss: 0.003931, val loss: 0.007269, val mae: 0.061260, val r2: 0.818864\n",
      "epoch: 38, train loss: 0.003921, val loss: 0.007241, val mae: 0.060984, val r2: 0.819565\n",
      "epoch: 39, train loss: 0.003926, val loss: 0.006917, val mae: 0.059447, val r2: 0.827542\n",
      "epoch: 40, train loss: 0.003909, val loss: 0.006604, val mae: 0.057890, val r2: 0.835108\n",
      "epoch: 41, train loss: 0.003885, val loss: 0.006384, val mae: 0.056695, val r2: 0.840396\n",
      "epoch: 42, train loss: 0.003833, val loss: 0.006239, val mae: 0.055868, val r2: 0.843918\n",
      "epoch: 43, train loss: 0.003760, val loss: 0.006167, val mae: 0.055451, val r2: 0.845610\n",
      "epoch: 44, train loss: 0.003696, val loss: 0.006154, val mae: 0.055336, val r2: 0.845895\n",
      "epoch: 45, train loss: 0.003625, val loss: 0.006168, val mae: 0.055373, val r2: 0.845647\n",
      "epoch: 46, train loss: 0.003579, val loss: 0.006126, val mae: 0.055204, val r2: 0.846665\n",
      "epoch: 47, train loss: 0.003547, val loss: 0.006125, val mae: 0.055166, val r2: 0.846687\n",
      "epoch: 48, train loss: 0.003516, val loss: 0.006108, val mae: 0.055058, val r2: 0.847158\n",
      "epoch: 49, train loss: 0.003488, val loss: 0.006100, val mae: 0.054988, val r2: 0.847372\n",
      "epoch: 50, train loss: 0.003467, val loss: 0.006079, val mae: 0.054828, val r2: 0.847943\n",
      "epoch: 51, train loss: 0.003459, val loss: 0.006070, val mae: 0.054776, val r2: 0.848209\n",
      "epoch: 52, train loss: 0.003444, val loss: 0.006039, val mae: 0.054595, val r2: 0.848951\n",
      "epoch: 53, train loss: 0.003425, val loss: 0.006039, val mae: 0.054478, val r2: 0.848941\n",
      "epoch: 54, train loss: 0.003414, val loss: 0.006029, val mae: 0.054419, val r2: 0.849080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55, train loss: 0.003408, val loss: 0.006016, val mae: 0.054342, val r2: 0.849457\n",
      "epoch: 56, train loss: 0.003397, val loss: 0.006022, val mae: 0.054361, val r2: 0.849328\n",
      "epoch: 57, train loss: 0.003379, val loss: 0.006006, val mae: 0.054237, val r2: 0.849795\n",
      "epoch: 58, train loss: 0.003367, val loss: 0.005976, val mae: 0.054117, val r2: 0.850523\n",
      "epoch: 59, train loss: 0.003367, val loss: 0.005996, val mae: 0.054095, val r2: 0.850044\n",
      "epoch: 60, train loss: 0.003362, val loss: 0.005982, val mae: 0.053954, val r2: 0.850298\n",
      "epoch: 61, train loss: 0.003366, val loss: 0.005994, val mae: 0.054038, val r2: 0.850014\n",
      "epoch: 62, train loss: 0.003381, val loss: 0.006000, val mae: 0.054014, val r2: 0.849941\n",
      "epoch: 63, train loss: 0.003404, val loss: 0.006003, val mae: 0.054098, val r2: 0.849908\n",
      "epoch: 64, train loss: 0.003440, val loss: 0.006080, val mae: 0.054499, val r2: 0.848209\n",
      "epoch: 65, train loss: 0.003476, val loss: 0.006328, val mae: 0.055898, val r2: 0.842384\n",
      "epoch: 66, train loss: 0.003462, val loss: 0.006645, val mae: 0.057525, val r2: 0.834797\n",
      "epoch: 67, train loss: 0.003419, val loss: 0.006887, val mae: 0.058712, val r2: 0.829201\n",
      "epoch: 68, train loss: 0.003363, val loss: 0.007073, val mae: 0.059583, val r2: 0.824883\n",
      "epoch: 69, train loss: 0.003371, val loss: 0.007002, val mae: 0.059256, val r2: 0.826732\n",
      "epoch: 70, train loss: 0.003469, val loss: 0.006660, val mae: 0.057619, val r2: 0.834766\n",
      "epoch: 71, train loss: 0.003595, val loss: 0.006049, val mae: 0.054510, val r2: 0.849299\n",
      "epoch: 72, train loss: 0.003664, val loss: 0.005747, val mae: 0.052701, val r2: 0.856182\n",
      "epoch: 73, train loss: 0.003581, val loss: 0.005779, val mae: 0.052689, val r2: 0.855227\n",
      "epoch: 74, train loss: 0.003389, val loss: 0.005726, val mae: 0.052494, val r2: 0.856614\n",
      "epoch: 75, train loss: 0.003236, val loss: 0.005707, val mae: 0.052533, val r2: 0.857155\n",
      "epoch: 76, train loss: 0.003153, val loss: 0.005748, val mae: 0.052804, val r2: 0.856189\n",
      "epoch: 77, train loss: 0.003134, val loss: 0.005778, val mae: 0.052903, val r2: 0.855569\n",
      "epoch: 78, train loss: 0.003121, val loss: 0.005806, val mae: 0.053057, val r2: 0.854887\n",
      "epoch: 79, train loss: 0.003108, val loss: 0.005828, val mae: 0.053178, val r2: 0.854483\n",
      "epoch: 80, train loss: 0.003100, val loss: 0.005842, val mae: 0.053186, val r2: 0.854074\n",
      "epoch: 81, train loss: 0.003088, val loss: 0.005812, val mae: 0.053073, val r2: 0.854857\n",
      "epoch: 82, train loss: 0.003078, val loss: 0.005814, val mae: 0.053122, val r2: 0.854843\n",
      "epoch: 83, train loss: 0.003071, val loss: 0.005766, val mae: 0.052777, val r2: 0.855988\n",
      "epoch: 84, train loss: 0.003057, val loss: 0.005787, val mae: 0.052870, val r2: 0.855396\n",
      "epoch: 85, train loss: 0.003043, val loss: 0.005752, val mae: 0.052701, val r2: 0.856190\n",
      "epoch: 86, train loss: 0.003023, val loss: 0.005774, val mae: 0.052827, val r2: 0.855728\n",
      "epoch: 87, train loss: 0.003010, val loss: 0.005773, val mae: 0.052806, val r2: 0.855634\n",
      "epoch: 88, train loss: 0.003001, val loss: 0.005747, val mae: 0.052734, val r2: 0.856325\n",
      "epoch: 89, train loss: 0.002990, val loss: 0.005772, val mae: 0.052837, val r2: 0.855642\n",
      "epoch: 90, train loss: 0.002972, val loss: 0.005752, val mae: 0.052722, val r2: 0.856205\n",
      "epoch: 91, train loss: 0.002959, val loss: 0.005762, val mae: 0.052740, val r2: 0.855758\n",
      "epoch: 92, train loss: 0.002952, val loss: 0.005778, val mae: 0.052880, val r2: 0.855522\n",
      "epoch: 93, train loss: 0.002950, val loss: 0.005724, val mae: 0.052510, val r2: 0.856719\n",
      "epoch: 94, train loss: 0.002926, val loss: 0.005714, val mae: 0.052523, val r2: 0.856897\n",
      "epoch: 95, train loss: 0.002926, val loss: 0.005737, val mae: 0.052657, val r2: 0.856314\n",
      "epoch: 96, train loss: 0.002922, val loss: 0.005715, val mae: 0.052469, val r2: 0.856752\n",
      "epoch: 97, train loss: 0.002911, val loss: 0.005722, val mae: 0.052520, val r2: 0.856683\n",
      "epoch: 98, train loss: 0.002905, val loss: 0.005703, val mae: 0.052439, val r2: 0.856976\n",
      "epoch: 99, train loss: 0.002899, val loss: 0.005766, val mae: 0.052855, val r2: 0.855582\n",
      "epoch: 100, train loss: 0.002889, val loss: 0.005756, val mae: 0.052701, val r2: 0.855692\n",
      "epoch: 101, train loss: 0.002887, val loss: 0.005756, val mae: 0.052807, val r2: 0.855787\n",
      "epoch: 102, train loss: 0.002887, val loss: 0.005773, val mae: 0.052808, val r2: 0.855306\n",
      "epoch: 103, train loss: 0.002884, val loss: 0.005847, val mae: 0.053284, val r2: 0.853452\n",
      "epoch: 104, train loss: 0.002885, val loss: 0.005862, val mae: 0.053314, val r2: 0.853040\n",
      "epoch: 105, train loss: 0.002889, val loss: 0.005984, val mae: 0.053965, val r2: 0.850012\n",
      "epoch: 106, train loss: 0.002893, val loss: 0.006059, val mae: 0.054400, val r2: 0.848332\n",
      "epoch: 107, train loss: 0.002894, val loss: 0.006258, val mae: 0.055391, val r2: 0.843572\n",
      "epoch: 108, train loss: 0.002913, val loss: 0.006336, val mae: 0.055818, val r2: 0.841499\n",
      "epoch: 109, train loss: 0.002941, val loss: 0.006618, val mae: 0.057063, val r2: 0.834779\n",
      "epoch: 110, train loss: 0.002978, val loss: 0.006583, val mae: 0.056899, val r2: 0.835315\n",
      "epoch: 111, train loss: 0.003042, val loss: 0.006207, val mae: 0.055123, val r2: 0.844425\n",
      "epoch: 112, train loss: 0.003136, val loss: 0.005849, val mae: 0.053267, val r2: 0.853049\n",
      "epoch: 113, train loss: 0.003148, val loss: 0.005694, val mae: 0.052549, val r2: 0.856616\n",
      "epoch: 114, train loss: 0.003169, val loss: 0.006067, val mae: 0.054663, val r2: 0.846988\n",
      "epoch: 115, train loss: 0.003289, val loss: 0.007119, val mae: 0.059717, val r2: 0.820078\n",
      "epoch: 116, train loss: 0.003358, val loss: 0.007836, val mae: 0.063210, val r2: 0.801582\n",
      "epoch: 117, train loss: 0.003386, val loss: 0.008145, val mae: 0.064714, val r2: 0.793392\n",
      "epoch: 118, train loss: 0.003359, val loss: 0.008110, val mae: 0.064675, val r2: 0.793818\n",
      "epoch: 119, train loss: 0.003319, val loss: 0.008024, val mae: 0.064368, val r2: 0.795709\n",
      "epoch: 120, train loss: 0.003256, val loss: 0.007717, val mae: 0.063043, val r2: 0.803375\n",
      "epoch: 121, train loss: 0.003223, val loss: 0.007697, val mae: 0.063021, val r2: 0.803361\n",
      "epoch: 122, train loss: 0.003164, val loss: 0.007499, val mae: 0.062102, val r2: 0.808558\n",
      "epoch: 123, train loss: 0.003133, val loss: 0.007312, val mae: 0.061231, val r2: 0.813361\n",
      "epoch: 124, train loss: 0.003101, val loss: 0.007198, val mae: 0.060733, val r2: 0.816190\n",
      "epoch: 125, train loss: 0.003057, val loss: 0.007120, val mae: 0.060332, val r2: 0.818092\n",
      "epoch: 126, train loss: 0.003038, val loss: 0.007019, val mae: 0.059843, val r2: 0.820766\n",
      "epoch: 127, train loss: 0.002993, val loss: 0.006798, val mae: 0.058749, val r2: 0.826650\n",
      "epoch: 128, train loss: 0.002958, val loss: 0.006769, val mae: 0.058679, val r2: 0.827484\n",
      "epoch: 129, train loss: 0.002916, val loss: 0.006530, val mae: 0.057431, val r2: 0.833738\n",
      "epoch: 130, train loss: 0.002882, val loss: 0.006508, val mae: 0.057305, val r2: 0.834575\n",
      "epoch: 131, train loss: 0.002843, val loss: 0.006328, val mae: 0.056388, val r2: 0.839311\n",
      "epoch: 132, train loss: 0.002814, val loss: 0.006169, val mae: 0.055662, val r2: 0.843589\n",
      "epoch: 133, train loss: 0.002786, val loss: 0.006172, val mae: 0.055695, val r2: 0.843775\n",
      "epoch: 134, train loss: 0.002753, val loss: 0.006078, val mae: 0.055148, val r2: 0.846320\n",
      "epoch: 135, train loss: 0.002730, val loss: 0.006008, val mae: 0.054796, val r2: 0.848228\n",
      "epoch: 136, train loss: 0.002707, val loss: 0.005884, val mae: 0.054170, val r2: 0.851491\n",
      "epoch: 137, train loss: 0.002704, val loss: 0.005936, val mae: 0.054444, val r2: 0.850343\n",
      "epoch: 138, train loss: 0.002678, val loss: 0.005872, val mae: 0.054059, val r2: 0.852180\n",
      "epoch: 139, train loss: 0.002657, val loss: 0.005695, val mae: 0.053149, val r2: 0.856838\n",
      "epoch: 140, train loss: 0.002640, val loss: 0.005702, val mae: 0.053108, val r2: 0.856674\n",
      "epoch: 141, train loss: 0.002632, val loss: 0.005619, val mae: 0.052683, val r2: 0.858902\n",
      "epoch: 142, train loss: 0.002619, val loss: 0.005695, val mae: 0.053134, val r2: 0.856921\n",
      "epoch: 143, train loss: 0.002609, val loss: 0.005618, val mae: 0.052747, val r2: 0.858889\n",
      "epoch: 144, train loss: 0.002593, val loss: 0.005536, val mae: 0.052272, val r2: 0.860969\n",
      "epoch: 145, train loss: 0.002585, val loss: 0.005568, val mae: 0.052401, val r2: 0.860289\n",
      "epoch: 146, train loss: 0.002572, val loss: 0.005510, val mae: 0.052089, val r2: 0.861689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 147, train loss: 0.002563, val loss: 0.005547, val mae: 0.052327, val r2: 0.860789\n",
      "epoch: 148, train loss: 0.002551, val loss: 0.005563, val mae: 0.052421, val r2: 0.860374\n",
      "epoch: 149, train loss: 0.002540, val loss: 0.005523, val mae: 0.052238, val r2: 0.861382\n",
      "epoch: 150, train loss: 0.002531, val loss: 0.005598, val mae: 0.052572, val r2: 0.859605\n",
      "epoch: 151, train loss: 0.002519, val loss: 0.005621, val mae: 0.052709, val r2: 0.859124\n",
      "epoch: 152, train loss: 0.002505, val loss: 0.005628, val mae: 0.052679, val r2: 0.858950\n",
      "epoch: 153, train loss: 0.002498, val loss: 0.005579, val mae: 0.052523, val r2: 0.860114\n",
      "epoch: 154, train loss: 0.002490, val loss: 0.005498, val mae: 0.052063, val r2: 0.862167\n",
      "epoch: 155, train loss: 0.002482, val loss: 0.005595, val mae: 0.052603, val r2: 0.859706\n",
      "epoch: 156, train loss: 0.002472, val loss: 0.005536, val mae: 0.052328, val r2: 0.861137\n",
      "epoch: 157, train loss: 0.002464, val loss: 0.005709, val mae: 0.053185, val r2: 0.856997\n",
      "epoch: 158, train loss: 0.002453, val loss: 0.005615, val mae: 0.052761, val r2: 0.859165\n",
      "epoch: 159, train loss: 0.002449, val loss: 0.005573, val mae: 0.052521, val r2: 0.860325\n",
      "epoch: 160, train loss: 0.002443, val loss: 0.005599, val mae: 0.052688, val r2: 0.859492\n",
      "epoch: 161, train loss: 0.002435, val loss: 0.005612, val mae: 0.052745, val r2: 0.859219\n",
      "epoch: 162, train loss: 0.002423, val loss: 0.005600, val mae: 0.052682, val r2: 0.859559\n",
      "epoch: 163, train loss: 0.002420, val loss: 0.005593, val mae: 0.052659, val r2: 0.859691\n",
      "epoch: 164, train loss: 0.002419, val loss: 0.005650, val mae: 0.052983, val r2: 0.858215\n",
      "epoch: 165, train loss: 0.002408, val loss: 0.005613, val mae: 0.052756, val r2: 0.858909\n",
      "epoch: 166, train loss: 0.002403, val loss: 0.005644, val mae: 0.052892, val r2: 0.858233\n",
      "epoch: 167, train loss: 0.002399, val loss: 0.005630, val mae: 0.052861, val r2: 0.858399\n",
      "epoch: 168, train loss: 0.002393, val loss: 0.005503, val mae: 0.052162, val r2: 0.861664\n",
      "epoch: 169, train loss: 0.002389, val loss: 0.005478, val mae: 0.051986, val r2: 0.862157\n",
      "epoch: 170, train loss: 0.002388, val loss: 0.005407, val mae: 0.051625, val r2: 0.863811\n",
      "epoch: 171, train loss: 0.002386, val loss: 0.005576, val mae: 0.052491, val r2: 0.859593\n",
      "epoch: 172, train loss: 0.002378, val loss: 0.005486, val mae: 0.052080, val r2: 0.861576\n",
      "epoch: 173, train loss: 0.002377, val loss: 0.005438, val mae: 0.051766, val r2: 0.862873\n",
      "epoch: 174, train loss: 0.002383, val loss: 0.005349, val mae: 0.051229, val r2: 0.865145\n",
      "epoch: 175, train loss: 0.002385, val loss: 0.005342, val mae: 0.051169, val r2: 0.865138\n",
      "epoch: 176, train loss: 0.002400, val loss: 0.005276, val mae: 0.050621, val r2: 0.866887\n",
      "epoch: 177, train loss: 0.002431, val loss: 0.005329, val mae: 0.050788, val r2: 0.865487\n",
      "epoch: 178, train loss: 0.002476, val loss: 0.005423, val mae: 0.051222, val r2: 0.862806\n",
      "epoch: 179, train loss: 0.002597, val loss: 0.005704, val mae: 0.052718, val r2: 0.855140\n",
      "epoch: 180, train loss: 0.002797, val loss: 0.006988, val mae: 0.059184, val r2: 0.821676\n",
      "epoch: 181, train loss: 0.002937, val loss: 0.009964, val mae: 0.071581, val r2: 0.746384\n",
      "epoch: 182, train loss: 0.002743, val loss: 0.008320, val mae: 0.065228, val r2: 0.787711\n",
      "epoch: 183, train loss: 0.002671, val loss: 0.006391, val mae: 0.056772, val r2: 0.837099\n",
      "epoch: 184, train loss: 0.002596, val loss: 0.006263, val mae: 0.056166, val r2: 0.840382\n",
      "epoch: 185, train loss: 0.002514, val loss: 0.006524, val mae: 0.057431, val r2: 0.833632\n",
      "epoch: 186, train loss: 0.002476, val loss: 0.006612, val mae: 0.057851, val r2: 0.831785\n",
      "epoch: 187, train loss: 0.002452, val loss: 0.006488, val mae: 0.057333, val r2: 0.835150\n",
      "epoch: 188, train loss: 0.002437, val loss: 0.006244, val mae: 0.056121, val r2: 0.841462\n",
      "epoch: 189, train loss: 0.002409, val loss: 0.006022, val mae: 0.054983, val r2: 0.847404\n",
      "epoch: 190, train loss: 0.002398, val loss: 0.005986, val mae: 0.054758, val r2: 0.848413\n",
      "epoch: 191, train loss: 0.002370, val loss: 0.005935, val mae: 0.054546, val r2: 0.849830\n",
      "epoch: 192, train loss: 0.002356, val loss: 0.005886, val mae: 0.054240, val r2: 0.851253\n",
      "epoch: 193, train loss: 0.002344, val loss: 0.005675, val mae: 0.053169, val r2: 0.856817\n",
      "epoch: 194, train loss: 0.002334, val loss: 0.005609, val mae: 0.052760, val r2: 0.858578\n",
      "epoch: 195, train loss: 0.002319, val loss: 0.005549, val mae: 0.052425, val r2: 0.860218\n",
      "epoch: 196, train loss: 0.002314, val loss: 0.005522, val mae: 0.052255, val r2: 0.860992\n",
      "epoch: 197, train loss: 0.002303, val loss: 0.005437, val mae: 0.051781, val r2: 0.863222\n",
      "epoch: 198, train loss: 0.002296, val loss: 0.005456, val mae: 0.051839, val r2: 0.862752\n",
      "epoch: 199, train loss: 0.002291, val loss: 0.005400, val mae: 0.051547, val r2: 0.864154\n",
      "epoch: 200, train loss: 0.002280, val loss: 0.005364, val mae: 0.051281, val r2: 0.865175\n",
      "epoch: 201, train loss: 0.002276, val loss: 0.005352, val mae: 0.051292, val r2: 0.865461\n",
      "epoch: 202, train loss: 0.002265, val loss: 0.005367, val mae: 0.051312, val r2: 0.865290\n",
      "epoch: 203, train loss: 0.002262, val loss: 0.005333, val mae: 0.051108, val r2: 0.866069\n",
      "epoch: 204, train loss: 0.002256, val loss: 0.005341, val mae: 0.051085, val r2: 0.865960\n",
      "epoch: 205, train loss: 0.002245, val loss: 0.005276, val mae: 0.050736, val r2: 0.867604\n",
      "epoch: 206, train loss: 0.002237, val loss: 0.005271, val mae: 0.050705, val r2: 0.867706\n",
      "epoch: 207, train loss: 0.002232, val loss: 0.005272, val mae: 0.050726, val r2: 0.867687\n",
      "epoch: 208, train loss: 0.002228, val loss: 0.005237, val mae: 0.050525, val r2: 0.868650\n",
      "epoch: 209, train loss: 0.002223, val loss: 0.005204, val mae: 0.050276, val r2: 0.869428\n",
      "epoch: 210, train loss: 0.002213, val loss: 0.005211, val mae: 0.050354, val r2: 0.869249\n",
      "epoch: 211, train loss: 0.002205, val loss: 0.005176, val mae: 0.050142, val r2: 0.870088\n",
      "epoch: 212, train loss: 0.002199, val loss: 0.005125, val mae: 0.049771, val r2: 0.871336\n",
      "epoch: 213, train loss: 0.002193, val loss: 0.005138, val mae: 0.049877, val r2: 0.870985\n",
      "epoch: 214, train loss: 0.002189, val loss: 0.005181, val mae: 0.050107, val r2: 0.869969\n",
      "epoch: 215, train loss: 0.002181, val loss: 0.005228, val mae: 0.050474, val r2: 0.868644\n",
      "epoch: 216, train loss: 0.002181, val loss: 0.005157, val mae: 0.050004, val r2: 0.870431\n",
      "epoch: 217, train loss: 0.002171, val loss: 0.005174, val mae: 0.049992, val r2: 0.869861\n",
      "epoch: 218, train loss: 0.002171, val loss: 0.005211, val mae: 0.050203, val r2: 0.868996\n",
      "epoch: 219, train loss: 0.002167, val loss: 0.005197, val mae: 0.050121, val r2: 0.869275\n",
      "epoch: 220, train loss: 0.002170, val loss: 0.005303, val mae: 0.050705, val r2: 0.866568\n",
      "epoch: 221, train loss: 0.002166, val loss: 0.005308, val mae: 0.050761, val r2: 0.866277\n",
      "epoch: 222, train loss: 0.002174, val loss: 0.005332, val mae: 0.050969, val r2: 0.865673\n",
      "epoch: 223, train loss: 0.002179, val loss: 0.005472, val mae: 0.051687, val r2: 0.862062\n",
      "epoch: 224, train loss: 0.002186, val loss: 0.005523, val mae: 0.051995, val r2: 0.860814\n",
      "epoch: 225, train loss: 0.002205, val loss: 0.005797, val mae: 0.053504, val r2: 0.853916\n",
      "epoch: 226, train loss: 0.002224, val loss: 0.006222, val mae: 0.055775, val r2: 0.843229\n",
      "epoch: 227, train loss: 0.002233, val loss: 0.006999, val mae: 0.059465, val r2: 0.823622\n",
      "epoch: 228, train loss: 0.002277, val loss: 0.007464, val mae: 0.061519, val r2: 0.811562\n",
      "epoch: 229, train loss: 0.002352, val loss: 0.006684, val mae: 0.058065, val r2: 0.830931\n",
      "epoch: 230, train loss: 0.002427, val loss: 0.005603, val mae: 0.052891, val r2: 0.858342\n",
      "epoch: 231, train loss: 0.002388, val loss: 0.005417, val mae: 0.051935, val r2: 0.862884\n",
      "epoch: 232, train loss: 0.002347, val loss: 0.005730, val mae: 0.053574, val r2: 0.854772\n",
      "epoch: 233, train loss: 0.002350, val loss: 0.006152, val mae: 0.055735, val r2: 0.843875\n",
      "epoch: 234, train loss: 0.002271, val loss: 0.006318, val mae: 0.056569, val r2: 0.839507\n",
      "epoch: 235, train loss: 0.002236, val loss: 0.005926, val mae: 0.054650, val r2: 0.849712\n",
      "epoch: 236, train loss: 0.002242, val loss: 0.005593, val mae: 0.052813, val r2: 0.858496\n",
      "epoch: 237, train loss: 0.002220, val loss: 0.005430, val mae: 0.051909, val r2: 0.862829\n",
      "epoch: 238, train loss: 0.002189, val loss: 0.005389, val mae: 0.051641, val r2: 0.864051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 239, train loss: 0.002157, val loss: 0.005418, val mae: 0.051776, val r2: 0.863338\n",
      "epoch: 240, train loss: 0.002149, val loss: 0.005376, val mae: 0.051552, val r2: 0.864610\n",
      "epoch: 241, train loss: 0.002136, val loss: 0.005246, val mae: 0.050845, val r2: 0.868063\n",
      "epoch: 242, train loss: 0.002130, val loss: 0.005196, val mae: 0.050482, val r2: 0.869352\n",
      "epoch: 243, train loss: 0.002117, val loss: 0.005161, val mae: 0.050211, val r2: 0.870366\n",
      "epoch: 244, train loss: 0.002110, val loss: 0.005126, val mae: 0.049924, val r2: 0.871359\n",
      "epoch: 245, train loss: 0.002099, val loss: 0.005094, val mae: 0.049711, val r2: 0.872156\n",
      "epoch: 246, train loss: 0.002095, val loss: 0.005101, val mae: 0.049723, val r2: 0.872081\n",
      "epoch: 247, train loss: 0.002090, val loss: 0.005111, val mae: 0.049718, val r2: 0.871782\n",
      "epoch: 248, train loss: 0.002083, val loss: 0.005095, val mae: 0.049629, val r2: 0.872216\n",
      "epoch: 249, train loss: 0.002078, val loss: 0.005074, val mae: 0.049531, val r2: 0.872708\n",
      "epoch: 250, train loss: 0.002075, val loss: 0.005070, val mae: 0.049501, val r2: 0.872789\n",
      "epoch: 251, train loss: 0.002073, val loss: 0.005086, val mae: 0.049503, val r2: 0.872444\n",
      "epoch: 252, train loss: 0.002067, val loss: 0.005067, val mae: 0.049481, val r2: 0.872850\n",
      "epoch: 253, train loss: 0.002058, val loss: 0.005101, val mae: 0.049595, val r2: 0.872002\n",
      "epoch: 254, train loss: 0.002055, val loss: 0.005078, val mae: 0.049574, val r2: 0.872659\n",
      "epoch: 255, train loss: 0.002052, val loss: 0.005085, val mae: 0.049669, val r2: 0.872440\n",
      "epoch: 256, train loss: 0.002050, val loss: 0.005099, val mae: 0.049794, val r2: 0.872096\n",
      "epoch: 257, train loss: 0.002040, val loss: 0.005167, val mae: 0.050208, val r2: 0.870434\n",
      "epoch: 258, train loss: 0.002041, val loss: 0.005170, val mae: 0.050290, val r2: 0.870309\n",
      "epoch: 259, train loss: 0.002031, val loss: 0.005160, val mae: 0.050235, val r2: 0.870469\n",
      "epoch: 260, train loss: 0.002030, val loss: 0.005202, val mae: 0.050511, val r2: 0.869435\n",
      "epoch: 261, train loss: 0.002025, val loss: 0.005194, val mae: 0.050387, val r2: 0.869624\n",
      "epoch: 262, train loss: 0.002019, val loss: 0.005159, val mae: 0.050288, val r2: 0.870480\n",
      "epoch: 263, train loss: 0.002013, val loss: 0.005177, val mae: 0.050307, val r2: 0.870030\n",
      "epoch: 264, train loss: 0.002012, val loss: 0.005189, val mae: 0.050458, val r2: 0.869635\n",
      "epoch: 265, train loss: 0.002012, val loss: 0.005150, val mae: 0.050247, val r2: 0.870592\n",
      "epoch: 266, train loss: 0.002008, val loss: 0.005179, val mae: 0.050440, val r2: 0.869937\n",
      "epoch: 267, train loss: 0.002007, val loss: 0.005146, val mae: 0.050241, val r2: 0.870783\n",
      "epoch: 268, train loss: 0.002002, val loss: 0.005189, val mae: 0.050368, val r2: 0.869715\n",
      "epoch: 269, train loss: 0.002005, val loss: 0.005176, val mae: 0.050285, val r2: 0.870033\n",
      "epoch: 270, train loss: 0.002004, val loss: 0.005265, val mae: 0.050629, val r2: 0.867844\n",
      "epoch: 271, train loss: 0.002009, val loss: 0.005261, val mae: 0.050604, val r2: 0.867944\n",
      "epoch: 272, train loss: 0.002022, val loss: 0.005398, val mae: 0.051008, val r2: 0.864360\n",
      "epoch: 273, train loss: 0.002046, val loss: 0.005420, val mae: 0.051076, val r2: 0.863640\n",
      "epoch: 274, train loss: 0.002092, val loss: 0.005348, val mae: 0.050964, val r2: 0.865189\n",
      "epoch: 275, train loss: 0.002115, val loss: 0.005531, val mae: 0.052235, val r2: 0.859927\n",
      "epoch: 276, train loss: 0.002140, val loss: 0.006164, val mae: 0.055586, val r2: 0.843078\n",
      "epoch: 277, train loss: 0.002143, val loss: 0.006565, val mae: 0.057529, val r2: 0.832807\n",
      "epoch: 278, train loss: 0.002159, val loss: 0.006444, val mae: 0.056898, val r2: 0.836110\n",
      "epoch: 279, train loss: 0.002213, val loss: 0.005769, val mae: 0.053691, val r2: 0.853935\n",
      "epoch: 280, train loss: 0.002233, val loss: 0.005385, val mae: 0.051626, val r2: 0.864295\n",
      "epoch: 281, train loss: 0.002155, val loss: 0.005262, val mae: 0.050855, val r2: 0.867517\n",
      "epoch: 282, train loss: 0.002080, val loss: 0.005300, val mae: 0.050931, val r2: 0.866389\n",
      "epoch: 283, train loss: 0.002047, val loss: 0.005368, val mae: 0.051439, val r2: 0.864480\n",
      "epoch: 284, train loss: 0.002037, val loss: 0.005385, val mae: 0.051590, val r2: 0.864132\n",
      "epoch: 285, train loss: 0.002035, val loss: 0.005326, val mae: 0.051290, val r2: 0.865760\n",
      "epoch: 286, train loss: 0.002029, val loss: 0.005208, val mae: 0.050658, val r2: 0.868939\n",
      "epoch: 287, train loss: 0.002019, val loss: 0.005119, val mae: 0.049983, val r2: 0.871355\n",
      "epoch: 288, train loss: 0.002007, val loss: 0.005126, val mae: 0.049824, val r2: 0.871243\n",
      "epoch: 289, train loss: 0.002009, val loss: 0.005078, val mae: 0.049509, val r2: 0.872538\n",
      "epoch: 290, train loss: 0.002012, val loss: 0.005104, val mae: 0.049556, val r2: 0.871847\n",
      "epoch: 291, train loss: 0.002019, val loss: 0.005123, val mae: 0.049630, val r2: 0.871261\n",
      "epoch: 292, train loss: 0.002024, val loss: 0.005139, val mae: 0.049784, val r2: 0.870884\n",
      "epoch: 293, train loss: 0.002023, val loss: 0.005162, val mae: 0.050113, val r2: 0.870351\n",
      "epoch: 294, train loss: 0.002020, val loss: 0.005170, val mae: 0.050211, val r2: 0.870135\n",
      "epoch: 295, train loss: 0.002031, val loss: 0.005157, val mae: 0.050217, val r2: 0.870467\n",
      "epoch: 296, train loss: 0.002031, val loss: 0.005165, val mae: 0.050147, val r2: 0.870226\n",
      "epoch: 297, train loss: 0.002016, val loss: 0.005266, val mae: 0.050368, val r2: 0.867636\n",
      "epoch: 298, train loss: 0.002003, val loss: 0.005322, val mae: 0.050515, val r2: 0.866049\n",
      "epoch: 299, train loss: 0.002008, val loss: 0.005370, val mae: 0.050763, val r2: 0.864657\n",
      "epoch: 300, train loss: 0.002002, val loss: 0.005368, val mae: 0.050980, val r2: 0.864833\n",
      "epoch: 301, train loss: 0.001982, val loss: 0.005392, val mae: 0.051302, val r2: 0.864212\n",
      "epoch: 302, train loss: 0.001978, val loss: 0.005448, val mae: 0.051661, val r2: 0.862527\n",
      "epoch: 303, train loss: 0.001979, val loss: 0.005493, val mae: 0.051896, val r2: 0.861175\n",
      "epoch: 304, train loss: 0.001988, val loss: 0.005502, val mae: 0.051966, val r2: 0.860850\n",
      "epoch: 305, train loss: 0.001986, val loss: 0.005497, val mae: 0.051826, val r2: 0.860891\n",
      "epoch: 306, train loss: 0.001982, val loss: 0.005544, val mae: 0.051986, val r2: 0.859583\n",
      "epoch: 307, train loss: 0.001978, val loss: 0.005661, val mae: 0.052636, val r2: 0.856426\n",
      "epoch: 308, train loss: 0.001986, val loss: 0.005795, val mae: 0.053395, val r2: 0.852805\n",
      "epoch: 309, train loss: 0.001992, val loss: 0.006077, val mae: 0.054740, val r2: 0.845438\n",
      "epoch: 310, train loss: 0.001997, val loss: 0.006117, val mae: 0.055114, val r2: 0.844416\n",
      "epoch: 311, train loss: 0.002007, val loss: 0.006199, val mae: 0.055633, val r2: 0.842244\n",
      "epoch: 312, train loss: 0.002013, val loss: 0.006077, val mae: 0.055026, val r2: 0.845219\n",
      "epoch: 313, train loss: 0.002016, val loss: 0.005902, val mae: 0.054283, val r2: 0.849692\n",
      "epoch: 314, train loss: 0.002006, val loss: 0.005723, val mae: 0.053492, val r2: 0.854406\n",
      "epoch: 315, train loss: 0.002001, val loss: 0.005566, val mae: 0.052509, val r2: 0.858657\n",
      "epoch: 316, train loss: 0.001992, val loss: 0.005382, val mae: 0.051423, val r2: 0.863852\n",
      "epoch: 317, train loss: 0.001990, val loss: 0.005279, val mae: 0.050889, val r2: 0.866666\n",
      "epoch: 318, train loss: 0.002000, val loss: 0.005257, val mae: 0.050470, val r2: 0.867531\n",
      "epoch: 319, train loss: 0.002008, val loss: 0.005297, val mae: 0.050559, val r2: 0.866681\n",
      "epoch: 320, train loss: 0.002005, val loss: 0.005430, val mae: 0.051345, val r2: 0.863307\n",
      "epoch: 321, train loss: 0.001997, val loss: 0.005440, val mae: 0.051370, val r2: 0.863142\n",
      "epoch: 322, train loss: 0.001989, val loss: 0.005396, val mae: 0.051119, val r2: 0.864128\n",
      "epoch: 323, train loss: 0.001979, val loss: 0.005471, val mae: 0.051544, val r2: 0.862070\n",
      "epoch: 324, train loss: 0.001971, val loss: 0.005514, val mae: 0.051685, val r2: 0.860997\n",
      "epoch: 325, train loss: 0.001956, val loss: 0.005724, val mae: 0.052853, val r2: 0.855635\n",
      "epoch: 326, train loss: 0.001949, val loss: 0.005870, val mae: 0.053562, val r2: 0.851767\n",
      "epoch: 327, train loss: 0.001929, val loss: 0.006178, val mae: 0.055052, val r2: 0.843825\n",
      "epoch: 328, train loss: 0.001916, val loss: 0.006166, val mae: 0.054976, val r2: 0.844002\n",
      "epoch: 329, train loss: 0.001916, val loss: 0.006384, val mae: 0.055985, val r2: 0.838030\n",
      "epoch: 330, train loss: 0.001912, val loss: 0.006554, val mae: 0.056825, val r2: 0.833301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 331, train loss: 0.001917, val loss: 0.006591, val mae: 0.056979, val r2: 0.831900\n",
      "epoch: 332, train loss: 0.001920, val loss: 0.006805, val mae: 0.058077, val r2: 0.826167\n",
      "epoch: 333, train loss: 0.001924, val loss: 0.006814, val mae: 0.058283, val r2: 0.825549\n",
      "epoch: 334, train loss: 0.001932, val loss: 0.006701, val mae: 0.057798, val r2: 0.828537\n",
      "epoch: 335, train loss: 0.001926, val loss: 0.006478, val mae: 0.056881, val r2: 0.834534\n",
      "epoch: 336, train loss: 0.001927, val loss: 0.006073, val mae: 0.054870, val r2: 0.845226\n",
      "epoch: 337, train loss: 0.001912, val loss: 0.005834, val mae: 0.053737, val r2: 0.851890\n",
      "epoch: 338, train loss: 0.001898, val loss: 0.005570, val mae: 0.052188, val r2: 0.858968\n",
      "epoch: 339, train loss: 0.001889, val loss: 0.005455, val mae: 0.051508, val r2: 0.862068\n",
      "epoch: 340, train loss: 0.001893, val loss: 0.005420, val mae: 0.051222, val r2: 0.863273\n",
      "epoch: 341, train loss: 0.001898, val loss: 0.005481, val mae: 0.051573, val r2: 0.861859\n",
      "epoch: 342, train loss: 0.001893, val loss: 0.005573, val mae: 0.051979, val r2: 0.859793\n",
      "epoch: 343, train loss: 0.001879, val loss: 0.005658, val mae: 0.052583, val r2: 0.857591\n",
      "epoch: 344, train loss: 0.001871, val loss: 0.005733, val mae: 0.052860, val r2: 0.855588\n",
      "epoch: 345, train loss: 0.001874, val loss: 0.005839, val mae: 0.053418, val r2: 0.852650\n",
      "epoch: 346, train loss: 0.001875, val loss: 0.006027, val mae: 0.054300, val r2: 0.847820\n",
      "epoch: 347, train loss: 0.001873, val loss: 0.006229, val mae: 0.055222, val r2: 0.842681\n",
      "epoch: 348, train loss: 0.001868, val loss: 0.006549, val mae: 0.056721, val r2: 0.834405\n",
      "epoch: 349, train loss: 0.001861, val loss: 0.006904, val mae: 0.058402, val r2: 0.825093\n",
      "epoch: 350, train loss: 0.001859, val loss: 0.007004, val mae: 0.059030, val r2: 0.821861\n",
      "epoch: 351, train loss: 0.001854, val loss: 0.007389, val mae: 0.060905, val r2: 0.811689\n",
      "epoch: 352, train loss: 0.001846, val loss: 0.007499, val mae: 0.061596, val r2: 0.808567\n",
      "epoch: 353, train loss: 0.001842, val loss: 0.007379, val mae: 0.061108, val r2: 0.811794\n",
      "epoch: 354, train loss: 0.001854, val loss: 0.006813, val mae: 0.058598, val r2: 0.826451\n",
      "epoch: 355, train loss: 0.001844, val loss: 0.006432, val mae: 0.056770, val r2: 0.836524\n",
      "epoch: 356, train loss: 0.001839, val loss: 0.006238, val mae: 0.055732, val r2: 0.841756\n",
      "epoch: 357, train loss: 0.001829, val loss: 0.005985, val mae: 0.054378, val r2: 0.848386\n",
      "epoch: 358, train loss: 0.001821, val loss: 0.005902, val mae: 0.054028, val r2: 0.850832\n",
      "epoch: 359, train loss: 0.001818, val loss: 0.005755, val mae: 0.053195, val r2: 0.854853\n",
      "epoch: 360, train loss: 0.001814, val loss: 0.005681, val mae: 0.052855, val r2: 0.856996\n",
      "epoch: 361, train loss: 0.001806, val loss: 0.005754, val mae: 0.053339, val r2: 0.855425\n",
      "epoch: 362, train loss: 0.001803, val loss: 0.005688, val mae: 0.052966, val r2: 0.857016\n",
      "epoch: 363, train loss: 0.001797, val loss: 0.005643, val mae: 0.052624, val r2: 0.858221\n",
      "epoch: 364, train loss: 0.001792, val loss: 0.005742, val mae: 0.053105, val r2: 0.855715\n",
      "epoch: 365, train loss: 0.001793, val loss: 0.005726, val mae: 0.053051, val r2: 0.856085\n",
      "epoch: 366, train loss: 0.001795, val loss: 0.005795, val mae: 0.053495, val r2: 0.854067\n",
      "epoch: 367, train loss: 0.001801, val loss: 0.005835, val mae: 0.053713, val r2: 0.852891\n",
      "epoch: 368, train loss: 0.001808, val loss: 0.006070, val mae: 0.054992, val r2: 0.846800\n",
      "epoch: 369, train loss: 0.001812, val loss: 0.006348, val mae: 0.056333, val r2: 0.839405\n",
      "epoch: 370, train loss: 0.001801, val loss: 0.006755, val mae: 0.058403, val r2: 0.828896\n",
      "epoch: 371, train loss: 0.001790, val loss: 0.007028, val mae: 0.059662, val r2: 0.821711\n",
      "epoch: 372, train loss: 0.001787, val loss: 0.006885, val mae: 0.059007, val r2: 0.825364\n",
      "epoch: 373, train loss: 0.001785, val loss: 0.006631, val mae: 0.057814, val r2: 0.831852\n",
      "epoch: 374, train loss: 0.001785, val loss: 0.006169, val mae: 0.055582, val r2: 0.843918\n",
      "epoch: 375, train loss: 0.001777, val loss: 0.005975, val mae: 0.054560, val r2: 0.848865\n",
      "epoch: 376, train loss: 0.001772, val loss: 0.005830, val mae: 0.053747, val r2: 0.852647\n",
      "epoch: 377, train loss: 0.001765, val loss: 0.005795, val mae: 0.053529, val r2: 0.853613\n",
      "epoch: 378, train loss: 0.001769, val loss: 0.005783, val mae: 0.053507, val r2: 0.854076\n",
      "epoch: 379, train loss: 0.001764, val loss: 0.005737, val mae: 0.053271, val r2: 0.855395\n",
      "epoch: 380, train loss: 0.001759, val loss: 0.005570, val mae: 0.052395, val r2: 0.859911\n",
      "epoch: 381, train loss: 0.001757, val loss: 0.005427, val mae: 0.051470, val r2: 0.863625\n",
      "epoch: 382, train loss: 0.001749, val loss: 0.005385, val mae: 0.051149, val r2: 0.864698\n",
      "epoch: 383, train loss: 0.001741, val loss: 0.005315, val mae: 0.050743, val r2: 0.866528\n",
      "epoch: 384, train loss: 0.001729, val loss: 0.005303, val mae: 0.050595, val r2: 0.866805\n",
      "epoch: 385, train loss: 0.001731, val loss: 0.005291, val mae: 0.050614, val r2: 0.866967\n",
      "epoch: 386, train loss: 0.001733, val loss: 0.005309, val mae: 0.050772, val r2: 0.866528\n",
      "epoch: 387, train loss: 0.001735, val loss: 0.005445, val mae: 0.051594, val r2: 0.862998\n",
      "epoch: 388, train loss: 0.001732, val loss: 0.005652, val mae: 0.052762, val r2: 0.857685\n",
      "epoch: 389, train loss: 0.001718, val loss: 0.005768, val mae: 0.053417, val r2: 0.854662\n",
      "epoch: 390, train loss: 0.001711, val loss: 0.005837, val mae: 0.053861, val r2: 0.852877\n",
      "epoch: 391, train loss: 0.001707, val loss: 0.005781, val mae: 0.053648, val r2: 0.854239\n",
      "epoch: 392, train loss: 0.001705, val loss: 0.005756, val mae: 0.053625, val r2: 0.854902\n",
      "epoch: 393, train loss: 0.001700, val loss: 0.005576, val mae: 0.052625, val r2: 0.859627\n",
      "epoch: 394, train loss: 0.001696, val loss: 0.005536, val mae: 0.052354, val r2: 0.860518\n",
      "epoch: 395, train loss: 0.001691, val loss: 0.005443, val mae: 0.051775, val r2: 0.862841\n",
      "epoch: 396, train loss: 0.001689, val loss: 0.005376, val mae: 0.051282, val r2: 0.864626\n",
      "epoch: 397, train loss: 0.001692, val loss: 0.005361, val mae: 0.051223, val r2: 0.864918\n",
      "epoch: 398, train loss: 0.001695, val loss: 0.005347, val mae: 0.051168, val r2: 0.865330\n",
      "epoch: 399, train loss: 0.001697, val loss: 0.005328, val mae: 0.051156, val r2: 0.865904\n",
      "epoch: 400, train loss: 0.001697, val loss: 0.005403, val mae: 0.051660, val r2: 0.864140\n",
      "epoch: 401, train loss: 0.001695, val loss: 0.005409, val mae: 0.051594, val r2: 0.863978\n",
      "epoch: 402, train loss: 0.001693, val loss: 0.005409, val mae: 0.051641, val r2: 0.864033\n",
      "epoch: 403, train loss: 0.001694, val loss: 0.005332, val mae: 0.051055, val r2: 0.865917\n",
      "epoch: 404, train loss: 0.001689, val loss: 0.005288, val mae: 0.050735, val r2: 0.867068\n",
      "epoch: 405, train loss: 0.001681, val loss: 0.005300, val mae: 0.050522, val r2: 0.866690\n",
      "epoch: 406, train loss: 0.001675, val loss: 0.005307, val mae: 0.050486, val r2: 0.866496\n",
      "epoch: 407, train loss: 0.001691, val loss: 0.005314, val mae: 0.050534, val r2: 0.866331\n",
      "epoch: 408, train loss: 0.001702, val loss: 0.005358, val mae: 0.050890, val r2: 0.865025\n",
      "epoch: 409, train loss: 0.001710, val loss: 0.005643, val mae: 0.052579, val r2: 0.857483\n",
      "epoch: 410, train loss: 0.001702, val loss: 0.005923, val mae: 0.054109, val r2: 0.850120\n",
      "epoch: 411, train loss: 0.001705, val loss: 0.005732, val mae: 0.053184, val r2: 0.855225\n",
      "epoch: 412, train loss: 0.001716, val loss: 0.005393, val mae: 0.051309, val r2: 0.864224\n",
      "epoch: 413, train loss: 0.001704, val loss: 0.005226, val mae: 0.050273, val r2: 0.868688\n",
      "epoch: 414, train loss: 0.001683, val loss: 0.005218, val mae: 0.049988, val r2: 0.869010\n",
      "epoch: 415, train loss: 0.001676, val loss: 0.005263, val mae: 0.050064, val r2: 0.867753\n",
      "epoch: 416, train loss: 0.001685, val loss: 0.005245, val mae: 0.050100, val r2: 0.868140\n",
      "epoch: 417, train loss: 0.001678, val loss: 0.005236, val mae: 0.050284, val r2: 0.868494\n",
      "epoch: 418, train loss: 0.001662, val loss: 0.005282, val mae: 0.050628, val r2: 0.867302\n",
      "epoch: 419, train loss: 0.001661, val loss: 0.005247, val mae: 0.050592, val r2: 0.868146\n",
      "epoch: 420, train loss: 0.001659, val loss: 0.005248, val mae: 0.050439, val r2: 0.868059\n",
      "epoch: 421, train loss: 0.001657, val loss: 0.005264, val mae: 0.050423, val r2: 0.867689\n",
      "epoch: 422, train loss: 0.001653, val loss: 0.005260, val mae: 0.050415, val r2: 0.867625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 423, train loss: 0.001653, val loss: 0.005377, val mae: 0.051200, val r2: 0.864495\n",
      "epoch: 424, train loss: 0.001644, val loss: 0.005557, val mae: 0.052327, val r2: 0.859717\n",
      "epoch: 425, train loss: 0.001641, val loss: 0.005746, val mae: 0.053458, val r2: 0.854621\n",
      "epoch: 426, train loss: 0.001636, val loss: 0.005813, val mae: 0.053921, val r2: 0.852760\n",
      "epoch: 427, train loss: 0.001640, val loss: 0.005793, val mae: 0.053729, val r2: 0.853241\n",
      "epoch: 428, train loss: 0.001640, val loss: 0.005706, val mae: 0.053332, val r2: 0.855486\n",
      "epoch: 429, train loss: 0.001644, val loss: 0.005589, val mae: 0.052580, val r2: 0.858631\n",
      "epoch: 430, train loss: 0.001648, val loss: 0.005503, val mae: 0.051908, val r2: 0.860909\n",
      "epoch: 431, train loss: 0.001642, val loss: 0.005427, val mae: 0.051421, val r2: 0.862923\n",
      "epoch: 432, train loss: 0.001637, val loss: 0.005414, val mae: 0.051345, val r2: 0.863352\n",
      "epoch: 433, train loss: 0.001637, val loss: 0.005364, val mae: 0.050994, val r2: 0.864679\n",
      "epoch: 434, train loss: 0.001635, val loss: 0.005339, val mae: 0.050923, val r2: 0.865337\n",
      "epoch: 435, train loss: 0.001629, val loss: 0.005329, val mae: 0.050886, val r2: 0.865669\n",
      "epoch: 436, train loss: 0.001623, val loss: 0.005336, val mae: 0.050992, val r2: 0.865431\n",
      "epoch: 437, train loss: 0.001620, val loss: 0.005375, val mae: 0.051095, val r2: 0.864446\n",
      "epoch: 438, train loss: 0.001617, val loss: 0.005403, val mae: 0.051213, val r2: 0.863680\n",
      "epoch: 439, train loss: 0.001611, val loss: 0.005398, val mae: 0.051147, val r2: 0.863794\n",
      "epoch: 440, train loss: 0.001606, val loss: 0.005445, val mae: 0.051415, val r2: 0.862645\n",
      "epoch: 441, train loss: 0.001602, val loss: 0.005485, val mae: 0.051676, val r2: 0.861676\n",
      "epoch: 442, train loss: 0.001605, val loss: 0.005564, val mae: 0.052322, val r2: 0.859649\n",
      "epoch: 443, train loss: 0.001596, val loss: 0.005694, val mae: 0.053115, val r2: 0.856250\n",
      "epoch: 444, train loss: 0.001594, val loss: 0.005835, val mae: 0.053987, val r2: 0.852663\n",
      "epoch: 445, train loss: 0.001594, val loss: 0.005821, val mae: 0.053943, val r2: 0.852793\n",
      "epoch: 446, train loss: 0.001594, val loss: 0.005904, val mae: 0.054416, val r2: 0.850704\n",
      "epoch: 447, train loss: 0.001594, val loss: 0.005814, val mae: 0.053898, val r2: 0.852919\n",
      "epoch: 448, train loss: 0.001592, val loss: 0.005887, val mae: 0.054225, val r2: 0.850977\n",
      "epoch: 449, train loss: 0.001591, val loss: 0.005796, val mae: 0.053787, val r2: 0.853174\n",
      "epoch: 450, train loss: 0.001592, val loss: 0.005897, val mae: 0.054339, val r2: 0.850341\n",
      "epoch: 451, train loss: 0.001594, val loss: 0.005876, val mae: 0.054110, val r2: 0.850826\n",
      "epoch: 452, train loss: 0.001594, val loss: 0.006039, val mae: 0.054959, val r2: 0.846807\n",
      "epoch: 453, train loss: 0.001588, val loss: 0.006029, val mae: 0.054649, val r2: 0.846979\n",
      "epoch: 454, train loss: 0.001584, val loss: 0.005930, val mae: 0.053944, val r2: 0.849674\n",
      "epoch: 455, train loss: 0.001580, val loss: 0.005898, val mae: 0.053739, val r2: 0.850641\n",
      "epoch: 456, train loss: 0.001582, val loss: 0.005880, val mae: 0.053599, val r2: 0.851157\n",
      "epoch: 457, train loss: 0.001584, val loss: 0.005774, val mae: 0.053035, val r2: 0.853879\n",
      "epoch: 458, train loss: 0.001581, val loss: 0.005573, val mae: 0.051935, val r2: 0.859097\n",
      "epoch: 459, train loss: 0.001574, val loss: 0.005535, val mae: 0.051794, val r2: 0.860180\n",
      "epoch: 460, train loss: 0.001566, val loss: 0.005567, val mae: 0.051922, val r2: 0.859440\n",
      "epoch: 461, train loss: 0.001560, val loss: 0.005566, val mae: 0.051963, val r2: 0.859498\n",
      "epoch: 462, train loss: 0.001562, val loss: 0.005547, val mae: 0.051960, val r2: 0.860019\n",
      "epoch: 463, train loss: 0.001561, val loss: 0.005698, val mae: 0.052923, val r2: 0.856179\n",
      "epoch: 464, train loss: 0.001555, val loss: 0.005978, val mae: 0.054445, val r2: 0.849179\n",
      "epoch: 465, train loss: 0.001552, val loss: 0.006175, val mae: 0.055624, val r2: 0.843969\n",
      "epoch: 466, train loss: 0.001549, val loss: 0.006368, val mae: 0.056444, val r2: 0.838940\n",
      "epoch: 467, train loss: 0.001560, val loss: 0.006045, val mae: 0.054775, val r2: 0.847089\n",
      "epoch: 468, train loss: 0.001556, val loss: 0.005831, val mae: 0.053748, val r2: 0.852527\n",
      "epoch: 469, train loss: 0.001551, val loss: 0.005603, val mae: 0.052443, val r2: 0.858453\n",
      "epoch: 470, train loss: 0.001551, val loss: 0.005657, val mae: 0.052641, val r2: 0.857042\n",
      "epoch: 471, train loss: 0.001555, val loss: 0.005699, val mae: 0.052751, val r2: 0.855966\n",
      "epoch: 472, train loss: 0.001557, val loss: 0.005800, val mae: 0.053323, val r2: 0.853323\n",
      "epoch: 473, train loss: 0.001550, val loss: 0.005870, val mae: 0.053526, val r2: 0.851907\n",
      "epoch: 474, train loss: 0.001549, val loss: 0.005681, val mae: 0.052589, val r2: 0.856879\n",
      "epoch: 475, train loss: 0.001553, val loss: 0.005565, val mae: 0.051882, val r2: 0.859766\n",
      "epoch: 476, train loss: 0.001548, val loss: 0.005515, val mae: 0.051703, val r2: 0.861191\n",
      "epoch: 477, train loss: 0.001546, val loss: 0.005455, val mae: 0.051349, val r2: 0.862698\n",
      "epoch: 478, train loss: 0.001539, val loss: 0.005413, val mae: 0.051074, val r2: 0.863765\n",
      "epoch: 479, train loss: 0.001536, val loss: 0.005405, val mae: 0.051048, val r2: 0.863957\n",
      "epoch: 480, train loss: 0.001535, val loss: 0.005487, val mae: 0.051629, val r2: 0.861843\n",
      "epoch: 481, train loss: 0.001538, val loss: 0.005719, val mae: 0.053063, val r2: 0.855953\n",
      "epoch: 482, train loss: 0.001539, val loss: 0.006116, val mae: 0.055079, val r2: 0.845710\n",
      "epoch: 483, train loss: 0.001538, val loss: 0.006533, val mae: 0.057066, val r2: 0.834765\n",
      "epoch: 484, train loss: 0.001540, val loss: 0.006443, val mae: 0.056582, val r2: 0.836478\n",
      "epoch: 485, train loss: 0.001548, val loss: 0.006135, val mae: 0.054986, val r2: 0.844540\n",
      "epoch: 486, train loss: 0.001546, val loss: 0.005933, val mae: 0.053933, val r2: 0.849642\n",
      "epoch: 487, train loss: 0.001543, val loss: 0.005754, val mae: 0.053053, val r2: 0.854431\n",
      "epoch: 488, train loss: 0.001542, val loss: 0.005659, val mae: 0.052464, val r2: 0.856971\n",
      "epoch: 489, train loss: 0.001539, val loss: 0.005541, val mae: 0.051858, val r2: 0.860107\n",
      "epoch: 490, train loss: 0.001533, val loss: 0.005469, val mae: 0.051387, val r2: 0.862097\n",
      "epoch: 491, train loss: 0.001530, val loss: 0.005448, val mae: 0.051433, val r2: 0.862671\n",
      "epoch: 492, train loss: 0.001523, val loss: 0.005483, val mae: 0.051784, val r2: 0.861930\n",
      "epoch: 493, train loss: 0.001518, val loss: 0.005451, val mae: 0.051654, val r2: 0.862840\n",
      "epoch: 494, train loss: 0.001513, val loss: 0.005566, val mae: 0.052349, val r2: 0.859927\n",
      "epoch: 495, train loss: 0.001523, val loss: 0.005610, val mae: 0.052739, val r2: 0.858715\n",
      "epoch: 496, train loss: 0.001519, val loss: 0.005544, val mae: 0.052257, val r2: 0.860234\n",
      "epoch: 497, train loss: 0.001515, val loss: 0.005528, val mae: 0.052221, val r2: 0.860538\n",
      "epoch: 498, train loss: 0.001521, val loss: 0.005544, val mae: 0.052274, val r2: 0.860073\n",
      "epoch: 499, train loss: 0.001523, val loss: 0.005623, val mae: 0.052784, val r2: 0.857956\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.15, \n",
    "                               'layers': [8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 32,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 2,\n",
    "                      'dropout': 0.2, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 6, \n",
    "                      'epoches': 500,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc5be3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps!\n",
      "Transformer(\n",
      "  (element_embedding_temporal): Linear(in_features=294, out_features=192, bias=True)\n",
      "  (element_embedding_spatial): Linear(in_features=2695, out_features=1760, bias=True)\n",
      "  (pos_embedding): Embedding(6, 32)\n",
      "  (variable_embedding): Embedding(55, 32)\n",
      "  (temporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatial): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatiotemporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (keys): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (queries): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatter): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=11220, out_features=2640, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=2640, out_features=330, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch: 0, train loss: 0.018325, val loss: 0.015361, val mae: 0.097832, val r2: 0.610527\n",
      "epoch: 1, train loss: 0.010644, val loss: 0.013005, val mae: 0.087330, val r2: 0.670078\n",
      "epoch: 2, train loss: 0.009166, val loss: 0.011602, val mae: 0.081370, val r2: 0.705740\n",
      "epoch: 3, train loss: 0.008244, val loss: 0.010748, val mae: 0.077754, val r2: 0.727632\n",
      "epoch: 4, train loss: 0.007602, val loss: 0.010108, val mae: 0.075016, val r2: 0.744059\n",
      "epoch: 5, train loss: 0.007075, val loss: 0.009635, val mae: 0.073002, val r2: 0.756239\n",
      "epoch: 6, train loss: 0.006656, val loss: 0.009276, val mae: 0.071437, val r2: 0.765686\n",
      "epoch: 7, train loss: 0.006297, val loss: 0.008888, val mae: 0.069648, val r2: 0.775825\n",
      "epoch: 8, train loss: 0.005978, val loss: 0.008652, val mae: 0.068457, val r2: 0.782100\n",
      "epoch: 9, train loss: 0.005710, val loss: 0.008392, val mae: 0.067209, val r2: 0.788956\n",
      "epoch: 10, train loss: 0.005471, val loss: 0.008226, val mae: 0.066366, val r2: 0.793409\n",
      "epoch: 11, train loss: 0.005276, val loss: 0.008030, val mae: 0.065336, val r2: 0.798387\n",
      "epoch: 12, train loss: 0.005088, val loss: 0.007920, val mae: 0.064751, val r2: 0.801426\n",
      "epoch: 13, train loss: 0.004922, val loss: 0.007733, val mae: 0.063779, val r2: 0.806245\n",
      "epoch: 14, train loss: 0.004777, val loss: 0.007613, val mae: 0.063103, val r2: 0.809293\n",
      "epoch: 15, train loss: 0.004647, val loss: 0.007565, val mae: 0.062808, val r2: 0.810694\n",
      "epoch: 16, train loss: 0.004529, val loss: 0.007388, val mae: 0.061983, val r2: 0.815188\n",
      "epoch: 17, train loss: 0.004438, val loss: 0.007315, val mae: 0.061589, val r2: 0.817146\n",
      "epoch: 18, train loss: 0.004341, val loss: 0.007273, val mae: 0.061301, val r2: 0.818273\n",
      "epoch: 19, train loss: 0.004259, val loss: 0.007212, val mae: 0.060987, val r2: 0.819927\n",
      "epoch: 20, train loss: 0.004193, val loss: 0.007210, val mae: 0.061007, val r2: 0.820067\n",
      "epoch: 21, train loss: 0.004126, val loss: 0.007135, val mae: 0.060588, val r2: 0.822000\n",
      "epoch: 22, train loss: 0.004078, val loss: 0.007191, val mae: 0.060914, val r2: 0.820741\n",
      "epoch: 23, train loss: 0.004026, val loss: 0.007171, val mae: 0.060748, val r2: 0.821431\n",
      "epoch: 24, train loss: 0.003985, val loss: 0.007154, val mae: 0.060686, val r2: 0.821869\n",
      "epoch: 25, train loss: 0.003942, val loss: 0.007079, val mae: 0.060317, val r2: 0.823764\n",
      "epoch: 26, train loss: 0.003902, val loss: 0.007106, val mae: 0.060492, val r2: 0.823038\n",
      "epoch: 27, train loss: 0.003880, val loss: 0.007135, val mae: 0.060654, val r2: 0.822473\n",
      "epoch: 28, train loss: 0.003860, val loss: 0.007153, val mae: 0.060753, val r2: 0.822034\n",
      "epoch: 29, train loss: 0.003826, val loss: 0.007138, val mae: 0.060629, val r2: 0.822476\n",
      "epoch: 30, train loss: 0.003803, val loss: 0.007033, val mae: 0.060139, val r2: 0.824966\n",
      "epoch: 31, train loss: 0.003798, val loss: 0.007062, val mae: 0.060290, val r2: 0.824361\n",
      "epoch: 32, train loss: 0.003796, val loss: 0.006983, val mae: 0.059876, val r2: 0.826272\n",
      "epoch: 33, train loss: 0.003798, val loss: 0.006909, val mae: 0.059521, val r2: 0.828043\n",
      "epoch: 34, train loss: 0.003821, val loss: 0.006651, val mae: 0.058200, val r2: 0.834272\n",
      "epoch: 35, train loss: 0.003837, val loss: 0.006402, val mae: 0.056840, val r2: 0.840391\n",
      "epoch: 36, train loss: 0.003873, val loss: 0.006164, val mae: 0.055497, val r2: 0.845970\n",
      "epoch: 37, train loss: 0.003893, val loss: 0.006006, val mae: 0.054488, val r2: 0.849678\n",
      "epoch: 38, train loss: 0.003878, val loss: 0.005989, val mae: 0.054166, val r2: 0.849881\n",
      "epoch: 39, train loss: 0.003827, val loss: 0.006036, val mae: 0.054278, val r2: 0.848597\n",
      "epoch: 40, train loss: 0.003711, val loss: 0.006080, val mae: 0.054410, val r2: 0.847485\n",
      "epoch: 41, train loss: 0.003593, val loss: 0.005983, val mae: 0.054017, val r2: 0.849913\n",
      "epoch: 42, train loss: 0.003513, val loss: 0.005956, val mae: 0.053846, val r2: 0.850672\n",
      "epoch: 43, train loss: 0.003457, val loss: 0.005923, val mae: 0.053684, val r2: 0.851520\n",
      "epoch: 44, train loss: 0.003453, val loss: 0.005916, val mae: 0.053641, val r2: 0.851598\n",
      "epoch: 45, train loss: 0.003448, val loss: 0.005899, val mae: 0.053535, val r2: 0.852070\n",
      "epoch: 46, train loss: 0.003457, val loss: 0.005893, val mae: 0.053501, val r2: 0.852189\n",
      "epoch: 47, train loss: 0.003458, val loss: 0.005865, val mae: 0.053381, val r2: 0.852909\n",
      "epoch: 48, train loss: 0.003477, val loss: 0.005870, val mae: 0.053349, val r2: 0.852762\n",
      "epoch: 49, train loss: 0.003480, val loss: 0.005880, val mae: 0.053447, val r2: 0.852560\n",
      "epoch: 50, train loss: 0.003489, val loss: 0.005882, val mae: 0.053496, val r2: 0.852629\n",
      "epoch: 51, train loss: 0.003482, val loss: 0.005871, val mae: 0.053494, val r2: 0.853045\n",
      "epoch: 52, train loss: 0.003479, val loss: 0.005901, val mae: 0.053739, val r2: 0.852463\n",
      "epoch: 53, train loss: 0.003451, val loss: 0.005956, val mae: 0.054051, val r2: 0.851222\n",
      "epoch: 54, train loss: 0.003397, val loss: 0.005978, val mae: 0.054224, val r2: 0.850832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55, train loss: 0.003336, val loss: 0.006024, val mae: 0.054462, val r2: 0.849891\n",
      "epoch: 56, train loss: 0.003270, val loss: 0.006012, val mae: 0.054412, val r2: 0.850198\n",
      "epoch: 57, train loss: 0.003229, val loss: 0.005988, val mae: 0.054287, val r2: 0.850940\n",
      "epoch: 58, train loss: 0.003210, val loss: 0.006049, val mae: 0.054672, val r2: 0.849517\n",
      "epoch: 59, train loss: 0.003212, val loss: 0.006000, val mae: 0.054475, val r2: 0.850741\n",
      "epoch: 60, train loss: 0.003234, val loss: 0.006004, val mae: 0.054554, val r2: 0.850686\n",
      "epoch: 61, train loss: 0.003251, val loss: 0.005935, val mae: 0.054205, val r2: 0.852278\n",
      "epoch: 62, train loss: 0.003279, val loss: 0.005838, val mae: 0.053679, val r2: 0.854509\n",
      "epoch: 63, train loss: 0.003313, val loss: 0.005694, val mae: 0.052769, val r2: 0.857908\n",
      "epoch: 64, train loss: 0.003330, val loss: 0.005629, val mae: 0.052305, val r2: 0.859224\n",
      "epoch: 65, train loss: 0.003344, val loss: 0.005657, val mae: 0.052223, val r2: 0.858181\n",
      "epoch: 66, train loss: 0.003296, val loss: 0.005758, val mae: 0.052599, val r2: 0.855529\n",
      "epoch: 67, train loss: 0.003205, val loss: 0.005789, val mae: 0.052675, val r2: 0.854741\n",
      "epoch: 68, train loss: 0.003102, val loss: 0.005758, val mae: 0.052570, val r2: 0.855481\n",
      "epoch: 69, train loss: 0.003043, val loss: 0.005721, val mae: 0.052414, val r2: 0.856429\n",
      "epoch: 70, train loss: 0.003048, val loss: 0.005688, val mae: 0.052313, val r2: 0.857237\n",
      "epoch: 71, train loss: 0.003070, val loss: 0.005694, val mae: 0.052305, val r2: 0.857078\n",
      "epoch: 72, train loss: 0.003092, val loss: 0.005680, val mae: 0.052249, val r2: 0.857422\n",
      "epoch: 73, train loss: 0.003101, val loss: 0.005641, val mae: 0.052080, val r2: 0.858440\n",
      "epoch: 74, train loss: 0.003103, val loss: 0.005644, val mae: 0.052167, val r2: 0.858433\n",
      "epoch: 75, train loss: 0.003078, val loss: 0.005629, val mae: 0.052119, val r2: 0.858974\n",
      "epoch: 76, train loss: 0.003037, val loss: 0.005639, val mae: 0.052246, val r2: 0.858956\n",
      "epoch: 77, train loss: 0.003002, val loss: 0.005644, val mae: 0.052341, val r2: 0.858913\n",
      "epoch: 78, train loss: 0.002964, val loss: 0.005650, val mae: 0.052350, val r2: 0.858722\n",
      "epoch: 79, train loss: 0.002958, val loss: 0.005622, val mae: 0.052256, val r2: 0.859623\n",
      "epoch: 80, train loss: 0.002971, val loss: 0.005617, val mae: 0.052288, val r2: 0.859834\n",
      "epoch: 81, train loss: 0.002998, val loss: 0.005620, val mae: 0.052271, val r2: 0.859760\n",
      "epoch: 82, train loss: 0.003031, val loss: 0.005567, val mae: 0.051992, val r2: 0.860957\n",
      "epoch: 83, train loss: 0.003064, val loss: 0.005508, val mae: 0.051577, val r2: 0.862238\n",
      "epoch: 84, train loss: 0.003076, val loss: 0.005498, val mae: 0.051339, val r2: 0.862179\n",
      "epoch: 85, train loss: 0.003066, val loss: 0.005637, val mae: 0.051872, val r2: 0.858475\n",
      "epoch: 86, train loss: 0.003004, val loss: 0.005723, val mae: 0.052228, val r2: 0.856184\n",
      "epoch: 87, train loss: 0.002934, val loss: 0.005747, val mae: 0.052351, val r2: 0.855594\n",
      "epoch: 88, train loss: 0.002894, val loss: 0.005707, val mae: 0.052210, val r2: 0.856474\n",
      "epoch: 89, train loss: 0.002894, val loss: 0.005680, val mae: 0.052203, val r2: 0.857108\n",
      "epoch: 90, train loss: 0.002912, val loss: 0.005666, val mae: 0.052179, val r2: 0.857446\n",
      "epoch: 91, train loss: 0.002905, val loss: 0.005647, val mae: 0.052130, val r2: 0.858001\n",
      "epoch: 92, train loss: 0.002882, val loss: 0.005618, val mae: 0.052105, val r2: 0.858955\n",
      "epoch: 93, train loss: 0.002854, val loss: 0.005620, val mae: 0.052083, val r2: 0.859011\n",
      "epoch: 94, train loss: 0.002836, val loss: 0.005613, val mae: 0.052073, val r2: 0.859314\n",
      "epoch: 95, train loss: 0.002829, val loss: 0.005602, val mae: 0.051998, val r2: 0.859531\n",
      "epoch: 96, train loss: 0.002838, val loss: 0.005574, val mae: 0.051879, val r2: 0.860307\n",
      "epoch: 97, train loss: 0.002854, val loss: 0.005561, val mae: 0.051731, val r2: 0.860462\n",
      "epoch: 98, train loss: 0.002866, val loss: 0.005570, val mae: 0.051753, val r2: 0.860115\n",
      "epoch: 99, train loss: 0.002863, val loss: 0.005599, val mae: 0.051798, val r2: 0.859266\n",
      "epoch: 100, train loss: 0.002854, val loss: 0.005620, val mae: 0.051872, val r2: 0.858680\n",
      "epoch: 101, train loss: 0.002843, val loss: 0.005713, val mae: 0.052363, val r2: 0.856148\n",
      "epoch: 102, train loss: 0.002846, val loss: 0.005736, val mae: 0.052583, val r2: 0.855458\n",
      "epoch: 103, train loss: 0.002865, val loss: 0.005782, val mae: 0.052897, val r2: 0.854154\n",
      "epoch: 104, train loss: 0.002876, val loss: 0.005861, val mae: 0.053432, val r2: 0.852185\n",
      "epoch: 105, train loss: 0.002881, val loss: 0.005992, val mae: 0.054159, val r2: 0.848888\n",
      "epoch: 106, train loss: 0.002860, val loss: 0.006114, val mae: 0.054854, val r2: 0.845746\n",
      "epoch: 107, train loss: 0.002863, val loss: 0.006117, val mae: 0.054903, val r2: 0.845641\n",
      "epoch: 108, train loss: 0.002854, val loss: 0.006190, val mae: 0.055282, val r2: 0.843620\n",
      "epoch: 109, train loss: 0.002878, val loss: 0.006281, val mae: 0.055811, val r2: 0.841085\n",
      "epoch: 110, train loss: 0.002906, val loss: 0.006390, val mae: 0.056438, val r2: 0.837975\n",
      "epoch: 111, train loss: 0.002933, val loss: 0.006573, val mae: 0.057447, val r2: 0.833032\n",
      "epoch: 112, train loss: 0.002961, val loss: 0.006980, val mae: 0.059530, val r2: 0.821953\n",
      "epoch: 113, train loss: 0.002992, val loss: 0.007337, val mae: 0.061400, val r2: 0.812582\n",
      "epoch: 114, train loss: 0.003011, val loss: 0.007823, val mae: 0.063713, val r2: 0.799449\n",
      "epoch: 115, train loss: 0.003056, val loss: 0.008390, val mae: 0.066366, val r2: 0.784414\n",
      "epoch: 116, train loss: 0.003073, val loss: 0.008196, val mae: 0.065549, val r2: 0.788884\n",
      "epoch: 117, train loss: 0.003127, val loss: 0.008323, val mae: 0.066140, val r2: 0.785205\n",
      "epoch: 118, train loss: 0.003194, val loss: 0.008227, val mae: 0.065767, val r2: 0.787628\n",
      "epoch: 119, train loss: 0.003272, val loss: 0.007961, val mae: 0.064605, val r2: 0.794702\n",
      "epoch: 120, train loss: 0.003327, val loss: 0.007469, val mae: 0.062207, val r2: 0.807681\n",
      "epoch: 121, train loss: 0.003320, val loss: 0.006695, val mae: 0.058346, val r2: 0.828650\n",
      "epoch: 122, train loss: 0.003235, val loss: 0.006137, val mae: 0.055415, val r2: 0.843951\n",
      "epoch: 123, train loss: 0.003121, val loss: 0.005807, val mae: 0.053569, val r2: 0.852948\n",
      "epoch: 124, train loss: 0.002980, val loss: 0.005630, val mae: 0.052577, val r2: 0.857847\n",
      "epoch: 125, train loss: 0.002894, val loss: 0.005524, val mae: 0.052033, val r2: 0.860822\n",
      "epoch: 126, train loss: 0.002831, val loss: 0.005431, val mae: 0.051507, val r2: 0.863366\n",
      "epoch: 127, train loss: 0.002795, val loss: 0.005394, val mae: 0.051333, val r2: 0.864460\n",
      "epoch: 128, train loss: 0.002785, val loss: 0.005362, val mae: 0.051209, val r2: 0.865407\n",
      "epoch: 129, train loss: 0.002777, val loss: 0.005302, val mae: 0.050874, val r2: 0.866958\n",
      "epoch: 130, train loss: 0.002757, val loss: 0.005257, val mae: 0.050607, val r2: 0.868200\n",
      "epoch: 131, train loss: 0.002735, val loss: 0.005254, val mae: 0.050594, val r2: 0.868316\n",
      "epoch: 132, train loss: 0.002724, val loss: 0.005214, val mae: 0.050354, val r2: 0.869357\n",
      "epoch: 133, train loss: 0.002699, val loss: 0.005230, val mae: 0.050455, val r2: 0.868973\n",
      "epoch: 134, train loss: 0.002670, val loss: 0.005168, val mae: 0.050055, val r2: 0.870556\n",
      "epoch: 135, train loss: 0.002637, val loss: 0.005166, val mae: 0.050033, val r2: 0.870607\n",
      "epoch: 136, train loss: 0.002606, val loss: 0.005161, val mae: 0.050042, val r2: 0.870734\n",
      "epoch: 137, train loss: 0.002578, val loss: 0.005167, val mae: 0.050134, val r2: 0.870597\n",
      "epoch: 138, train loss: 0.002553, val loss: 0.005147, val mae: 0.049991, val r2: 0.871131\n",
      "epoch: 139, train loss: 0.002522, val loss: 0.005145, val mae: 0.049954, val r2: 0.871131\n",
      "epoch: 140, train loss: 0.002498, val loss: 0.005130, val mae: 0.049944, val r2: 0.871467\n",
      "epoch: 141, train loss: 0.002478, val loss: 0.005150, val mae: 0.050023, val r2: 0.870976\n",
      "epoch: 142, train loss: 0.002454, val loss: 0.005148, val mae: 0.050021, val r2: 0.870960\n",
      "epoch: 143, train loss: 0.002437, val loss: 0.005188, val mae: 0.050323, val r2: 0.869975\n",
      "epoch: 144, train loss: 0.002424, val loss: 0.005197, val mae: 0.050393, val r2: 0.869627\n",
      "epoch: 145, train loss: 0.002407, val loss: 0.005214, val mae: 0.050472, val r2: 0.869185\n",
      "epoch: 146, train loss: 0.002400, val loss: 0.005241, val mae: 0.050684, val r2: 0.868447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 147, train loss: 0.002398, val loss: 0.005262, val mae: 0.050812, val r2: 0.867769\n",
      "epoch: 148, train loss: 0.002393, val loss: 0.005343, val mae: 0.051247, val r2: 0.865662\n",
      "epoch: 149, train loss: 0.002391, val loss: 0.005439, val mae: 0.051876, val r2: 0.863046\n",
      "epoch: 150, train loss: 0.002393, val loss: 0.005474, val mae: 0.052048, val r2: 0.862061\n",
      "epoch: 151, train loss: 0.002396, val loss: 0.005516, val mae: 0.052228, val r2: 0.860785\n",
      "epoch: 152, train loss: 0.002396, val loss: 0.005678, val mae: 0.053181, val r2: 0.856388\n",
      "epoch: 153, train loss: 0.002395, val loss: 0.005697, val mae: 0.053226, val r2: 0.855836\n",
      "epoch: 154, train loss: 0.002405, val loss: 0.005864, val mae: 0.054150, val r2: 0.851194\n",
      "epoch: 155, train loss: 0.002418, val loss: 0.005931, val mae: 0.054472, val r2: 0.849187\n",
      "epoch: 156, train loss: 0.002438, val loss: 0.005995, val mae: 0.054729, val r2: 0.847423\n",
      "epoch: 157, train loss: 0.002458, val loss: 0.006305, val mae: 0.056372, val r2: 0.838923\n",
      "epoch: 158, train loss: 0.002481, val loss: 0.006675, val mae: 0.058172, val r2: 0.828858\n",
      "epoch: 159, train loss: 0.002507, val loss: 0.007468, val mae: 0.061876, val r2: 0.807833\n",
      "epoch: 160, train loss: 0.002525, val loss: 0.008289, val mae: 0.065492, val r2: 0.786026\n",
      "epoch: 161, train loss: 0.002526, val loss: 0.008730, val mae: 0.067387, val r2: 0.774535\n",
      "epoch: 162, train loss: 0.002525, val loss: 0.008504, val mae: 0.066337, val r2: 0.780377\n",
      "epoch: 163, train loss: 0.002550, val loss: 0.007025, val mae: 0.059817, val r2: 0.819265\n",
      "epoch: 164, train loss: 0.002564, val loss: 0.006503, val mae: 0.057421, val r2: 0.833015\n",
      "epoch: 165, train loss: 0.002535, val loss: 0.006534, val mae: 0.057655, val r2: 0.832454\n",
      "epoch: 166, train loss: 0.002529, val loss: 0.006633, val mae: 0.058108, val r2: 0.830142\n",
      "epoch: 167, train loss: 0.002499, val loss: 0.006134, val mae: 0.055640, val r2: 0.843521\n",
      "epoch: 168, train loss: 0.002475, val loss: 0.005646, val mae: 0.052987, val r2: 0.856837\n",
      "epoch: 169, train loss: 0.002456, val loss: 0.005309, val mae: 0.051050, val r2: 0.866165\n",
      "epoch: 170, train loss: 0.002440, val loss: 0.005192, val mae: 0.050226, val r2: 0.869447\n",
      "epoch: 171, train loss: 0.002442, val loss: 0.005120, val mae: 0.049716, val r2: 0.871451\n",
      "epoch: 172, train loss: 0.002449, val loss: 0.005097, val mae: 0.049486, val r2: 0.872053\n",
      "epoch: 173, train loss: 0.002457, val loss: 0.005095, val mae: 0.049342, val r2: 0.872036\n",
      "epoch: 174, train loss: 0.002457, val loss: 0.005124, val mae: 0.049514, val r2: 0.871261\n",
      "epoch: 175, train loss: 0.002460, val loss: 0.005130, val mae: 0.049539, val r2: 0.871128\n",
      "epoch: 176, train loss: 0.002485, val loss: 0.005135, val mae: 0.049688, val r2: 0.871024\n",
      "epoch: 177, train loss: 0.002489, val loss: 0.005111, val mae: 0.049580, val r2: 0.871723\n",
      "epoch: 178, train loss: 0.002481, val loss: 0.005127, val mae: 0.049625, val r2: 0.871272\n",
      "epoch: 179, train loss: 0.002472, val loss: 0.005126, val mae: 0.049633, val r2: 0.871312\n",
      "epoch: 180, train loss: 0.002454, val loss: 0.005130, val mae: 0.049719, val r2: 0.871306\n",
      "epoch: 181, train loss: 0.002436, val loss: 0.005138, val mae: 0.049786, val r2: 0.871088\n",
      "epoch: 182, train loss: 0.002398, val loss: 0.005146, val mae: 0.049897, val r2: 0.870936\n",
      "epoch: 183, train loss: 0.002355, val loss: 0.005174, val mae: 0.050066, val r2: 0.870246\n",
      "epoch: 184, train loss: 0.002312, val loss: 0.005175, val mae: 0.050068, val r2: 0.870195\n",
      "epoch: 185, train loss: 0.002266, val loss: 0.005181, val mae: 0.050184, val r2: 0.869918\n",
      "epoch: 186, train loss: 0.002235, val loss: 0.005211, val mae: 0.050416, val r2: 0.869112\n",
      "epoch: 187, train loss: 0.002213, val loss: 0.005295, val mae: 0.050913, val r2: 0.866827\n",
      "epoch: 188, train loss: 0.002208, val loss: 0.005353, val mae: 0.051312, val r2: 0.865223\n",
      "epoch: 189, train loss: 0.002207, val loss: 0.005426, val mae: 0.051741, val r2: 0.863087\n",
      "epoch: 190, train loss: 0.002205, val loss: 0.005570, val mae: 0.052535, val r2: 0.859281\n",
      "epoch: 191, train loss: 0.002210, val loss: 0.005664, val mae: 0.053024, val r2: 0.856797\n",
      "epoch: 192, train loss: 0.002211, val loss: 0.005773, val mae: 0.053624, val r2: 0.853616\n",
      "epoch: 193, train loss: 0.002206, val loss: 0.005947, val mae: 0.054515, val r2: 0.848992\n",
      "epoch: 194, train loss: 0.002204, val loss: 0.005971, val mae: 0.054680, val r2: 0.848072\n",
      "epoch: 195, train loss: 0.002203, val loss: 0.006064, val mae: 0.055134, val r2: 0.845470\n",
      "epoch: 196, train loss: 0.002195, val loss: 0.006111, val mae: 0.055363, val r2: 0.844253\n",
      "epoch: 197, train loss: 0.002190, val loss: 0.006161, val mae: 0.055583, val r2: 0.842714\n",
      "epoch: 198, train loss: 0.002183, val loss: 0.006304, val mae: 0.056324, val r2: 0.838909\n",
      "epoch: 199, train loss: 0.002177, val loss: 0.006365, val mae: 0.056606, val r2: 0.837258\n",
      "epoch: 200, train loss: 0.002168, val loss: 0.006363, val mae: 0.056673, val r2: 0.837499\n",
      "epoch: 201, train loss: 0.002163, val loss: 0.006338, val mae: 0.056527, val r2: 0.838422\n",
      "epoch: 202, train loss: 0.002165, val loss: 0.006040, val mae: 0.055073, val r2: 0.846509\n",
      "epoch: 203, train loss: 0.002163, val loss: 0.005750, val mae: 0.053515, val r2: 0.854452\n",
      "epoch: 204, train loss: 0.002175, val loss: 0.005446, val mae: 0.051812, val r2: 0.862736\n",
      "epoch: 205, train loss: 0.002186, val loss: 0.005187, val mae: 0.050177, val r2: 0.869623\n",
      "epoch: 206, train loss: 0.002194, val loss: 0.005126, val mae: 0.049579, val r2: 0.871306\n",
      "epoch: 207, train loss: 0.002189, val loss: 0.005166, val mae: 0.049500, val r2: 0.870305\n",
      "epoch: 208, train loss: 0.002188, val loss: 0.005238, val mae: 0.049821, val r2: 0.868430\n",
      "epoch: 209, train loss: 0.002188, val loss: 0.005256, val mae: 0.049895, val r2: 0.867890\n",
      "epoch: 210, train loss: 0.002185, val loss: 0.005196, val mae: 0.049680, val r2: 0.869435\n",
      "epoch: 211, train loss: 0.002171, val loss: 0.005178, val mae: 0.049712, val r2: 0.869918\n",
      "epoch: 212, train loss: 0.002152, val loss: 0.005163, val mae: 0.049729, val r2: 0.870455\n",
      "epoch: 213, train loss: 0.002135, val loss: 0.005138, val mae: 0.049686, val r2: 0.871091\n",
      "epoch: 214, train loss: 0.002115, val loss: 0.005134, val mae: 0.049632, val r2: 0.871263\n",
      "epoch: 215, train loss: 0.002103, val loss: 0.005163, val mae: 0.049800, val r2: 0.870536\n",
      "epoch: 216, train loss: 0.002086, val loss: 0.005153, val mae: 0.049813, val r2: 0.870763\n",
      "epoch: 217, train loss: 0.002069, val loss: 0.005199, val mae: 0.049978, val r2: 0.869498\n",
      "epoch: 218, train loss: 0.002059, val loss: 0.005222, val mae: 0.050078, val r2: 0.868864\n",
      "epoch: 219, train loss: 0.002060, val loss: 0.005234, val mae: 0.050145, val r2: 0.868417\n",
      "epoch: 220, train loss: 0.002061, val loss: 0.005285, val mae: 0.050607, val r2: 0.866978\n",
      "epoch: 221, train loss: 0.002057, val loss: 0.005473, val mae: 0.051738, val r2: 0.861947\n",
      "epoch: 222, train loss: 0.002052, val loss: 0.005714, val mae: 0.053206, val r2: 0.855541\n",
      "epoch: 223, train loss: 0.002047, val loss: 0.006033, val mae: 0.054913, val r2: 0.846969\n",
      "epoch: 224, train loss: 0.002058, val loss: 0.006464, val mae: 0.057069, val r2: 0.835339\n",
      "epoch: 225, train loss: 0.002064, val loss: 0.006561, val mae: 0.057436, val r2: 0.832569\n",
      "epoch: 226, train loss: 0.002080, val loss: 0.006388, val mae: 0.056627, val r2: 0.837413\n",
      "epoch: 227, train loss: 0.002092, val loss: 0.006071, val mae: 0.055053, val r2: 0.845756\n",
      "epoch: 228, train loss: 0.002092, val loss: 0.005707, val mae: 0.053123, val r2: 0.855398\n",
      "epoch: 229, train loss: 0.002075, val loss: 0.005467, val mae: 0.051794, val r2: 0.862079\n",
      "epoch: 230, train loss: 0.002065, val loss: 0.005352, val mae: 0.051031, val r2: 0.865255\n",
      "epoch: 231, train loss: 0.002074, val loss: 0.005393, val mae: 0.051191, val r2: 0.864116\n",
      "epoch: 232, train loss: 0.002089, val loss: 0.005393, val mae: 0.051185, val r2: 0.864155\n",
      "epoch: 233, train loss: 0.002081, val loss: 0.005329, val mae: 0.050897, val r2: 0.866016\n",
      "epoch: 234, train loss: 0.002065, val loss: 0.005248, val mae: 0.050520, val r2: 0.868184\n",
      "epoch: 235, train loss: 0.002040, val loss: 0.005175, val mae: 0.049940, val r2: 0.870211\n",
      "epoch: 236, train loss: 0.002018, val loss: 0.005198, val mae: 0.049761, val r2: 0.869621\n",
      "epoch: 237, train loss: 0.002007, val loss: 0.005236, val mae: 0.049804, val r2: 0.868676\n",
      "epoch: 238, train loss: 0.002012, val loss: 0.005211, val mae: 0.049874, val r2: 0.869323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 239, train loss: 0.002004, val loss: 0.005234, val mae: 0.050269, val r2: 0.868639\n",
      "epoch: 240, train loss: 0.001968, val loss: 0.005287, val mae: 0.050736, val r2: 0.867191\n",
      "epoch: 241, train loss: 0.001937, val loss: 0.005343, val mae: 0.051121, val r2: 0.865735\n",
      "epoch: 242, train loss: 0.001933, val loss: 0.005416, val mae: 0.051577, val r2: 0.863677\n",
      "epoch: 243, train loss: 0.001930, val loss: 0.005396, val mae: 0.051552, val r2: 0.864001\n",
      "epoch: 244, train loss: 0.001926, val loss: 0.005343, val mae: 0.051187, val r2: 0.865363\n",
      "epoch: 245, train loss: 0.001915, val loss: 0.005320, val mae: 0.051041, val r2: 0.865891\n",
      "epoch: 246, train loss: 0.001906, val loss: 0.005315, val mae: 0.050978, val r2: 0.865995\n",
      "epoch: 247, train loss: 0.001908, val loss: 0.005362, val mae: 0.051121, val r2: 0.864721\n",
      "epoch: 248, train loss: 0.001913, val loss: 0.005424, val mae: 0.051469, val r2: 0.863115\n",
      "epoch: 249, train loss: 0.001911, val loss: 0.005435, val mae: 0.051641, val r2: 0.862915\n",
      "epoch: 250, train loss: 0.001920, val loss: 0.005511, val mae: 0.052201, val r2: 0.860858\n",
      "epoch: 251, train loss: 0.001921, val loss: 0.005407, val mae: 0.051627, val r2: 0.863783\n",
      "epoch: 252, train loss: 0.001922, val loss: 0.005286, val mae: 0.050854, val r2: 0.867145\n",
      "epoch: 253, train loss: 0.001919, val loss: 0.005222, val mae: 0.050385, val r2: 0.869080\n",
      "epoch: 254, train loss: 0.001905, val loss: 0.005249, val mae: 0.050096, val r2: 0.868501\n",
      "epoch: 255, train loss: 0.001891, val loss: 0.005363, val mae: 0.050304, val r2: 0.865591\n",
      "epoch: 256, train loss: 0.001919, val loss: 0.005347, val mae: 0.050329, val r2: 0.865773\n",
      "epoch: 257, train loss: 0.001944, val loss: 0.005339, val mae: 0.050577, val r2: 0.865897\n",
      "epoch: 258, train loss: 0.001911, val loss: 0.005336, val mae: 0.050756, val r2: 0.865980\n",
      "epoch: 259, train loss: 0.001866, val loss: 0.005361, val mae: 0.051113, val r2: 0.865398\n",
      "epoch: 260, train loss: 0.001862, val loss: 0.005383, val mae: 0.051374, val r2: 0.864705\n",
      "epoch: 261, train loss: 0.001873, val loss: 0.005280, val mae: 0.050741, val r2: 0.867378\n",
      "epoch: 262, train loss: 0.001857, val loss: 0.005206, val mae: 0.050251, val r2: 0.869385\n",
      "epoch: 263, train loss: 0.001829, val loss: 0.005209, val mae: 0.050112, val r2: 0.869278\n",
      "epoch: 264, train loss: 0.001824, val loss: 0.005283, val mae: 0.050495, val r2: 0.867318\n",
      "epoch: 265, train loss: 0.001822, val loss: 0.005364, val mae: 0.050933, val r2: 0.865086\n",
      "epoch: 266, train loss: 0.001809, val loss: 0.005391, val mae: 0.051157, val r2: 0.864326\n",
      "epoch: 267, train loss: 0.001805, val loss: 0.005441, val mae: 0.051517, val r2: 0.862966\n",
      "epoch: 268, train loss: 0.001804, val loss: 0.005399, val mae: 0.051465, val r2: 0.864032\n",
      "epoch: 269, train loss: 0.001809, val loss: 0.005393, val mae: 0.051478, val r2: 0.864166\n",
      "epoch: 270, train loss: 0.001807, val loss: 0.005391, val mae: 0.051501, val r2: 0.864414\n",
      "epoch: 271, train loss: 0.001808, val loss: 0.005299, val mae: 0.050958, val r2: 0.866829\n",
      "epoch: 272, train loss: 0.001798, val loss: 0.005286, val mae: 0.050603, val r2: 0.867364\n",
      "epoch: 273, train loss: 0.001790, val loss: 0.005361, val mae: 0.050863, val r2: 0.865381\n",
      "epoch: 274, train loss: 0.001786, val loss: 0.005412, val mae: 0.050993, val r2: 0.864005\n",
      "epoch: 275, train loss: 0.001788, val loss: 0.005408, val mae: 0.051032, val r2: 0.864009\n",
      "epoch: 276, train loss: 0.001795, val loss: 0.005371, val mae: 0.050853, val r2: 0.864952\n",
      "epoch: 277, train loss: 0.001786, val loss: 0.005384, val mae: 0.051168, val r2: 0.864625\n",
      "epoch: 278, train loss: 0.001779, val loss: 0.005445, val mae: 0.051633, val r2: 0.863056\n",
      "epoch: 279, train loss: 0.001768, val loss: 0.005562, val mae: 0.052360, val r2: 0.860118\n",
      "epoch: 280, train loss: 0.001774, val loss: 0.005476, val mae: 0.051917, val r2: 0.862257\n",
      "epoch: 281, train loss: 0.001783, val loss: 0.005343, val mae: 0.051147, val r2: 0.865635\n",
      "epoch: 282, train loss: 0.001779, val loss: 0.005262, val mae: 0.050625, val r2: 0.867975\n",
      "epoch: 283, train loss: 0.001759, val loss: 0.005319, val mae: 0.050642, val r2: 0.866592\n",
      "epoch: 284, train loss: 0.001740, val loss: 0.005351, val mae: 0.050618, val r2: 0.865765\n",
      "epoch: 285, train loss: 0.001743, val loss: 0.005362, val mae: 0.050696, val r2: 0.865373\n",
      "epoch: 286, train loss: 0.001761, val loss: 0.005403, val mae: 0.051094, val r2: 0.864312\n",
      "epoch: 287, train loss: 0.001738, val loss: 0.005464, val mae: 0.051593, val r2: 0.862809\n",
      "epoch: 288, train loss: 0.001716, val loss: 0.005564, val mae: 0.052257, val r2: 0.860125\n",
      "epoch: 289, train loss: 0.001709, val loss: 0.005606, val mae: 0.052490, val r2: 0.859069\n",
      "epoch: 290, train loss: 0.001709, val loss: 0.005688, val mae: 0.052969, val r2: 0.856868\n",
      "epoch: 291, train loss: 0.001719, val loss: 0.005528, val mae: 0.052104, val r2: 0.860833\n",
      "epoch: 292, train loss: 0.001720, val loss: 0.005430, val mae: 0.051492, val r2: 0.863498\n",
      "epoch: 293, train loss: 0.001713, val loss: 0.005326, val mae: 0.050828, val r2: 0.866217\n",
      "epoch: 294, train loss: 0.001707, val loss: 0.005309, val mae: 0.050574, val r2: 0.866798\n",
      "epoch: 295, train loss: 0.001701, val loss: 0.005308, val mae: 0.050651, val r2: 0.866774\n",
      "epoch: 296, train loss: 0.001710, val loss: 0.005393, val mae: 0.051192, val r2: 0.864407\n",
      "epoch: 297, train loss: 0.001706, val loss: 0.005498, val mae: 0.051988, val r2: 0.861699\n",
      "epoch: 298, train loss: 0.001701, val loss: 0.005689, val mae: 0.053151, val r2: 0.856640\n",
      "epoch: 299, train loss: 0.001703, val loss: 0.005882, val mae: 0.054135, val r2: 0.851519\n",
      "epoch: 300, train loss: 0.001706, val loss: 0.005964, val mae: 0.054567, val r2: 0.849600\n",
      "epoch: 301, train loss: 0.001705, val loss: 0.006045, val mae: 0.054873, val r2: 0.847651\n",
      "epoch: 302, train loss: 0.001709, val loss: 0.005916, val mae: 0.054132, val r2: 0.850995\n",
      "epoch: 303, train loss: 0.001716, val loss: 0.005609, val mae: 0.052656, val r2: 0.858851\n",
      "epoch: 304, train loss: 0.001712, val loss: 0.005472, val mae: 0.051865, val r2: 0.862431\n",
      "epoch: 305, train loss: 0.001698, val loss: 0.005449, val mae: 0.051506, val r2: 0.863007\n",
      "epoch: 306, train loss: 0.001688, val loss: 0.005483, val mae: 0.051683, val r2: 0.862105\n",
      "epoch: 307, train loss: 0.001708, val loss: 0.005601, val mae: 0.052277, val r2: 0.858978\n",
      "epoch: 308, train loss: 0.001713, val loss: 0.005823, val mae: 0.053477, val r2: 0.853383\n",
      "epoch: 309, train loss: 0.001703, val loss: 0.006056, val mae: 0.054584, val r2: 0.847636\n",
      "epoch: 310, train loss: 0.001694, val loss: 0.006083, val mae: 0.054900, val r2: 0.847135\n",
      "epoch: 311, train loss: 0.001706, val loss: 0.005920, val mae: 0.054225, val r2: 0.851066\n",
      "epoch: 312, train loss: 0.001724, val loss: 0.005812, val mae: 0.053709, val r2: 0.853712\n",
      "epoch: 313, train loss: 0.001722, val loss: 0.005660, val mae: 0.052749, val r2: 0.857386\n",
      "epoch: 314, train loss: 0.001700, val loss: 0.005855, val mae: 0.053584, val r2: 0.852240\n",
      "epoch: 315, train loss: 0.001678, val loss: 0.006128, val mae: 0.054887, val r2: 0.845294\n",
      "epoch: 316, train loss: 0.001677, val loss: 0.006260, val mae: 0.055528, val r2: 0.842103\n",
      "epoch: 317, train loss: 0.001669, val loss: 0.006336, val mae: 0.056007, val r2: 0.840186\n",
      "epoch: 318, train loss: 0.001661, val loss: 0.006430, val mae: 0.056387, val r2: 0.837898\n",
      "epoch: 319, train loss: 0.001658, val loss: 0.006453, val mae: 0.056571, val r2: 0.837137\n",
      "epoch: 320, train loss: 0.001662, val loss: 0.006360, val mae: 0.056203, val r2: 0.839197\n",
      "epoch: 321, train loss: 0.001660, val loss: 0.006255, val mae: 0.055598, val r2: 0.841825\n",
      "epoch: 322, train loss: 0.001654, val loss: 0.006169, val mae: 0.055267, val r2: 0.843861\n",
      "epoch: 323, train loss: 0.001648, val loss: 0.006327, val mae: 0.055994, val r2: 0.839836\n",
      "epoch: 324, train loss: 0.001640, val loss: 0.006433, val mae: 0.056681, val r2: 0.837224\n",
      "epoch: 325, train loss: 0.001635, val loss: 0.006471, val mae: 0.056920, val r2: 0.836230\n",
      "epoch: 326, train loss: 0.001634, val loss: 0.006500, val mae: 0.057126, val r2: 0.835443\n",
      "epoch: 327, train loss: 0.001637, val loss: 0.006347, val mae: 0.056396, val r2: 0.839289\n",
      "epoch: 328, train loss: 0.001638, val loss: 0.006147, val mae: 0.055310, val r2: 0.844686\n",
      "epoch: 329, train loss: 0.001641, val loss: 0.005949, val mae: 0.054212, val r2: 0.849957\n",
      "epoch: 330, train loss: 0.001642, val loss: 0.005895, val mae: 0.053928, val r2: 0.851510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 331, train loss: 0.001643, val loss: 0.005741, val mae: 0.053123, val r2: 0.855561\n",
      "epoch: 332, train loss: 0.001641, val loss: 0.005835, val mae: 0.053581, val r2: 0.853361\n",
      "epoch: 333, train loss: 0.001641, val loss: 0.005839, val mae: 0.053565, val r2: 0.853365\n",
      "epoch: 334, train loss: 0.001643, val loss: 0.005839, val mae: 0.053498, val r2: 0.853661\n",
      "epoch: 335, train loss: 0.001644, val loss: 0.005865, val mae: 0.053499, val r2: 0.853113\n",
      "epoch: 336, train loss: 0.001651, val loss: 0.005871, val mae: 0.053439, val r2: 0.852823\n",
      "epoch: 337, train loss: 0.001652, val loss: 0.005756, val mae: 0.052737, val r2: 0.855632\n",
      "epoch: 338, train loss: 0.001661, val loss: 0.005750, val mae: 0.052545, val r2: 0.855603\n",
      "epoch: 339, train loss: 0.001679, val loss: 0.005872, val mae: 0.053152, val r2: 0.852445\n",
      "epoch: 340, train loss: 0.001696, val loss: 0.006116, val mae: 0.054454, val r2: 0.846251\n",
      "epoch: 341, train loss: 0.001696, val loss: 0.006470, val mae: 0.056064, val r2: 0.837531\n",
      "epoch: 342, train loss: 0.001667, val loss: 0.006816, val mae: 0.057909, val r2: 0.828632\n",
      "epoch: 343, train loss: 0.001642, val loss: 0.006694, val mae: 0.057404, val r2: 0.831477\n",
      "epoch: 344, train loss: 0.001657, val loss: 0.006136, val mae: 0.054737, val r2: 0.845096\n",
      "epoch: 345, train loss: 0.001648, val loss: 0.005875, val mae: 0.053393, val r2: 0.851895\n",
      "epoch: 346, train loss: 0.001637, val loss: 0.005822, val mae: 0.053161, val r2: 0.853081\n",
      "epoch: 347, train loss: 0.001635, val loss: 0.005935, val mae: 0.053790, val r2: 0.850419\n",
      "epoch: 348, train loss: 0.001628, val loss: 0.006149, val mae: 0.055095, val r2: 0.845128\n",
      "epoch: 349, train loss: 0.001609, val loss: 0.005974, val mae: 0.054183, val r2: 0.849560\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 32,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 2,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 6, \n",
    "                      'epoches': 350,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a8965d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps!\n",
      "Transformer(\n",
      "  (element_embedding_temporal): Linear(in_features=294, out_features=192, bias=True)\n",
      "  (element_embedding_spatial): Linear(in_features=2695, out_features=1760, bias=True)\n",
      "  (pos_embedding): Embedding(6, 32)\n",
      "  (variable_embedding): Embedding(55, 32)\n",
      "  (temporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatial): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatiotemporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (keys): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (queries): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatter): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=11220, out_features=2640, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=2640, out_features=330, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch: 0, train loss: 0.017835, val loss: 0.014964, val mae: 0.096328, val r2: 0.620287\n",
      "epoch: 1, train loss: 0.010568, val loss: 0.012563, val mae: 0.085327, val r2: 0.681452\n",
      "epoch: 2, train loss: 0.009116, val loss: 0.011295, val mae: 0.079913, val r2: 0.713612\n",
      "epoch: 3, train loss: 0.008211, val loss: 0.010433, val mae: 0.076308, val r2: 0.735503\n",
      "epoch: 4, train loss: 0.007560, val loss: 0.009797, val mae: 0.073667, val r2: 0.751852\n",
      "epoch: 5, train loss: 0.007042, val loss: 0.009322, val mae: 0.071721, val r2: 0.764071\n",
      "epoch: 6, train loss: 0.006621, val loss: 0.008932, val mae: 0.069984, val r2: 0.774268\n",
      "epoch: 7, train loss: 0.006268, val loss: 0.008672, val mae: 0.068892, val r2: 0.781055\n",
      "epoch: 8, train loss: 0.005973, val loss: 0.008356, val mae: 0.067437, val r2: 0.789287\n",
      "epoch: 9, train loss: 0.005711, val loss: 0.008197, val mae: 0.066650, val r2: 0.793541\n",
      "epoch: 10, train loss: 0.005498, val loss: 0.008011, val mae: 0.065683, val r2: 0.798426\n",
      "epoch: 11, train loss: 0.005303, val loss: 0.007836, val mae: 0.064810, val r2: 0.802899\n",
      "epoch: 12, train loss: 0.005115, val loss: 0.007684, val mae: 0.064049, val r2: 0.806877\n",
      "epoch: 13, train loss: 0.004959, val loss: 0.007545, val mae: 0.063271, val r2: 0.810541\n",
      "epoch: 14, train loss: 0.004804, val loss: 0.007436, val mae: 0.062651, val r2: 0.813507\n",
      "epoch: 15, train loss: 0.004687, val loss: 0.007299, val mae: 0.061926, val r2: 0.817054\n",
      "epoch: 16, train loss: 0.004566, val loss: 0.007245, val mae: 0.061636, val r2: 0.818453\n",
      "epoch: 17, train loss: 0.004462, val loss: 0.007156, val mae: 0.061096, val r2: 0.820861\n",
      "epoch: 18, train loss: 0.004366, val loss: 0.007068, val mae: 0.060679, val r2: 0.823120\n",
      "epoch: 19, train loss: 0.004291, val loss: 0.007031, val mae: 0.060427, val r2: 0.824228\n",
      "epoch: 20, train loss: 0.004210, val loss: 0.006948, val mae: 0.059961, val r2: 0.826226\n",
      "epoch: 21, train loss: 0.004129, val loss: 0.006921, val mae: 0.059779, val r2: 0.827143\n",
      "epoch: 22, train loss: 0.004075, val loss: 0.006903, val mae: 0.059676, val r2: 0.827632\n",
      "epoch: 23, train loss: 0.004009, val loss: 0.006831, val mae: 0.059289, val r2: 0.829480\n",
      "epoch: 24, train loss: 0.003965, val loss: 0.006877, val mae: 0.059564, val r2: 0.828514\n",
      "epoch: 25, train loss: 0.003919, val loss: 0.006847, val mae: 0.059348, val r2: 0.829277\n",
      "epoch: 26, train loss: 0.003861, val loss: 0.006768, val mae: 0.058907, val r2: 0.831246\n",
      "epoch: 27, train loss: 0.003823, val loss: 0.006680, val mae: 0.058455, val r2: 0.833426\n",
      "epoch: 28, train loss: 0.003781, val loss: 0.006714, val mae: 0.058642, val r2: 0.832615\n",
      "epoch: 29, train loss: 0.003748, val loss: 0.006748, val mae: 0.058785, val r2: 0.831852\n",
      "epoch: 30, train loss: 0.003714, val loss: 0.006716, val mae: 0.058619, val r2: 0.832728\n",
      "epoch: 31, train loss: 0.003674, val loss: 0.006689, val mae: 0.058480, val r2: 0.833407\n",
      "epoch: 32, train loss: 0.003642, val loss: 0.006656, val mae: 0.058342, val r2: 0.834292\n",
      "epoch: 33, train loss: 0.003602, val loss: 0.006660, val mae: 0.058327, val r2: 0.834102\n",
      "epoch: 34, train loss: 0.003575, val loss: 0.006686, val mae: 0.058452, val r2: 0.833561\n",
      "epoch: 35, train loss: 0.003549, val loss: 0.006766, val mae: 0.058836, val r2: 0.831720\n",
      "epoch: 36, train loss: 0.003517, val loss: 0.006656, val mae: 0.058329, val r2: 0.834292\n",
      "epoch: 37, train loss: 0.003491, val loss: 0.006617, val mae: 0.058117, val r2: 0.835282\n",
      "epoch: 38, train loss: 0.003459, val loss: 0.006619, val mae: 0.058093, val r2: 0.835339\n",
      "epoch: 39, train loss: 0.003440, val loss: 0.006694, val mae: 0.058476, val r2: 0.833538\n",
      "epoch: 40, train loss: 0.003415, val loss: 0.006721, val mae: 0.058631, val r2: 0.832764\n",
      "epoch: 41, train loss: 0.003397, val loss: 0.006769, val mae: 0.058868, val r2: 0.831705\n",
      "epoch: 42, train loss: 0.003369, val loss: 0.006737, val mae: 0.058705, val r2: 0.832459\n",
      "epoch: 43, train loss: 0.003345, val loss: 0.006796, val mae: 0.059019, val r2: 0.831020\n",
      "epoch: 44, train loss: 0.003338, val loss: 0.006855, val mae: 0.059344, val r2: 0.829508\n",
      "epoch: 45, train loss: 0.003321, val loss: 0.007004, val mae: 0.060061, val r2: 0.825939\n",
      "epoch: 46, train loss: 0.003301, val loss: 0.007072, val mae: 0.060395, val r2: 0.824290\n",
      "epoch: 47, train loss: 0.003281, val loss: 0.007022, val mae: 0.060083, val r2: 0.825626\n",
      "epoch: 48, train loss: 0.003290, val loss: 0.007388, val mae: 0.061960, val r2: 0.816493\n",
      "epoch: 49, train loss: 0.003277, val loss: 0.007351, val mae: 0.061682, val r2: 0.817504\n",
      "epoch: 50, train loss: 0.003265, val loss: 0.007404, val mae: 0.061953, val r2: 0.816204\n",
      "epoch: 51, train loss: 0.003279, val loss: 0.007595, val mae: 0.062858, val r2: 0.811534\n",
      "epoch: 52, train loss: 0.003281, val loss: 0.007566, val mae: 0.062679, val r2: 0.812194\n",
      "epoch: 53, train loss: 0.003333, val loss: 0.007540, val mae: 0.062592, val r2: 0.812794\n",
      "epoch: 54, train loss: 0.003375, val loss: 0.007441, val mae: 0.062110, val r2: 0.815220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55, train loss: 0.003478, val loss: 0.006792, val mae: 0.058897, val r2: 0.831202\n",
      "epoch: 56, train loss: 0.003655, val loss: 0.005832, val mae: 0.053878, val r2: 0.854477\n",
      "epoch: 57, train loss: 0.003776, val loss: 0.005739, val mae: 0.052717, val r2: 0.856317\n",
      "epoch: 58, train loss: 0.003831, val loss: 0.006477, val mae: 0.056002, val r2: 0.837710\n",
      "epoch: 59, train loss: 0.003604, val loss: 0.006296, val mae: 0.055058, val r2: 0.842289\n",
      "epoch: 60, train loss: 0.003390, val loss: 0.005743, val mae: 0.052461, val r2: 0.856039\n",
      "epoch: 61, train loss: 0.003370, val loss: 0.005597, val mae: 0.051868, val r2: 0.859634\n",
      "epoch: 62, train loss: 0.003444, val loss: 0.005597, val mae: 0.052023, val r2: 0.859663\n",
      "epoch: 63, train loss: 0.003482, val loss: 0.005673, val mae: 0.052595, val r2: 0.857895\n",
      "epoch: 64, train loss: 0.003444, val loss: 0.005750, val mae: 0.053069, val r2: 0.856162\n",
      "epoch: 65, train loss: 0.003331, val loss: 0.005749, val mae: 0.052998, val r2: 0.856400\n",
      "epoch: 66, train loss: 0.003222, val loss: 0.005674, val mae: 0.052561, val r2: 0.858210\n",
      "epoch: 67, train loss: 0.003121, val loss: 0.005646, val mae: 0.052302, val r2: 0.858806\n",
      "epoch: 68, train loss: 0.003066, val loss: 0.005613, val mae: 0.052034, val r2: 0.859497\n",
      "epoch: 69, train loss: 0.003037, val loss: 0.005609, val mae: 0.051964, val r2: 0.859604\n",
      "epoch: 70, train loss: 0.003027, val loss: 0.005607, val mae: 0.051940, val r2: 0.859639\n",
      "epoch: 71, train loss: 0.003026, val loss: 0.005582, val mae: 0.051750, val r2: 0.860172\n",
      "epoch: 72, train loss: 0.003031, val loss: 0.005561, val mae: 0.051661, val r2: 0.860705\n",
      "epoch: 73, train loss: 0.003023, val loss: 0.005588, val mae: 0.051724, val r2: 0.859941\n",
      "epoch: 74, train loss: 0.003012, val loss: 0.005591, val mae: 0.051740, val r2: 0.859798\n",
      "epoch: 75, train loss: 0.002995, val loss: 0.005643, val mae: 0.051931, val r2: 0.858380\n",
      "epoch: 76, train loss: 0.002978, val loss: 0.005643, val mae: 0.051911, val r2: 0.858376\n",
      "epoch: 77, train loss: 0.002951, val loss: 0.005657, val mae: 0.051931, val r2: 0.858008\n",
      "epoch: 78, train loss: 0.002933, val loss: 0.005718, val mae: 0.052198, val r2: 0.856436\n",
      "epoch: 79, train loss: 0.002909, val loss: 0.005708, val mae: 0.052185, val r2: 0.856595\n",
      "epoch: 80, train loss: 0.002900, val loss: 0.005757, val mae: 0.052359, val r2: 0.855341\n",
      "epoch: 81, train loss: 0.002897, val loss: 0.005753, val mae: 0.052331, val r2: 0.855397\n",
      "epoch: 82, train loss: 0.002900, val loss: 0.005770, val mae: 0.052433, val r2: 0.854957\n",
      "epoch: 83, train loss: 0.002902, val loss: 0.005796, val mae: 0.052529, val r2: 0.854273\n",
      "epoch: 84, train loss: 0.002910, val loss: 0.005780, val mae: 0.052494, val r2: 0.854642\n",
      "epoch: 85, train loss: 0.002934, val loss: 0.005787, val mae: 0.052434, val r2: 0.854425\n",
      "epoch: 86, train loss: 0.002946, val loss: 0.005699, val mae: 0.052062, val r2: 0.856774\n",
      "epoch: 87, train loss: 0.002960, val loss: 0.005619, val mae: 0.051788, val r2: 0.858832\n",
      "epoch: 88, train loss: 0.002972, val loss: 0.005574, val mae: 0.051630, val r2: 0.860114\n",
      "epoch: 89, train loss: 0.002967, val loss: 0.005547, val mae: 0.051651, val r2: 0.860956\n",
      "epoch: 90, train loss: 0.002952, val loss: 0.005609, val mae: 0.052098, val r2: 0.859740\n",
      "epoch: 91, train loss: 0.002920, val loss: 0.005670, val mae: 0.052504, val r2: 0.858469\n",
      "epoch: 92, train loss: 0.002922, val loss: 0.005651, val mae: 0.052382, val r2: 0.859062\n",
      "epoch: 93, train loss: 0.003007, val loss: 0.005576, val mae: 0.051940, val r2: 0.860786\n",
      "epoch: 94, train loss: 0.003120, val loss: 0.005512, val mae: 0.051372, val r2: 0.861886\n",
      "epoch: 95, train loss: 0.003197, val loss: 0.005720, val mae: 0.051982, val r2: 0.856216\n",
      "epoch: 96, train loss: 0.003173, val loss: 0.005986, val mae: 0.053150, val r2: 0.849469\n",
      "epoch: 97, train loss: 0.003042, val loss: 0.005754, val mae: 0.052221, val r2: 0.855301\n",
      "epoch: 98, train loss: 0.002953, val loss: 0.005634, val mae: 0.051846, val r2: 0.858260\n",
      "epoch: 99, train loss: 0.002942, val loss: 0.005645, val mae: 0.051999, val r2: 0.858009\n",
      "epoch: 100, train loss: 0.002931, val loss: 0.005650, val mae: 0.052116, val r2: 0.858038\n",
      "epoch: 101, train loss: 0.002915, val loss: 0.005663, val mae: 0.052208, val r2: 0.857664\n",
      "epoch: 102, train loss: 0.002903, val loss: 0.005639, val mae: 0.052063, val r2: 0.858271\n",
      "epoch: 103, train loss: 0.002907, val loss: 0.005646, val mae: 0.052095, val r2: 0.858144\n",
      "epoch: 104, train loss: 0.002915, val loss: 0.005658, val mae: 0.052170, val r2: 0.857741\n",
      "epoch: 105, train loss: 0.002928, val loss: 0.005663, val mae: 0.052233, val r2: 0.857569\n",
      "epoch: 106, train loss: 0.002926, val loss: 0.005726, val mae: 0.052617, val r2: 0.855919\n",
      "epoch: 107, train loss: 0.002924, val loss: 0.005778, val mae: 0.052977, val r2: 0.854603\n",
      "epoch: 108, train loss: 0.002921, val loss: 0.005784, val mae: 0.053019, val r2: 0.854376\n",
      "epoch: 109, train loss: 0.002923, val loss: 0.005824, val mae: 0.053287, val r2: 0.853392\n",
      "epoch: 110, train loss: 0.002926, val loss: 0.005910, val mae: 0.053826, val r2: 0.851081\n",
      "epoch: 111, train loss: 0.002916, val loss: 0.006022, val mae: 0.054485, val r2: 0.848129\n",
      "epoch: 112, train loss: 0.002910, val loss: 0.006096, val mae: 0.054895, val r2: 0.846257\n",
      "epoch: 113, train loss: 0.002906, val loss: 0.006211, val mae: 0.055581, val r2: 0.843223\n",
      "epoch: 114, train loss: 0.002900, val loss: 0.006409, val mae: 0.056692, val r2: 0.837865\n",
      "epoch: 115, train loss: 0.002917, val loss: 0.006606, val mae: 0.057774, val r2: 0.832593\n",
      "epoch: 116, train loss: 0.002939, val loss: 0.006953, val mae: 0.059594, val r2: 0.823414\n",
      "epoch: 117, train loss: 0.002961, val loss: 0.007403, val mae: 0.061895, val r2: 0.811395\n",
      "epoch: 118, train loss: 0.002988, val loss: 0.007805, val mae: 0.063772, val r2: 0.800597\n",
      "epoch: 119, train loss: 0.003004, val loss: 0.008035, val mae: 0.064889, val r2: 0.794381\n",
      "epoch: 120, train loss: 0.003047, val loss: 0.007774, val mae: 0.063752, val r2: 0.800581\n",
      "epoch: 121, train loss: 0.003071, val loss: 0.007842, val mae: 0.064092, val r2: 0.798804\n",
      "epoch: 122, train loss: 0.003083, val loss: 0.007358, val mae: 0.061774, val r2: 0.811459\n",
      "epoch: 123, train loss: 0.003068, val loss: 0.007001, val mae: 0.059994, val r2: 0.820840\n",
      "epoch: 124, train loss: 0.003021, val loss: 0.006613, val mae: 0.058005, val r2: 0.831091\n",
      "epoch: 125, train loss: 0.002966, val loss: 0.006317, val mae: 0.056496, val r2: 0.839115\n",
      "epoch: 126, train loss: 0.002896, val loss: 0.005973, val mae: 0.054644, val r2: 0.848183\n",
      "epoch: 127, train loss: 0.002827, val loss: 0.005728, val mae: 0.053374, val r2: 0.854842\n",
      "epoch: 128, train loss: 0.002769, val loss: 0.005630, val mae: 0.052838, val r2: 0.857616\n",
      "epoch: 129, train loss: 0.002723, val loss: 0.005464, val mae: 0.051923, val r2: 0.862108\n",
      "epoch: 130, train loss: 0.002692, val loss: 0.005310, val mae: 0.051043, val r2: 0.866318\n",
      "epoch: 131, train loss: 0.002671, val loss: 0.005251, val mae: 0.050700, val r2: 0.867966\n",
      "epoch: 132, train loss: 0.002655, val loss: 0.005222, val mae: 0.050541, val r2: 0.868785\n",
      "epoch: 133, train loss: 0.002634, val loss: 0.005166, val mae: 0.050173, val r2: 0.870266\n",
      "epoch: 134, train loss: 0.002616, val loss: 0.005112, val mae: 0.049815, val r2: 0.871692\n",
      "epoch: 135, train loss: 0.002597, val loss: 0.005087, val mae: 0.049655, val r2: 0.872426\n",
      "epoch: 136, train loss: 0.002585, val loss: 0.005083, val mae: 0.049647, val r2: 0.872485\n",
      "epoch: 137, train loss: 0.002569, val loss: 0.005071, val mae: 0.049553, val r2: 0.872864\n",
      "epoch: 138, train loss: 0.002544, val loss: 0.005055, val mae: 0.049475, val r2: 0.873269\n",
      "epoch: 139, train loss: 0.002530, val loss: 0.005071, val mae: 0.049622, val r2: 0.872862\n",
      "epoch: 140, train loss: 0.002520, val loss: 0.005065, val mae: 0.049564, val r2: 0.872992\n",
      "epoch: 141, train loss: 0.002502, val loss: 0.005059, val mae: 0.049505, val r2: 0.873138\n",
      "epoch: 142, train loss: 0.002479, val loss: 0.005052, val mae: 0.049442, val r2: 0.873393\n",
      "epoch: 143, train loss: 0.002468, val loss: 0.005043, val mae: 0.049486, val r2: 0.873546\n",
      "epoch: 144, train loss: 0.002451, val loss: 0.005057, val mae: 0.049529, val r2: 0.873256\n",
      "epoch: 145, train loss: 0.002440, val loss: 0.005042, val mae: 0.049402, val r2: 0.873524\n",
      "epoch: 146, train loss: 0.002423, val loss: 0.005069, val mae: 0.049613, val r2: 0.872890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 147, train loss: 0.002406, val loss: 0.005107, val mae: 0.049867, val r2: 0.871938\n",
      "epoch: 148, train loss: 0.002393, val loss: 0.005063, val mae: 0.049632, val r2: 0.872989\n",
      "epoch: 149, train loss: 0.002384, val loss: 0.005095, val mae: 0.049842, val r2: 0.872092\n",
      "epoch: 150, train loss: 0.002368, val loss: 0.005130, val mae: 0.049940, val r2: 0.871299\n",
      "epoch: 151, train loss: 0.002362, val loss: 0.005155, val mae: 0.050162, val r2: 0.870565\n",
      "epoch: 152, train loss: 0.002354, val loss: 0.005166, val mae: 0.050207, val r2: 0.870299\n",
      "epoch: 153, train loss: 0.002347, val loss: 0.005169, val mae: 0.050226, val r2: 0.870111\n",
      "epoch: 154, train loss: 0.002340, val loss: 0.005211, val mae: 0.050480, val r2: 0.868946\n",
      "epoch: 155, train loss: 0.002333, val loss: 0.005296, val mae: 0.050960, val r2: 0.866672\n",
      "epoch: 156, train loss: 0.002329, val loss: 0.005288, val mae: 0.050868, val r2: 0.866845\n",
      "epoch: 157, train loss: 0.002329, val loss: 0.005410, val mae: 0.051626, val r2: 0.863582\n",
      "epoch: 158, train loss: 0.002328, val loss: 0.005525, val mae: 0.052209, val r2: 0.860652\n",
      "epoch: 159, train loss: 0.002338, val loss: 0.005676, val mae: 0.053121, val r2: 0.856575\n",
      "epoch: 160, train loss: 0.002332, val loss: 0.005660, val mae: 0.053030, val r2: 0.856782\n",
      "epoch: 161, train loss: 0.002353, val loss: 0.005762, val mae: 0.053609, val r2: 0.854029\n",
      "epoch: 162, train loss: 0.002359, val loss: 0.005999, val mae: 0.054887, val r2: 0.847636\n",
      "epoch: 163, train loss: 0.002383, val loss: 0.006040, val mae: 0.055022, val r2: 0.846518\n",
      "epoch: 164, train loss: 0.002401, val loss: 0.006249, val mae: 0.056046, val r2: 0.841050\n",
      "epoch: 165, train loss: 0.002429, val loss: 0.006617, val mae: 0.057811, val r2: 0.831377\n",
      "epoch: 166, train loss: 0.002464, val loss: 0.006931, val mae: 0.059332, val r2: 0.823001\n",
      "epoch: 167, train loss: 0.002510, val loss: 0.007539, val mae: 0.061932, val r2: 0.807631\n",
      "epoch: 168, train loss: 0.002575, val loss: 0.008049, val mae: 0.064323, val r2: 0.794098\n",
      "epoch: 169, train loss: 0.002663, val loss: 0.008505, val mae: 0.066334, val r2: 0.781648\n",
      "epoch: 170, train loss: 0.002751, val loss: 0.008276, val mae: 0.065634, val r2: 0.786650\n",
      "epoch: 171, train loss: 0.002762, val loss: 0.007731, val mae: 0.063496, val r2: 0.800886\n",
      "epoch: 172, train loss: 0.002734, val loss: 0.007014, val mae: 0.060239, val r2: 0.819820\n",
      "epoch: 173, train loss: 0.002645, val loss: 0.005957, val mae: 0.054938, val r2: 0.848324\n",
      "epoch: 174, train loss: 0.002623, val loss: 0.005531, val mae: 0.052531, val r2: 0.859792\n",
      "epoch: 175, train loss: 0.002539, val loss: 0.005285, val mae: 0.050998, val r2: 0.866620\n",
      "epoch: 176, train loss: 0.002486, val loss: 0.005128, val mae: 0.049936, val r2: 0.870992\n",
      "epoch: 177, train loss: 0.002445, val loss: 0.005049, val mae: 0.049453, val r2: 0.873162\n",
      "epoch: 178, train loss: 0.002438, val loss: 0.005040, val mae: 0.049327, val r2: 0.873434\n",
      "epoch: 179, train loss: 0.002418, val loss: 0.005015, val mae: 0.049082, val r2: 0.874137\n",
      "epoch: 180, train loss: 0.002420, val loss: 0.005025, val mae: 0.049149, val r2: 0.873811\n",
      "epoch: 181, train loss: 0.002413, val loss: 0.005043, val mae: 0.049188, val r2: 0.873405\n",
      "epoch: 182, train loss: 0.002399, val loss: 0.005062, val mae: 0.049381, val r2: 0.872925\n",
      "epoch: 183, train loss: 0.002382, val loss: 0.005062, val mae: 0.049350, val r2: 0.873010\n",
      "epoch: 184, train loss: 0.002364, val loss: 0.005038, val mae: 0.049263, val r2: 0.873613\n",
      "epoch: 185, train loss: 0.002344, val loss: 0.005040, val mae: 0.049267, val r2: 0.873610\n",
      "epoch: 186, train loss: 0.002321, val loss: 0.005070, val mae: 0.049464, val r2: 0.872847\n",
      "epoch: 187, train loss: 0.002298, val loss: 0.005069, val mae: 0.049507, val r2: 0.872878\n",
      "epoch: 188, train loss: 0.002266, val loss: 0.005057, val mae: 0.049394, val r2: 0.873208\n",
      "epoch: 189, train loss: 0.002245, val loss: 0.005056, val mae: 0.049420, val r2: 0.873175\n",
      "epoch: 190, train loss: 0.002215, val loss: 0.005082, val mae: 0.049632, val r2: 0.872493\n",
      "epoch: 191, train loss: 0.002192, val loss: 0.005149, val mae: 0.050028, val r2: 0.870797\n",
      "epoch: 192, train loss: 0.002172, val loss: 0.005197, val mae: 0.050381, val r2: 0.869455\n",
      "epoch: 193, train loss: 0.002156, val loss: 0.005274, val mae: 0.050857, val r2: 0.867481\n",
      "epoch: 194, train loss: 0.002147, val loss: 0.005378, val mae: 0.051469, val r2: 0.864646\n",
      "epoch: 195, train loss: 0.002147, val loss: 0.005549, val mae: 0.052426, val r2: 0.860122\n",
      "epoch: 196, train loss: 0.002146, val loss: 0.005754, val mae: 0.053591, val r2: 0.854628\n",
      "epoch: 197, train loss: 0.002158, val loss: 0.006038, val mae: 0.055128, val r2: 0.846981\n",
      "epoch: 198, train loss: 0.002165, val loss: 0.006146, val mae: 0.055675, val r2: 0.843964\n",
      "epoch: 199, train loss: 0.002176, val loss: 0.006230, val mae: 0.056113, val r2: 0.841481\n",
      "epoch: 200, train loss: 0.002194, val loss: 0.006191, val mae: 0.055928, val r2: 0.842320\n",
      "epoch: 201, train loss: 0.002206, val loss: 0.006116, val mae: 0.055516, val r2: 0.844276\n",
      "epoch: 202, train loss: 0.002212, val loss: 0.006193, val mae: 0.055884, val r2: 0.842014\n",
      "epoch: 203, train loss: 0.002212, val loss: 0.006224, val mae: 0.056021, val r2: 0.841224\n",
      "epoch: 204, train loss: 0.002219, val loss: 0.006349, val mae: 0.056659, val r2: 0.837953\n",
      "epoch: 205, train loss: 0.002217, val loss: 0.006593, val mae: 0.057825, val r2: 0.831839\n",
      "epoch: 206, train loss: 0.002204, val loss: 0.006374, val mae: 0.056845, val r2: 0.837780\n",
      "epoch: 207, train loss: 0.002210, val loss: 0.005844, val mae: 0.054125, val r2: 0.851812\n",
      "epoch: 208, train loss: 0.002202, val loss: 0.005458, val mae: 0.052030, val r2: 0.862278\n",
      "epoch: 209, train loss: 0.002187, val loss: 0.005148, val mae: 0.050161, val r2: 0.870462\n",
      "epoch: 210, train loss: 0.002177, val loss: 0.005096, val mae: 0.049630, val r2: 0.871989\n",
      "epoch: 211, train loss: 0.002192, val loss: 0.005088, val mae: 0.049411, val r2: 0.872234\n",
      "epoch: 212, train loss: 0.002209, val loss: 0.005126, val mae: 0.049496, val r2: 0.871234\n",
      "epoch: 213, train loss: 0.002199, val loss: 0.005104, val mae: 0.049440, val r2: 0.871870\n",
      "epoch: 214, train loss: 0.002191, val loss: 0.005109, val mae: 0.049399, val r2: 0.871738\n",
      "epoch: 215, train loss: 0.002175, val loss: 0.005121, val mae: 0.049477, val r2: 0.871444\n",
      "epoch: 216, train loss: 0.002174, val loss: 0.005163, val mae: 0.049633, val r2: 0.870408\n",
      "epoch: 217, train loss: 0.002165, val loss: 0.005180, val mae: 0.049780, val r2: 0.869944\n",
      "epoch: 218, train loss: 0.002156, val loss: 0.005211, val mae: 0.049907, val r2: 0.869129\n",
      "epoch: 219, train loss: 0.002147, val loss: 0.005223, val mae: 0.050020, val r2: 0.868872\n",
      "epoch: 220, train loss: 0.002133, val loss: 0.005195, val mae: 0.049998, val r2: 0.869609\n",
      "epoch: 221, train loss: 0.002105, val loss: 0.005194, val mae: 0.049956, val r2: 0.869670\n",
      "epoch: 222, train loss: 0.002084, val loss: 0.005210, val mae: 0.050191, val r2: 0.869222\n",
      "epoch: 223, train loss: 0.002064, val loss: 0.005228, val mae: 0.050354, val r2: 0.868854\n",
      "epoch: 224, train loss: 0.002052, val loss: 0.005293, val mae: 0.050804, val r2: 0.867136\n",
      "epoch: 225, train loss: 0.002043, val loss: 0.005330, val mae: 0.051111, val r2: 0.866127\n",
      "epoch: 226, train loss: 0.002032, val loss: 0.005388, val mae: 0.051482, val r2: 0.864421\n",
      "epoch: 227, train loss: 0.002029, val loss: 0.005454, val mae: 0.051876, val r2: 0.862515\n",
      "epoch: 228, train loss: 0.002030, val loss: 0.005522, val mae: 0.052271, val r2: 0.860494\n",
      "epoch: 229, train loss: 0.002023, val loss: 0.005553, val mae: 0.052478, val r2: 0.859522\n",
      "epoch: 230, train loss: 0.002027, val loss: 0.005617, val mae: 0.052841, val r2: 0.857712\n",
      "epoch: 231, train loss: 0.002040, val loss: 0.005771, val mae: 0.053620, val r2: 0.853486\n",
      "epoch: 232, train loss: 0.002057, val loss: 0.006076, val mae: 0.055147, val r2: 0.845270\n",
      "epoch: 233, train loss: 0.002060, val loss: 0.006275, val mae: 0.056179, val r2: 0.840021\n",
      "epoch: 234, train loss: 0.002067, val loss: 0.006533, val mae: 0.057526, val r2: 0.833242\n",
      "epoch: 235, train loss: 0.002085, val loss: 0.006281, val mae: 0.056209, val r2: 0.840338\n",
      "epoch: 236, train loss: 0.002112, val loss: 0.005664, val mae: 0.052988, val r2: 0.856906\n",
      "epoch: 237, train loss: 0.002114, val loss: 0.005212, val mae: 0.050416, val r2: 0.869042\n",
      "epoch: 238, train loss: 0.002083, val loss: 0.005144, val mae: 0.049602, val r2: 0.871115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 239, train loss: 0.002048, val loss: 0.005244, val mae: 0.049841, val r2: 0.868545\n",
      "epoch: 240, train loss: 0.002044, val loss: 0.005264, val mae: 0.049860, val r2: 0.867801\n",
      "epoch: 241, train loss: 0.002042, val loss: 0.005240, val mae: 0.049837, val r2: 0.868380\n",
      "epoch: 242, train loss: 0.002019, val loss: 0.005191, val mae: 0.049774, val r2: 0.869717\n",
      "epoch: 243, train loss: 0.001986, val loss: 0.005157, val mae: 0.049779, val r2: 0.870615\n",
      "epoch: 244, train loss: 0.001959, val loss: 0.005129, val mae: 0.049788, val r2: 0.871340\n",
      "epoch: 245, train loss: 0.001941, val loss: 0.005139, val mae: 0.049860, val r2: 0.871137\n",
      "epoch: 246, train loss: 0.001926, val loss: 0.005146, val mae: 0.049871, val r2: 0.870790\n",
      "epoch: 247, train loss: 0.001921, val loss: 0.005146, val mae: 0.049889, val r2: 0.870857\n",
      "epoch: 248, train loss: 0.001907, val loss: 0.005203, val mae: 0.050281, val r2: 0.869291\n",
      "epoch: 249, train loss: 0.001896, val loss: 0.005241, val mae: 0.050494, val r2: 0.868229\n",
      "epoch: 250, train loss: 0.001896, val loss: 0.005351, val mae: 0.051206, val r2: 0.865168\n",
      "epoch: 251, train loss: 0.001888, val loss: 0.005410, val mae: 0.051636, val r2: 0.863370\n",
      "epoch: 252, train loss: 0.001892, val loss: 0.005637, val mae: 0.052903, val r2: 0.857400\n",
      "epoch: 253, train loss: 0.001893, val loss: 0.005784, val mae: 0.053799, val r2: 0.853353\n",
      "epoch: 254, train loss: 0.001901, val loss: 0.005876, val mae: 0.054298, val r2: 0.850946\n",
      "epoch: 255, train loss: 0.001916, val loss: 0.005681, val mae: 0.053220, val r2: 0.856200\n",
      "epoch: 256, train loss: 0.001925, val loss: 0.005344, val mae: 0.051376, val r2: 0.865250\n",
      "epoch: 257, train loss: 0.001931, val loss: 0.005214, val mae: 0.050464, val r2: 0.869041\n",
      "epoch: 258, train loss: 0.001917, val loss: 0.005152, val mae: 0.049766, val r2: 0.870861\n",
      "epoch: 259, train loss: 0.001902, val loss: 0.005332, val mae: 0.050340, val r2: 0.866463\n",
      "epoch: 260, train loss: 0.001893, val loss: 0.005418, val mae: 0.050575, val r2: 0.864071\n",
      "epoch: 261, train loss: 0.001899, val loss: 0.005372, val mae: 0.050334, val r2: 0.865181\n",
      "epoch: 262, train loss: 0.001898, val loss: 0.005238, val mae: 0.049866, val r2: 0.868469\n",
      "epoch: 263, train loss: 0.001884, val loss: 0.005202, val mae: 0.049851, val r2: 0.869416\n",
      "epoch: 264, train loss: 0.001863, val loss: 0.005208, val mae: 0.050092, val r2: 0.869256\n",
      "epoch: 265, train loss: 0.001849, val loss: 0.005244, val mae: 0.050550, val r2: 0.868276\n",
      "epoch: 266, train loss: 0.001839, val loss: 0.005227, val mae: 0.050494, val r2: 0.868717\n",
      "epoch: 267, train loss: 0.001834, val loss: 0.005204, val mae: 0.050340, val r2: 0.869267\n",
      "epoch: 268, train loss: 0.001832, val loss: 0.005174, val mae: 0.050136, val r2: 0.870092\n",
      "epoch: 269, train loss: 0.001818, val loss: 0.005166, val mae: 0.050016, val r2: 0.870278\n",
      "epoch: 270, train loss: 0.001811, val loss: 0.005219, val mae: 0.050270, val r2: 0.868936\n",
      "epoch: 271, train loss: 0.001810, val loss: 0.005265, val mae: 0.050490, val r2: 0.867634\n",
      "epoch: 272, train loss: 0.001807, val loss: 0.005342, val mae: 0.050957, val r2: 0.865499\n",
      "epoch: 273, train loss: 0.001801, val loss: 0.005449, val mae: 0.051551, val r2: 0.862649\n",
      "epoch: 274, train loss: 0.001798, val loss: 0.005479, val mae: 0.051758, val r2: 0.861853\n",
      "epoch: 275, train loss: 0.001805, val loss: 0.005487, val mae: 0.051951, val r2: 0.861642\n",
      "epoch: 276, train loss: 0.001827, val loss: 0.005393, val mae: 0.051530, val r2: 0.864209\n",
      "epoch: 277, train loss: 0.001834, val loss: 0.005293, val mae: 0.051186, val r2: 0.866926\n",
      "epoch: 278, train loss: 0.001814, val loss: 0.005201, val mae: 0.050373, val r2: 0.869514\n",
      "epoch: 279, train loss: 0.001796, val loss: 0.005200, val mae: 0.050073, val r2: 0.869703\n",
      "epoch: 280, train loss: 0.001784, val loss: 0.005298, val mae: 0.050486, val r2: 0.867124\n",
      "epoch: 281, train loss: 0.001775, val loss: 0.005366, val mae: 0.050800, val r2: 0.865247\n",
      "epoch: 282, train loss: 0.001765, val loss: 0.005349, val mae: 0.050730, val r2: 0.865615\n",
      "epoch: 283, train loss: 0.001754, val loss: 0.005318, val mae: 0.050644, val r2: 0.866392\n",
      "epoch: 284, train loss: 0.001745, val loss: 0.005307, val mae: 0.050559, val r2: 0.866711\n",
      "epoch: 285, train loss: 0.001740, val loss: 0.005277, val mae: 0.050529, val r2: 0.867453\n",
      "epoch: 286, train loss: 0.001731, val loss: 0.005299, val mae: 0.050805, val r2: 0.866939\n",
      "epoch: 287, train loss: 0.001726, val loss: 0.005252, val mae: 0.050622, val r2: 0.868120\n",
      "epoch: 288, train loss: 0.001730, val loss: 0.005265, val mae: 0.050742, val r2: 0.867630\n",
      "epoch: 289, train loss: 0.001725, val loss: 0.005239, val mae: 0.050457, val r2: 0.868375\n",
      "epoch: 290, train loss: 0.001724, val loss: 0.005234, val mae: 0.050405, val r2: 0.868562\n",
      "epoch: 291, train loss: 0.001716, val loss: 0.005275, val mae: 0.050502, val r2: 0.867546\n",
      "epoch: 292, train loss: 0.001710, val loss: 0.005322, val mae: 0.050660, val r2: 0.866342\n",
      "epoch: 293, train loss: 0.001714, val loss: 0.005345, val mae: 0.050822, val r2: 0.865618\n",
      "epoch: 294, train loss: 0.001728, val loss: 0.005390, val mae: 0.051157, val r2: 0.864334\n",
      "epoch: 295, train loss: 0.001719, val loss: 0.005477, val mae: 0.051644, val r2: 0.862001\n",
      "epoch: 296, train loss: 0.001705, val loss: 0.005593, val mae: 0.052533, val r2: 0.858882\n",
      "epoch: 297, train loss: 0.001706, val loss: 0.005670, val mae: 0.052876, val r2: 0.856946\n",
      "epoch: 298, train loss: 0.001704, val loss: 0.005527, val mae: 0.052222, val r2: 0.860759\n",
      "epoch: 299, train loss: 0.001715, val loss: 0.005439, val mae: 0.051824, val r2: 0.863031\n",
      "epoch: 300, train loss: 0.001712, val loss: 0.005226, val mae: 0.050543, val r2: 0.868752\n",
      "epoch: 301, train loss: 0.001701, val loss: 0.005169, val mae: 0.050068, val r2: 0.870407\n",
      "epoch: 302, train loss: 0.001687, val loss: 0.005203, val mae: 0.050129, val r2: 0.869636\n",
      "epoch: 303, train loss: 0.001693, val loss: 0.005244, val mae: 0.050150, val r2: 0.868501\n",
      "epoch: 304, train loss: 0.001698, val loss: 0.005274, val mae: 0.050359, val r2: 0.867731\n",
      "epoch: 305, train loss: 0.001692, val loss: 0.005331, val mae: 0.050847, val r2: 0.866202\n",
      "epoch: 306, train loss: 0.001684, val loss: 0.005371, val mae: 0.051167, val r2: 0.865209\n",
      "epoch: 307, train loss: 0.001681, val loss: 0.005409, val mae: 0.051443, val r2: 0.864269\n",
      "epoch: 308, train loss: 0.001687, val loss: 0.005376, val mae: 0.051342, val r2: 0.865061\n",
      "epoch: 309, train loss: 0.001693, val loss: 0.005245, val mae: 0.050553, val r2: 0.868336\n",
      "epoch: 310, train loss: 0.001682, val loss: 0.005238, val mae: 0.050380, val r2: 0.868611\n",
      "epoch: 311, train loss: 0.001668, val loss: 0.005299, val mae: 0.050622, val r2: 0.867024\n",
      "epoch: 312, train loss: 0.001663, val loss: 0.005372, val mae: 0.050965, val r2: 0.865097\n",
      "epoch: 313, train loss: 0.001672, val loss: 0.005507, val mae: 0.051765, val r2: 0.861704\n",
      "epoch: 314, train loss: 0.001666, val loss: 0.005688, val mae: 0.052810, val r2: 0.856901\n",
      "epoch: 315, train loss: 0.001658, val loss: 0.005878, val mae: 0.053790, val r2: 0.852168\n",
      "epoch: 316, train loss: 0.001648, val loss: 0.005967, val mae: 0.054333, val r2: 0.849719\n",
      "epoch: 317, train loss: 0.001656, val loss: 0.005903, val mae: 0.054030, val r2: 0.851188\n",
      "epoch: 318, train loss: 0.001654, val loss: 0.005859, val mae: 0.053899, val r2: 0.852049\n",
      "epoch: 319, train loss: 0.001655, val loss: 0.005815, val mae: 0.053619, val r2: 0.853166\n",
      "epoch: 320, train loss: 0.001650, val loss: 0.005877, val mae: 0.054023, val r2: 0.851365\n",
      "epoch: 321, train loss: 0.001646, val loss: 0.006011, val mae: 0.054681, val r2: 0.847885\n",
      "epoch: 322, train loss: 0.001642, val loss: 0.006105, val mae: 0.055155, val r2: 0.845482\n",
      "epoch: 323, train loss: 0.001640, val loss: 0.006186, val mae: 0.055629, val r2: 0.843372\n",
      "epoch: 324, train loss: 0.001644, val loss: 0.006180, val mae: 0.055559, val r2: 0.843676\n",
      "epoch: 325, train loss: 0.001646, val loss: 0.006007, val mae: 0.054733, val r2: 0.848158\n",
      "epoch: 326, train loss: 0.001647, val loss: 0.005852, val mae: 0.054061, val r2: 0.852341\n",
      "epoch: 327, train loss: 0.001645, val loss: 0.005822, val mae: 0.053929, val r2: 0.852969\n",
      "epoch: 328, train loss: 0.001648, val loss: 0.005822, val mae: 0.053941, val r2: 0.853104\n",
      "epoch: 329, train loss: 0.001650, val loss: 0.005815, val mae: 0.053718, val r2: 0.853418\n",
      "epoch: 330, train loss: 0.001648, val loss: 0.005857, val mae: 0.053919, val r2: 0.852308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 331, train loss: 0.001648, val loss: 0.005874, val mae: 0.053866, val r2: 0.852038\n",
      "epoch: 332, train loss: 0.001649, val loss: 0.005912, val mae: 0.053993, val r2: 0.851162\n",
      "epoch: 333, train loss: 0.001648, val loss: 0.005936, val mae: 0.053974, val r2: 0.850794\n",
      "epoch: 334, train loss: 0.001642, val loss: 0.006032, val mae: 0.054478, val r2: 0.848490\n",
      "epoch: 335, train loss: 0.001638, val loss: 0.006051, val mae: 0.054452, val r2: 0.847981\n",
      "epoch: 336, train loss: 0.001637, val loss: 0.006030, val mae: 0.054355, val r2: 0.848604\n",
      "epoch: 337, train loss: 0.001647, val loss: 0.005949, val mae: 0.053896, val r2: 0.850628\n",
      "epoch: 338, train loss: 0.001654, val loss: 0.005772, val mae: 0.052970, val r2: 0.854867\n",
      "epoch: 339, train loss: 0.001660, val loss: 0.005874, val mae: 0.053239, val r2: 0.852210\n",
      "epoch: 340, train loss: 0.001674, val loss: 0.006141, val mae: 0.054626, val r2: 0.845340\n",
      "epoch: 341, train loss: 0.001691, val loss: 0.006843, val mae: 0.058027, val r2: 0.827424\n",
      "epoch: 342, train loss: 0.001665, val loss: 0.007392, val mae: 0.060961, val r2: 0.812845\n",
      "epoch: 343, train loss: 0.001647, val loss: 0.007168, val mae: 0.060036, val r2: 0.818220\n",
      "epoch: 344, train loss: 0.001655, val loss: 0.006414, val mae: 0.056422, val r2: 0.837527\n",
      "epoch: 345, train loss: 0.001654, val loss: 0.005884, val mae: 0.053906, val r2: 0.851365\n",
      "epoch: 346, train loss: 0.001632, val loss: 0.005687, val mae: 0.052818, val r2: 0.856715\n",
      "epoch: 347, train loss: 0.001616, val loss: 0.005694, val mae: 0.052809, val r2: 0.856638\n",
      "epoch: 348, train loss: 0.001610, val loss: 0.005705, val mae: 0.052820, val r2: 0.856533\n",
      "epoch: 349, train loss: 0.001603, val loss: 0.005717, val mae: 0.052961, val r2: 0.856367\n"
     ]
    }
   ],
   "source": [
    "# 3 layers  -\n",
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 32,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 3,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 6, \n",
    "                      'epoches': 350,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f98987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [16, 8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 32,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 3,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 6, \n",
    "                      'epoches': 250,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29a60c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps!\n",
      "Transformer(\n",
      "  (element_embedding_temporal): Linear(in_features=294, out_features=192, bias=True)\n",
      "  (element_embedding_spatial): Linear(in_features=2695, out_features=1760, bias=True)\n",
      "  (pos_embedding): Embedding(6, 32)\n",
      "  (variable_embedding): Embedding(55, 32)\n",
      "  (temporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatial): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatiotemporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (keys): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (queries): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatter): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=11220, out_features=2640, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.15, inplace=False)\n",
      "    (3): Linear(in_features=2640, out_features=330, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch: 0, train loss: 0.018713, val loss: 0.017245, val mae: 0.103505, val r2: 0.562286\n",
      "epoch: 1, train loss: 0.011189, val loss: 0.014178, val mae: 0.090344, val r2: 0.640823\n",
      "epoch: 2, train loss: 0.009621, val loss: 0.012696, val mae: 0.084639, val r2: 0.678339\n",
      "epoch: 3, train loss: 0.008719, val loss: 0.011886, val mae: 0.081615, val r2: 0.698705\n",
      "epoch: 4, train loss: 0.008085, val loss: 0.011205, val mae: 0.079129, val r2: 0.716061\n",
      "epoch: 5, train loss: 0.007572, val loss: 0.010836, val mae: 0.077744, val r2: 0.725665\n",
      "epoch: 6, train loss: 0.007142, val loss: 0.010367, val mae: 0.076009, val r2: 0.737980\n",
      "epoch: 7, train loss: 0.006805, val loss: 0.010120, val mae: 0.075047, val r2: 0.744450\n",
      "epoch: 8, train loss: 0.006509, val loss: 0.009882, val mae: 0.073966, val r2: 0.750799\n",
      "epoch: 9, train loss: 0.006228, val loss: 0.009690, val mae: 0.073109, val r2: 0.756007\n",
      "epoch: 10, train loss: 0.005985, val loss: 0.009411, val mae: 0.071873, val r2: 0.763433\n",
      "epoch: 11, train loss: 0.005761, val loss: 0.009222, val mae: 0.070992, val r2: 0.768397\n",
      "epoch: 12, train loss: 0.005547, val loss: 0.009065, val mae: 0.070278, val r2: 0.772601\n",
      "epoch: 13, train loss: 0.005369, val loss: 0.008829, val mae: 0.069103, val r2: 0.778793\n",
      "epoch: 14, train loss: 0.005211, val loss: 0.008721, val mae: 0.068558, val r2: 0.781849\n",
      "epoch: 15, train loss: 0.005074, val loss: 0.008550, val mae: 0.067764, val r2: 0.786203\n",
      "epoch: 16, train loss: 0.004935, val loss: 0.008362, val mae: 0.066873, val r2: 0.791014\n",
      "epoch: 17, train loss: 0.004826, val loss: 0.008296, val mae: 0.066481, val r2: 0.792979\n",
      "epoch: 18, train loss: 0.004703, val loss: 0.008208, val mae: 0.066048, val r2: 0.795186\n",
      "epoch: 19, train loss: 0.004623, val loss: 0.008081, val mae: 0.065381, val r2: 0.798526\n",
      "epoch: 20, train loss: 0.004533, val loss: 0.008036, val mae: 0.065170, val r2: 0.799852\n",
      "epoch: 21, train loss: 0.004456, val loss: 0.008029, val mae: 0.065005, val r2: 0.800181\n",
      "epoch: 22, train loss: 0.004382, val loss: 0.007981, val mae: 0.064741, val r2: 0.801370\n",
      "epoch: 23, train loss: 0.004325, val loss: 0.008004, val mae: 0.064853, val r2: 0.800983\n",
      "epoch: 24, train loss: 0.004263, val loss: 0.007915, val mae: 0.064442, val r2: 0.803207\n",
      "epoch: 25, train loss: 0.004210, val loss: 0.008073, val mae: 0.065104, val r2: 0.799591\n",
      "epoch: 26, train loss: 0.004166, val loss: 0.008082, val mae: 0.065129, val r2: 0.799325\n",
      "epoch: 27, train loss: 0.004137, val loss: 0.008052, val mae: 0.064975, val r2: 0.800168\n",
      "epoch: 28, train loss: 0.004090, val loss: 0.007984, val mae: 0.064575, val r2: 0.801932\n",
      "epoch: 29, train loss: 0.004040, val loss: 0.008085, val mae: 0.065024, val r2: 0.799581\n",
      "epoch: 30, train loss: 0.004022, val loss: 0.007989, val mae: 0.064652, val r2: 0.801760\n",
      "epoch: 31, train loss: 0.003996, val loss: 0.007955, val mae: 0.064469, val r2: 0.802624\n",
      "epoch: 32, train loss: 0.003969, val loss: 0.008096, val mae: 0.065103, val r2: 0.799236\n",
      "epoch: 33, train loss: 0.003945, val loss: 0.007833, val mae: 0.063930, val r2: 0.805602\n",
      "epoch: 34, train loss: 0.003928, val loss: 0.007789, val mae: 0.063724, val r2: 0.806706\n",
      "epoch: 35, train loss: 0.003918, val loss: 0.007761, val mae: 0.063552, val r2: 0.807529\n",
      "epoch: 36, train loss: 0.003915, val loss: 0.007542, val mae: 0.062553, val r2: 0.812673\n",
      "epoch: 37, train loss: 0.003914, val loss: 0.007430, val mae: 0.061990, val r2: 0.815456\n",
      "epoch: 38, train loss: 0.003913, val loss: 0.007141, val mae: 0.060614, val r2: 0.822401\n",
      "epoch: 39, train loss: 0.003920, val loss: 0.006837, val mae: 0.059127, val r2: 0.829721\n",
      "epoch: 40, train loss: 0.003905, val loss: 0.006467, val mae: 0.057251, val r2: 0.838713\n",
      "epoch: 41, train loss: 0.003883, val loss: 0.006277, val mae: 0.056225, val r2: 0.843342\n",
      "epoch: 42, train loss: 0.003848, val loss: 0.006129, val mae: 0.055322, val r2: 0.846791\n",
      "epoch: 43, train loss: 0.003783, val loss: 0.006056, val mae: 0.054994, val r2: 0.848550\n",
      "epoch: 44, train loss: 0.003717, val loss: 0.006043, val mae: 0.054847, val r2: 0.848800\n",
      "epoch: 45, train loss: 0.003656, val loss: 0.006005, val mae: 0.054545, val r2: 0.849793\n",
      "epoch: 46, train loss: 0.003603, val loss: 0.005974, val mae: 0.054379, val r2: 0.850506\n",
      "epoch: 47, train loss: 0.003550, val loss: 0.005968, val mae: 0.054345, val r2: 0.850660\n",
      "epoch: 48, train loss: 0.003517, val loss: 0.005954, val mae: 0.054234, val r2: 0.851021\n",
      "epoch: 49, train loss: 0.003512, val loss: 0.005937, val mae: 0.054106, val r2: 0.851393\n",
      "epoch: 50, train loss: 0.003482, val loss: 0.005931, val mae: 0.054035, val r2: 0.851573\n",
      "epoch: 51, train loss: 0.003481, val loss: 0.005906, val mae: 0.053844, val r2: 0.852165\n",
      "epoch: 52, train loss: 0.003471, val loss: 0.005905, val mae: 0.053787, val r2: 0.852204\n",
      "epoch: 53, train loss: 0.003457, val loss: 0.005919, val mae: 0.053809, val r2: 0.851790\n",
      "epoch: 54, train loss: 0.003466, val loss: 0.005910, val mae: 0.053793, val r2: 0.852070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55, train loss: 0.003484, val loss: 0.005921, val mae: 0.053725, val r2: 0.851825\n",
      "epoch: 56, train loss: 0.003475, val loss: 0.005903, val mae: 0.053661, val r2: 0.852297\n",
      "epoch: 57, train loss: 0.003502, val loss: 0.005928, val mae: 0.053762, val r2: 0.851639\n",
      "epoch: 58, train loss: 0.003531, val loss: 0.005922, val mae: 0.053771, val r2: 0.851856\n",
      "epoch: 59, train loss: 0.003582, val loss: 0.005959, val mae: 0.054028, val r2: 0.851093\n",
      "epoch: 60, train loss: 0.003598, val loss: 0.006131, val mae: 0.054989, val r2: 0.847123\n",
      "epoch: 61, train loss: 0.003613, val loss: 0.006341, val mae: 0.056097, val r2: 0.842288\n",
      "epoch: 62, train loss: 0.003571, val loss: 0.006684, val mae: 0.057783, val r2: 0.834211\n",
      "epoch: 63, train loss: 0.003508, val loss: 0.006800, val mae: 0.058316, val r2: 0.831553\n",
      "epoch: 64, train loss: 0.003420, val loss: 0.006785, val mae: 0.058298, val r2: 0.832123\n",
      "epoch: 65, train loss: 0.003389, val loss: 0.006574, val mae: 0.057216, val r2: 0.837220\n",
      "epoch: 66, train loss: 0.003425, val loss: 0.006494, val mae: 0.056872, val r2: 0.839057\n",
      "epoch: 67, train loss: 0.003501, val loss: 0.006214, val mae: 0.055495, val r2: 0.845642\n",
      "epoch: 68, train loss: 0.003549, val loss: 0.005837, val mae: 0.053476, val r2: 0.854456\n",
      "epoch: 69, train loss: 0.003562, val loss: 0.005726, val mae: 0.052655, val r2: 0.856846\n",
      "epoch: 70, train loss: 0.003505, val loss: 0.005739, val mae: 0.052579, val r2: 0.856157\n",
      "epoch: 71, train loss: 0.003392, val loss: 0.005718, val mae: 0.052493, val r2: 0.856822\n",
      "epoch: 72, train loss: 0.003269, val loss: 0.005699, val mae: 0.052403, val r2: 0.857331\n",
      "epoch: 73, train loss: 0.003205, val loss: 0.005688, val mae: 0.052431, val r2: 0.857656\n",
      "epoch: 74, train loss: 0.003174, val loss: 0.005728, val mae: 0.052637, val r2: 0.856935\n",
      "epoch: 75, train loss: 0.003141, val loss: 0.005736, val mae: 0.052709, val r2: 0.856681\n",
      "epoch: 76, train loss: 0.003125, val loss: 0.005744, val mae: 0.052727, val r2: 0.856439\n",
      "epoch: 77, train loss: 0.003113, val loss: 0.005752, val mae: 0.052771, val r2: 0.856340\n",
      "epoch: 78, train loss: 0.003099, val loss: 0.005764, val mae: 0.052833, val r2: 0.856076\n",
      "epoch: 79, train loss: 0.003089, val loss: 0.005764, val mae: 0.052793, val r2: 0.856080\n",
      "epoch: 80, train loss: 0.003077, val loss: 0.005738, val mae: 0.052724, val r2: 0.856727\n",
      "epoch: 81, train loss: 0.003059, val loss: 0.005730, val mae: 0.052639, val r2: 0.857032\n",
      "epoch: 82, train loss: 0.003050, val loss: 0.005776, val mae: 0.052976, val r2: 0.855944\n",
      "epoch: 83, train loss: 0.003045, val loss: 0.005758, val mae: 0.052878, val r2: 0.856478\n",
      "epoch: 84, train loss: 0.003025, val loss: 0.005766, val mae: 0.052909, val r2: 0.856303\n",
      "epoch: 85, train loss: 0.003023, val loss: 0.005780, val mae: 0.053011, val r2: 0.856001\n",
      "epoch: 86, train loss: 0.003015, val loss: 0.005755, val mae: 0.052906, val r2: 0.856605\n",
      "epoch: 87, train loss: 0.003002, val loss: 0.005764, val mae: 0.052951, val r2: 0.856417\n",
      "epoch: 88, train loss: 0.002996, val loss: 0.005780, val mae: 0.053056, val r2: 0.856064\n",
      "epoch: 89, train loss: 0.002996, val loss: 0.005783, val mae: 0.053072, val r2: 0.856042\n",
      "epoch: 90, train loss: 0.002984, val loss: 0.005730, val mae: 0.052796, val r2: 0.857302\n",
      "epoch: 91, train loss: 0.002972, val loss: 0.005723, val mae: 0.052758, val r2: 0.857423\n",
      "epoch: 92, train loss: 0.002958, val loss: 0.005674, val mae: 0.052519, val r2: 0.858540\n",
      "epoch: 93, train loss: 0.002961, val loss: 0.005719, val mae: 0.052740, val r2: 0.857526\n",
      "epoch: 94, train loss: 0.002955, val loss: 0.005685, val mae: 0.052559, val r2: 0.858216\n",
      "epoch: 95, train loss: 0.002949, val loss: 0.005694, val mae: 0.052697, val r2: 0.858107\n",
      "epoch: 96, train loss: 0.002948, val loss: 0.005715, val mae: 0.052706, val r2: 0.857579\n",
      "epoch: 97, train loss: 0.002940, val loss: 0.005667, val mae: 0.052475, val r2: 0.858589\n",
      "epoch: 98, train loss: 0.002938, val loss: 0.005655, val mae: 0.052352, val r2: 0.858767\n",
      "epoch: 99, train loss: 0.002939, val loss: 0.005602, val mae: 0.052098, val r2: 0.860092\n",
      "epoch: 100, train loss: 0.002938, val loss: 0.005642, val mae: 0.052264, val r2: 0.859054\n",
      "epoch: 101, train loss: 0.002943, val loss: 0.005588, val mae: 0.051966, val r2: 0.860208\n",
      "epoch: 102, train loss: 0.002932, val loss: 0.005610, val mae: 0.052007, val r2: 0.859596\n",
      "epoch: 103, train loss: 0.002950, val loss: 0.005639, val mae: 0.052133, val r2: 0.858871\n",
      "epoch: 104, train loss: 0.002954, val loss: 0.005571, val mae: 0.051660, val r2: 0.860301\n",
      "epoch: 105, train loss: 0.002964, val loss: 0.005688, val mae: 0.052163, val r2: 0.857091\n",
      "epoch: 106, train loss: 0.002982, val loss: 0.005656, val mae: 0.051918, val r2: 0.857972\n",
      "epoch: 107, train loss: 0.002984, val loss: 0.005747, val mae: 0.052353, val r2: 0.855511\n",
      "epoch: 108, train loss: 0.002997, val loss: 0.005835, val mae: 0.052720, val r2: 0.853140\n",
      "epoch: 109, train loss: 0.003008, val loss: 0.005926, val mae: 0.053098, val r2: 0.850776\n",
      "epoch: 110, train loss: 0.003012, val loss: 0.005904, val mae: 0.053124, val r2: 0.851442\n",
      "epoch: 111, train loss: 0.003032, val loss: 0.005987, val mae: 0.053533, val r2: 0.849356\n",
      "epoch: 112, train loss: 0.003042, val loss: 0.005992, val mae: 0.053673, val r2: 0.849450\n",
      "epoch: 113, train loss: 0.003062, val loss: 0.005985, val mae: 0.053752, val r2: 0.849831\n",
      "epoch: 114, train loss: 0.003068, val loss: 0.006169, val mae: 0.054816, val r2: 0.845515\n",
      "epoch: 115, train loss: 0.003075, val loss: 0.006179, val mae: 0.054927, val r2: 0.845228\n",
      "epoch: 116, train loss: 0.003074, val loss: 0.006252, val mae: 0.055335, val r2: 0.843437\n",
      "epoch: 117, train loss: 0.003115, val loss: 0.006223, val mae: 0.055332, val r2: 0.843894\n",
      "epoch: 118, train loss: 0.003175, val loss: 0.006435, val mae: 0.056468, val r2: 0.838594\n",
      "epoch: 119, train loss: 0.003241, val loss: 0.006695, val mae: 0.057848, val r2: 0.831733\n",
      "epoch: 120, train loss: 0.003306, val loss: 0.007117, val mae: 0.059995, val r2: 0.820754\n",
      "epoch: 121, train loss: 0.003343, val loss: 0.007432, val mae: 0.061590, val r2: 0.812255\n",
      "epoch: 122, train loss: 0.003297, val loss: 0.007521, val mae: 0.061971, val r2: 0.810024\n",
      "epoch: 123, train loss: 0.003197, val loss: 0.007358, val mae: 0.061246, val r2: 0.813973\n",
      "epoch: 124, train loss: 0.003103, val loss: 0.007307, val mae: 0.061054, val r2: 0.814910\n",
      "epoch: 125, train loss: 0.003026, val loss: 0.007157, val mae: 0.060461, val r2: 0.818697\n",
      "epoch: 126, train loss: 0.002968, val loss: 0.007180, val mae: 0.060528, val r2: 0.817794\n",
      "epoch: 127, train loss: 0.002934, val loss: 0.007174, val mae: 0.060526, val r2: 0.817789\n",
      "epoch: 128, train loss: 0.002908, val loss: 0.007172, val mae: 0.060558, val r2: 0.817546\n",
      "epoch: 129, train loss: 0.002894, val loss: 0.007290, val mae: 0.061069, val r2: 0.814489\n",
      "epoch: 130, train loss: 0.002876, val loss: 0.007352, val mae: 0.061400, val r2: 0.812523\n",
      "epoch: 131, train loss: 0.002873, val loss: 0.007367, val mae: 0.061474, val r2: 0.812209\n",
      "epoch: 132, train loss: 0.002862, val loss: 0.007296, val mae: 0.061135, val r2: 0.813962\n",
      "epoch: 133, train loss: 0.002846, val loss: 0.007204, val mae: 0.060732, val r2: 0.816143\n",
      "epoch: 134, train loss: 0.002836, val loss: 0.007341, val mae: 0.061313, val r2: 0.812332\n",
      "epoch: 135, train loss: 0.002828, val loss: 0.007410, val mae: 0.061647, val r2: 0.810381\n",
      "epoch: 136, train loss: 0.002815, val loss: 0.007523, val mae: 0.062169, val r2: 0.807362\n",
      "epoch: 137, train loss: 0.002794, val loss: 0.007412, val mae: 0.061621, val r2: 0.810162\n",
      "epoch: 138, train loss: 0.002779, val loss: 0.007131, val mae: 0.060365, val r2: 0.817645\n",
      "epoch: 139, train loss: 0.002763, val loss: 0.007160, val mae: 0.060472, val r2: 0.816824\n",
      "epoch: 140, train loss: 0.002744, val loss: 0.007285, val mae: 0.061001, val r2: 0.813530\n",
      "epoch: 141, train loss: 0.002724, val loss: 0.007063, val mae: 0.060092, val r2: 0.819503\n",
      "epoch: 142, train loss: 0.002701, val loss: 0.006813, val mae: 0.058909, val r2: 0.825964\n",
      "epoch: 143, train loss: 0.002690, val loss: 0.006783, val mae: 0.058759, val r2: 0.826833\n",
      "epoch: 144, train loss: 0.002667, val loss: 0.006652, val mae: 0.058140, val r2: 0.830491\n",
      "epoch: 145, train loss: 0.002653, val loss: 0.006629, val mae: 0.058059, val r2: 0.830910\n",
      "epoch: 146, train loss: 0.002631, val loss: 0.006416, val mae: 0.056996, val r2: 0.836610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 147, train loss: 0.002612, val loss: 0.006308, val mae: 0.056477, val r2: 0.839751\n",
      "epoch: 148, train loss: 0.002598, val loss: 0.006184, val mae: 0.055805, val r2: 0.843068\n",
      "epoch: 149, train loss: 0.002584, val loss: 0.006319, val mae: 0.056524, val r2: 0.839812\n",
      "epoch: 150, train loss: 0.002573, val loss: 0.006138, val mae: 0.055602, val r2: 0.844765\n",
      "epoch: 151, train loss: 0.002559, val loss: 0.006004, val mae: 0.054948, val r2: 0.848270\n",
      "epoch: 152, train loss: 0.002543, val loss: 0.005884, val mae: 0.054309, val r2: 0.851458\n",
      "epoch: 153, train loss: 0.002538, val loss: 0.005780, val mae: 0.053752, val r2: 0.854376\n",
      "epoch: 154, train loss: 0.002529, val loss: 0.005667, val mae: 0.053136, val r2: 0.857336\n",
      "epoch: 155, train loss: 0.002512, val loss: 0.005570, val mae: 0.052551, val r2: 0.859990\n",
      "epoch: 156, train loss: 0.002507, val loss: 0.005453, val mae: 0.051886, val r2: 0.862996\n",
      "epoch: 157, train loss: 0.002496, val loss: 0.005404, val mae: 0.051658, val r2: 0.864374\n",
      "epoch: 158, train loss: 0.002493, val loss: 0.005354, val mae: 0.051409, val r2: 0.865569\n",
      "epoch: 159, train loss: 0.002491, val loss: 0.005299, val mae: 0.051097, val r2: 0.867054\n",
      "epoch: 160, train loss: 0.002486, val loss: 0.005243, val mae: 0.050752, val r2: 0.868342\n",
      "epoch: 161, train loss: 0.002479, val loss: 0.005212, val mae: 0.050558, val r2: 0.869206\n",
      "epoch: 162, train loss: 0.002476, val loss: 0.005235, val mae: 0.050707, val r2: 0.868495\n",
      "epoch: 163, train loss: 0.002478, val loss: 0.005254, val mae: 0.050772, val r2: 0.868079\n",
      "epoch: 164, train loss: 0.002467, val loss: 0.005262, val mae: 0.050843, val r2: 0.867850\n",
      "epoch: 165, train loss: 0.002461, val loss: 0.005345, val mae: 0.051299, val r2: 0.865845\n",
      "epoch: 166, train loss: 0.002456, val loss: 0.005290, val mae: 0.050996, val r2: 0.867185\n",
      "epoch: 167, train loss: 0.002452, val loss: 0.005383, val mae: 0.051489, val r2: 0.864975\n",
      "epoch: 168, train loss: 0.002451, val loss: 0.005397, val mae: 0.051548, val r2: 0.864775\n",
      "epoch: 169, train loss: 0.002444, val loss: 0.005399, val mae: 0.051600, val r2: 0.864729\n",
      "epoch: 170, train loss: 0.002437, val loss: 0.005608, val mae: 0.052760, val r2: 0.859643\n",
      "epoch: 171, train loss: 0.002437, val loss: 0.005670, val mae: 0.052987, val r2: 0.858303\n",
      "epoch: 172, train loss: 0.002433, val loss: 0.005816, val mae: 0.053677, val r2: 0.854900\n",
      "epoch: 173, train loss: 0.002447, val loss: 0.005973, val mae: 0.054468, val r2: 0.851159\n",
      "epoch: 174, train loss: 0.002473, val loss: 0.005943, val mae: 0.054242, val r2: 0.851969\n",
      "epoch: 175, train loss: 0.002497, val loss: 0.005775, val mae: 0.053366, val r2: 0.856260\n",
      "epoch: 176, train loss: 0.002542, val loss: 0.005445, val mae: 0.051535, val r2: 0.864343\n",
      "epoch: 177, train loss: 0.002587, val loss: 0.005186, val mae: 0.049945, val r2: 0.870397\n",
      "epoch: 178, train loss: 0.002573, val loss: 0.005304, val mae: 0.050228, val r2: 0.867085\n",
      "epoch: 179, train loss: 0.002521, val loss: 0.005268, val mae: 0.050080, val r2: 0.867852\n",
      "epoch: 180, train loss: 0.002541, val loss: 0.005268, val mae: 0.050252, val r2: 0.867789\n",
      "epoch: 181, train loss: 0.002650, val loss: 0.005515, val mae: 0.051851, val r2: 0.861773\n",
      "epoch: 182, train loss: 0.002651, val loss: 0.005952, val mae: 0.054273, val r2: 0.850990\n",
      "epoch: 183, train loss: 0.002533, val loss: 0.005825, val mae: 0.053636, val r2: 0.854175\n",
      "epoch: 184, train loss: 0.002455, val loss: 0.005620, val mae: 0.052609, val r2: 0.859040\n",
      "epoch: 185, train loss: 0.002437, val loss: 0.005485, val mae: 0.051923, val r2: 0.862329\n",
      "epoch: 186, train loss: 0.002433, val loss: 0.005481, val mae: 0.051873, val r2: 0.862243\n",
      "epoch: 187, train loss: 0.002422, val loss: 0.005676, val mae: 0.052946, val r2: 0.857180\n",
      "epoch: 188, train loss: 0.002418, val loss: 0.005831, val mae: 0.053773, val r2: 0.853167\n",
      "epoch: 189, train loss: 0.002398, val loss: 0.005947, val mae: 0.054422, val r2: 0.850001\n",
      "epoch: 190, train loss: 0.002396, val loss: 0.006032, val mae: 0.054921, val r2: 0.847679\n",
      "epoch: 191, train loss: 0.002394, val loss: 0.006190, val mae: 0.055638, val r2: 0.843519\n",
      "epoch: 192, train loss: 0.002403, val loss: 0.006147, val mae: 0.055546, val r2: 0.844377\n",
      "epoch: 193, train loss: 0.002417, val loss: 0.006238, val mae: 0.055943, val r2: 0.841743\n",
      "epoch: 194, train loss: 0.002420, val loss: 0.006376, val mae: 0.056703, val r2: 0.838020\n",
      "epoch: 195, train loss: 0.002424, val loss: 0.006414, val mae: 0.056900, val r2: 0.836636\n",
      "epoch: 196, train loss: 0.002438, val loss: 0.006727, val mae: 0.058375, val r2: 0.828340\n",
      "epoch: 197, train loss: 0.002447, val loss: 0.006894, val mae: 0.059147, val r2: 0.823970\n",
      "epoch: 198, train loss: 0.002459, val loss: 0.007062, val mae: 0.059931, val r2: 0.819215\n",
      "epoch: 199, train loss: 0.002466, val loss: 0.007113, val mae: 0.060203, val r2: 0.817615\n",
      "epoch: 200, train loss: 0.002479, val loss: 0.007244, val mae: 0.060858, val r2: 0.814201\n",
      "epoch: 201, train loss: 0.002480, val loss: 0.007165, val mae: 0.060451, val r2: 0.815984\n",
      "epoch: 202, train loss: 0.002477, val loss: 0.007359, val mae: 0.061403, val r2: 0.811032\n",
      "epoch: 203, train loss: 0.002464, val loss: 0.007261, val mae: 0.060868, val r2: 0.813562\n",
      "epoch: 204, train loss: 0.002449, val loss: 0.007112, val mae: 0.060193, val r2: 0.817687\n",
      "epoch: 205, train loss: 0.002432, val loss: 0.006943, val mae: 0.059434, val r2: 0.822167\n",
      "epoch: 206, train loss: 0.002419, val loss: 0.006551, val mae: 0.057585, val r2: 0.832765\n",
      "epoch: 207, train loss: 0.002408, val loss: 0.006210, val mae: 0.055894, val r2: 0.841719\n",
      "epoch: 208, train loss: 0.002384, val loss: 0.005996, val mae: 0.054869, val r2: 0.847577\n",
      "epoch: 209, train loss: 0.002373, val loss: 0.005908, val mae: 0.054383, val r2: 0.850210\n",
      "epoch: 210, train loss: 0.002357, val loss: 0.005613, val mae: 0.052831, val r2: 0.858147\n",
      "epoch: 211, train loss: 0.002343, val loss: 0.005384, val mae: 0.051573, val r2: 0.864430\n",
      "epoch: 212, train loss: 0.002343, val loss: 0.005152, val mae: 0.050160, val r2: 0.870550\n",
      "epoch: 213, train loss: 0.002341, val loss: 0.005119, val mae: 0.049845, val r2: 0.871486\n",
      "epoch: 214, train loss: 0.002343, val loss: 0.005104, val mae: 0.049711, val r2: 0.871982\n",
      "epoch: 215, train loss: 0.002339, val loss: 0.005106, val mae: 0.049704, val r2: 0.872009\n",
      "epoch: 216, train loss: 0.002330, val loss: 0.005077, val mae: 0.049501, val r2: 0.872685\n",
      "epoch: 217, train loss: 0.002320, val loss: 0.005051, val mae: 0.049357, val r2: 0.873353\n",
      "epoch: 218, train loss: 0.002317, val loss: 0.005040, val mae: 0.049286, val r2: 0.873627\n",
      "epoch: 219, train loss: 0.002315, val loss: 0.005074, val mae: 0.049387, val r2: 0.872720\n",
      "epoch: 220, train loss: 0.002311, val loss: 0.005114, val mae: 0.049660, val r2: 0.871693\n",
      "epoch: 221, train loss: 0.002314, val loss: 0.005120, val mae: 0.049726, val r2: 0.871614\n",
      "epoch: 222, train loss: 0.002306, val loss: 0.005196, val mae: 0.050317, val r2: 0.869656\n",
      "epoch: 223, train loss: 0.002288, val loss: 0.005260, val mae: 0.050781, val r2: 0.868120\n",
      "epoch: 224, train loss: 0.002270, val loss: 0.005283, val mae: 0.050921, val r2: 0.867482\n",
      "epoch: 225, train loss: 0.002253, val loss: 0.005307, val mae: 0.051038, val r2: 0.866826\n",
      "epoch: 226, train loss: 0.002244, val loss: 0.005310, val mae: 0.051077, val r2: 0.866675\n",
      "epoch: 227, train loss: 0.002230, val loss: 0.005301, val mae: 0.051020, val r2: 0.866809\n",
      "epoch: 228, train loss: 0.002224, val loss: 0.005292, val mae: 0.050990, val r2: 0.867024\n",
      "epoch: 229, train loss: 0.002215, val loss: 0.005330, val mae: 0.051232, val r2: 0.865914\n",
      "epoch: 230, train loss: 0.002215, val loss: 0.005347, val mae: 0.051313, val r2: 0.865275\n",
      "epoch: 231, train loss: 0.002223, val loss: 0.005440, val mae: 0.051854, val r2: 0.862539\n",
      "epoch: 232, train loss: 0.002232, val loss: 0.005608, val mae: 0.052732, val r2: 0.858161\n",
      "epoch: 233, train loss: 0.002230, val loss: 0.005906, val mae: 0.054349, val r2: 0.849922\n",
      "epoch: 234, train loss: 0.002234, val loss: 0.006122, val mae: 0.055488, val r2: 0.844222\n",
      "epoch: 235, train loss: 0.002232, val loss: 0.006232, val mae: 0.055947, val r2: 0.841268\n",
      "epoch: 236, train loss: 0.002229, val loss: 0.006571, val mae: 0.057631, val r2: 0.832280\n",
      "epoch: 237, train loss: 0.002225, val loss: 0.006850, val mae: 0.058891, val r2: 0.824730\n",
      "epoch: 238, train loss: 0.002227, val loss: 0.006665, val mae: 0.058013, val r2: 0.829960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 239, train loss: 0.002226, val loss: 0.006449, val mae: 0.056976, val r2: 0.835670\n",
      "epoch: 240, train loss: 0.002218, val loss: 0.006407, val mae: 0.056814, val r2: 0.836825\n",
      "epoch: 241, train loss: 0.002212, val loss: 0.006240, val mae: 0.056007, val r2: 0.840956\n",
      "epoch: 242, train loss: 0.002202, val loss: 0.005991, val mae: 0.054850, val r2: 0.847761\n",
      "epoch: 243, train loss: 0.002199, val loss: 0.005847, val mae: 0.054120, val r2: 0.851429\n",
      "epoch: 244, train loss: 0.002206, val loss: 0.005696, val mae: 0.053331, val r2: 0.855426\n",
      "epoch: 245, train loss: 0.002216, val loss: 0.005557, val mae: 0.052664, val r2: 0.859389\n",
      "epoch: 246, train loss: 0.002235, val loss: 0.005520, val mae: 0.052441, val r2: 0.860606\n",
      "epoch: 247, train loss: 0.002227, val loss: 0.005440, val mae: 0.051937, val r2: 0.862941\n",
      "epoch: 248, train loss: 0.002222, val loss: 0.005327, val mae: 0.051215, val r2: 0.866138\n",
      "epoch: 249, train loss: 0.002211, val loss: 0.005218, val mae: 0.050619, val r2: 0.869035\n",
      "epoch: 250, train loss: 0.002217, val loss: 0.005104, val mae: 0.049913, val r2: 0.871930\n",
      "epoch: 251, train loss: 0.002208, val loss: 0.005067, val mae: 0.049531, val r2: 0.872867\n",
      "epoch: 252, train loss: 0.002192, val loss: 0.005065, val mae: 0.049299, val r2: 0.872941\n",
      "epoch: 253, train loss: 0.002179, val loss: 0.005116, val mae: 0.049492, val r2: 0.871695\n",
      "epoch: 254, train loss: 0.002179, val loss: 0.005114, val mae: 0.049628, val r2: 0.871776\n",
      "epoch: 255, train loss: 0.002190, val loss: 0.005164, val mae: 0.050024, val r2: 0.870517\n",
      "epoch: 256, train loss: 0.002166, val loss: 0.005296, val mae: 0.050942, val r2: 0.867177\n",
      "epoch: 257, train loss: 0.002132, val loss: 0.005400, val mae: 0.051618, val r2: 0.864399\n",
      "epoch: 258, train loss: 0.002126, val loss: 0.005388, val mae: 0.051659, val r2: 0.864458\n",
      "epoch: 259, train loss: 0.002128, val loss: 0.005349, val mae: 0.051430, val r2: 0.865314\n",
      "epoch: 260, train loss: 0.002117, val loss: 0.005349, val mae: 0.051428, val r2: 0.865114\n",
      "epoch: 261, train loss: 0.002103, val loss: 0.005325, val mae: 0.051307, val r2: 0.865794\n",
      "epoch: 262, train loss: 0.002097, val loss: 0.005429, val mae: 0.051852, val r2: 0.862872\n",
      "epoch: 263, train loss: 0.002094, val loss: 0.005567, val mae: 0.052677, val r2: 0.859031\n",
      "epoch: 264, train loss: 0.002091, val loss: 0.005676, val mae: 0.053292, val r2: 0.856188\n",
      "epoch: 265, train loss: 0.002085, val loss: 0.005898, val mae: 0.054491, val r2: 0.850082\n",
      "epoch: 266, train loss: 0.002087, val loss: 0.005933, val mae: 0.054625, val r2: 0.849204\n",
      "epoch: 267, train loss: 0.002084, val loss: 0.005883, val mae: 0.054403, val r2: 0.850610\n",
      "epoch: 268, train loss: 0.002086, val loss: 0.005840, val mae: 0.054240, val r2: 0.851801\n",
      "epoch: 269, train loss: 0.002076, val loss: 0.005698, val mae: 0.053494, val r2: 0.855549\n",
      "epoch: 270, train loss: 0.002075, val loss: 0.005557, val mae: 0.052752, val r2: 0.859549\n",
      "epoch: 271, train loss: 0.002065, val loss: 0.005457, val mae: 0.052231, val r2: 0.862240\n",
      "epoch: 272, train loss: 0.002063, val loss: 0.005380, val mae: 0.051703, val r2: 0.864288\n",
      "epoch: 273, train loss: 0.002058, val loss: 0.005214, val mae: 0.050690, val r2: 0.868711\n",
      "epoch: 274, train loss: 0.002063, val loss: 0.005154, val mae: 0.050230, val r2: 0.870366\n",
      "epoch: 275, train loss: 0.002059, val loss: 0.005130, val mae: 0.049857, val r2: 0.870997\n",
      "epoch: 276, train loss: 0.002060, val loss: 0.005117, val mae: 0.049688, val r2: 0.871345\n",
      "epoch: 277, train loss: 0.002059, val loss: 0.005118, val mae: 0.049615, val r2: 0.871416\n",
      "epoch: 278, train loss: 0.002058, val loss: 0.005105, val mae: 0.049589, val r2: 0.871779\n",
      "epoch: 279, train loss: 0.002049, val loss: 0.005111, val mae: 0.049660, val r2: 0.871659\n",
      "epoch: 280, train loss: 0.002040, val loss: 0.005139, val mae: 0.049884, val r2: 0.870968\n",
      "epoch: 281, train loss: 0.002035, val loss: 0.005113, val mae: 0.049744, val r2: 0.871592\n",
      "epoch: 282, train loss: 0.002027, val loss: 0.005140, val mae: 0.049855, val r2: 0.870986\n",
      "epoch: 283, train loss: 0.002020, val loss: 0.005178, val mae: 0.050072, val r2: 0.870021\n",
      "epoch: 284, train loss: 0.002017, val loss: 0.005174, val mae: 0.050159, val r2: 0.870102\n",
      "epoch: 285, train loss: 0.002008, val loss: 0.005190, val mae: 0.050204, val r2: 0.869611\n",
      "epoch: 286, train loss: 0.002002, val loss: 0.005214, val mae: 0.050450, val r2: 0.868868\n",
      "epoch: 287, train loss: 0.002005, val loss: 0.005246, val mae: 0.050655, val r2: 0.868025\n",
      "epoch: 288, train loss: 0.002000, val loss: 0.005261, val mae: 0.050854, val r2: 0.867340\n",
      "epoch: 289, train loss: 0.002007, val loss: 0.005333, val mae: 0.051366, val r2: 0.865473\n",
      "epoch: 290, train loss: 0.002008, val loss: 0.005496, val mae: 0.052335, val r2: 0.860975\n",
      "epoch: 291, train loss: 0.002003, val loss: 0.005565, val mae: 0.052727, val r2: 0.858905\n",
      "epoch: 292, train loss: 0.002006, val loss: 0.005779, val mae: 0.053890, val r2: 0.853235\n",
      "epoch: 293, train loss: 0.002006, val loss: 0.005955, val mae: 0.054717, val r2: 0.848675\n",
      "epoch: 294, train loss: 0.002007, val loss: 0.006264, val mae: 0.056231, val r2: 0.840367\n",
      "epoch: 295, train loss: 0.002021, val loss: 0.006427, val mae: 0.057085, val r2: 0.836217\n",
      "epoch: 296, train loss: 0.002046, val loss: 0.006208, val mae: 0.056110, val r2: 0.842394\n",
      "epoch: 297, train loss: 0.002067, val loss: 0.006028, val mae: 0.055235, val r2: 0.847276\n",
      "epoch: 298, train loss: 0.002080, val loss: 0.005433, val mae: 0.052123, val r2: 0.862750\n",
      "epoch: 299, train loss: 0.002062, val loss: 0.005184, val mae: 0.050360, val r2: 0.869355\n",
      "epoch: 300, train loss: 0.002028, val loss: 0.005179, val mae: 0.050156, val r2: 0.869400\n",
      "epoch: 301, train loss: 0.002048, val loss: 0.005320, val mae: 0.050815, val r2: 0.865998\n",
      "epoch: 302, train loss: 0.002039, val loss: 0.005443, val mae: 0.051577, val r2: 0.863035\n",
      "epoch: 303, train loss: 0.001997, val loss: 0.005546, val mae: 0.052207, val r2: 0.860683\n",
      "epoch: 304, train loss: 0.001989, val loss: 0.005413, val mae: 0.051581, val r2: 0.863716\n",
      "epoch: 305, train loss: 0.001994, val loss: 0.005149, val mae: 0.049981, val r2: 0.870406\n",
      "epoch: 306, train loss: 0.001955, val loss: 0.005139, val mae: 0.049773, val r2: 0.870704\n",
      "epoch: 307, train loss: 0.001940, val loss: 0.005247, val mae: 0.050568, val r2: 0.867914\n",
      "epoch: 308, train loss: 0.001938, val loss: 0.005519, val mae: 0.052176, val r2: 0.861140\n",
      "epoch: 309, train loss: 0.001923, val loss: 0.005785, val mae: 0.053617, val r2: 0.854136\n",
      "epoch: 310, train loss: 0.001918, val loss: 0.005889, val mae: 0.054253, val r2: 0.851234\n",
      "epoch: 311, train loss: 0.001921, val loss: 0.005756, val mae: 0.053450, val r2: 0.854415\n",
      "epoch: 312, train loss: 0.001926, val loss: 0.005578, val mae: 0.052589, val r2: 0.858891\n",
      "epoch: 313, train loss: 0.001913, val loss: 0.005475, val mae: 0.052034, val r2: 0.861496\n",
      "epoch: 314, train loss: 0.001902, val loss: 0.005477, val mae: 0.052046, val r2: 0.861571\n",
      "epoch: 315, train loss: 0.001894, val loss: 0.005340, val mae: 0.051226, val r2: 0.865152\n",
      "epoch: 316, train loss: 0.001893, val loss: 0.005392, val mae: 0.051499, val r2: 0.863889\n",
      "epoch: 317, train loss: 0.001893, val loss: 0.005409, val mae: 0.051662, val r2: 0.863403\n",
      "epoch: 318, train loss: 0.001896, val loss: 0.005432, val mae: 0.051771, val r2: 0.863126\n",
      "epoch: 319, train loss: 0.001895, val loss: 0.005506, val mae: 0.052081, val r2: 0.861424\n",
      "epoch: 320, train loss: 0.001889, val loss: 0.005560, val mae: 0.052238, val r2: 0.860328\n",
      "epoch: 321, train loss: 0.001887, val loss: 0.005510, val mae: 0.052094, val r2: 0.861692\n",
      "epoch: 322, train loss: 0.001894, val loss: 0.005486, val mae: 0.051863, val r2: 0.862297\n",
      "epoch: 323, train loss: 0.001893, val loss: 0.005438, val mae: 0.051563, val r2: 0.863564\n",
      "epoch: 324, train loss: 0.001892, val loss: 0.005365, val mae: 0.051127, val r2: 0.865265\n",
      "epoch: 325, train loss: 0.001881, val loss: 0.005335, val mae: 0.050909, val r2: 0.866104\n",
      "epoch: 326, train loss: 0.001885, val loss: 0.005391, val mae: 0.051219, val r2: 0.864674\n",
      "epoch: 327, train loss: 0.001894, val loss: 0.005391, val mae: 0.051285, val r2: 0.864525\n",
      "epoch: 328, train loss: 0.001893, val loss: 0.005565, val mae: 0.052091, val r2: 0.859833\n",
      "epoch: 329, train loss: 0.001895, val loss: 0.005781, val mae: 0.053147, val r2: 0.854360\n",
      "epoch: 330, train loss: 0.001893, val loss: 0.006103, val mae: 0.054797, val r2: 0.846020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 331, train loss: 0.001881, val loss: 0.006577, val mae: 0.057211, val r2: 0.833505\n",
      "epoch: 332, train loss: 0.001883, val loss: 0.006992, val mae: 0.059147, val r2: 0.822930\n",
      "epoch: 333, train loss: 0.001898, val loss: 0.007059, val mae: 0.059632, val r2: 0.820804\n",
      "epoch: 334, train loss: 0.001889, val loss: 0.007026, val mae: 0.059657, val r2: 0.821635\n",
      "epoch: 335, train loss: 0.001901, val loss: 0.006505, val mae: 0.057153, val r2: 0.834833\n",
      "epoch: 336, train loss: 0.001909, val loss: 0.006001, val mae: 0.054528, val r2: 0.847845\n",
      "epoch: 337, train loss: 0.001876, val loss: 0.005970, val mae: 0.054425, val r2: 0.848505\n",
      "epoch: 338, train loss: 0.001863, val loss: 0.006139, val mae: 0.055232, val r2: 0.844282\n",
      "epoch: 339, train loss: 0.001866, val loss: 0.006423, val mae: 0.056741, val r2: 0.836953\n",
      "epoch: 340, train loss: 0.001858, val loss: 0.006696, val mae: 0.058158, val r2: 0.829968\n",
      "epoch: 341, train loss: 0.001851, val loss: 0.006716, val mae: 0.058252, val r2: 0.829535\n",
      "epoch: 342, train loss: 0.001856, val loss: 0.006283, val mae: 0.056048, val r2: 0.841059\n",
      "epoch: 343, train loss: 0.001857, val loss: 0.005714, val mae: 0.053127, val r2: 0.855541\n",
      "epoch: 344, train loss: 0.001839, val loss: 0.005472, val mae: 0.051738, val r2: 0.861988\n",
      "epoch: 345, train loss: 0.001844, val loss: 0.005423, val mae: 0.051407, val r2: 0.863377\n",
      "epoch: 346, train loss: 0.001853, val loss: 0.005678, val mae: 0.052828, val r2: 0.857257\n",
      "epoch: 347, train loss: 0.001847, val loss: 0.005910, val mae: 0.053899, val r2: 0.851697\n",
      "epoch: 348, train loss: 0.001837, val loss: 0.005965, val mae: 0.054163, val r2: 0.850448\n",
      "epoch: 349, train loss: 0.001841, val loss: 0.006096, val mae: 0.054692, val r2: 0.847391\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.15, \n",
    "                               'layers': [8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 32,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 1,\n",
    "                      'dropout': 0.2, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 6, \n",
    "                      'epoches': 350,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84511df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps!\n",
      "Transformer(\n",
      "  (element_embedding_temporal): Linear(in_features=294, out_features=192, bias=True)\n",
      "  (element_embedding_spatial): Linear(in_features=2695, out_features=1760, bias=True)\n",
      "  (pos_embedding): Embedding(6, 32)\n",
      "  (variable_embedding): Embedding(55, 32)\n",
      "  (temporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatial): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatiotemporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (keys): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (queries): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatter): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=11220, out_features=2640, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=2640, out_features=330, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch: 0, train loss: 0.017911, val loss: 0.015112, val mae: 0.096152, val r2: 0.617302\n",
      "epoch: 1, train loss: 0.010686, val loss: 0.012697, val mae: 0.085737, val r2: 0.678556\n",
      "epoch: 2, train loss: 0.009175, val loss: 0.011499, val mae: 0.080711, val r2: 0.708718\n",
      "epoch: 3, train loss: 0.008268, val loss: 0.010584, val mae: 0.076956, val r2: 0.731921\n",
      "epoch: 4, train loss: 0.007606, val loss: 0.010008, val mae: 0.074543, val r2: 0.746769\n",
      "epoch: 5, train loss: 0.007088, val loss: 0.009604, val mae: 0.072879, val r2: 0.757185\n",
      "epoch: 6, train loss: 0.006664, val loss: 0.009239, val mae: 0.071349, val r2: 0.766784\n",
      "epoch: 7, train loss: 0.006298, val loss: 0.008875, val mae: 0.069664, val r2: 0.776186\n",
      "epoch: 8, train loss: 0.005983, val loss: 0.008589, val mae: 0.068309, val r2: 0.783666\n",
      "epoch: 9, train loss: 0.005708, val loss: 0.008379, val mae: 0.067278, val r2: 0.789146\n",
      "epoch: 10, train loss: 0.005466, val loss: 0.008117, val mae: 0.065988, val r2: 0.795941\n",
      "epoch: 11, train loss: 0.005261, val loss: 0.007967, val mae: 0.065167, val r2: 0.799887\n",
      "epoch: 12, train loss: 0.005087, val loss: 0.007847, val mae: 0.064535, val r2: 0.803009\n",
      "epoch: 13, train loss: 0.004932, val loss: 0.007709, val mae: 0.063804, val r2: 0.806625\n",
      "epoch: 14, train loss: 0.004789, val loss: 0.007620, val mae: 0.063247, val r2: 0.809024\n",
      "epoch: 15, train loss: 0.004665, val loss: 0.007473, val mae: 0.062494, val r2: 0.812870\n",
      "epoch: 16, train loss: 0.004540, val loss: 0.007412, val mae: 0.062105, val r2: 0.814380\n",
      "epoch: 17, train loss: 0.004448, val loss: 0.007324, val mae: 0.061638, val r2: 0.816810\n",
      "epoch: 18, train loss: 0.004356, val loss: 0.007276, val mae: 0.061403, val r2: 0.818012\n",
      "epoch: 19, train loss: 0.004277, val loss: 0.007280, val mae: 0.061347, val r2: 0.818129\n",
      "epoch: 20, train loss: 0.004200, val loss: 0.007276, val mae: 0.061292, val r2: 0.818377\n",
      "epoch: 21, train loss: 0.004142, val loss: 0.007207, val mae: 0.060896, val r2: 0.820178\n",
      "epoch: 22, train loss: 0.004078, val loss: 0.007201, val mae: 0.060877, val r2: 0.820439\n",
      "epoch: 23, train loss: 0.004031, val loss: 0.007276, val mae: 0.061222, val r2: 0.818743\n",
      "epoch: 24, train loss: 0.003983, val loss: 0.007194, val mae: 0.060859, val r2: 0.820827\n",
      "epoch: 25, train loss: 0.003944, val loss: 0.007160, val mae: 0.060624, val r2: 0.821624\n",
      "epoch: 26, train loss: 0.003899, val loss: 0.007145, val mae: 0.060595, val r2: 0.822037\n",
      "epoch: 27, train loss: 0.003869, val loss: 0.007173, val mae: 0.060718, val r2: 0.821454\n",
      "epoch: 28, train loss: 0.003839, val loss: 0.007080, val mae: 0.060315, val r2: 0.823693\n",
      "epoch: 29, train loss: 0.003810, val loss: 0.007116, val mae: 0.060450, val r2: 0.822865\n",
      "epoch: 30, train loss: 0.003795, val loss: 0.007164, val mae: 0.060631, val r2: 0.821822\n",
      "epoch: 31, train loss: 0.003784, val loss: 0.007123, val mae: 0.060431, val r2: 0.822739\n",
      "epoch: 32, train loss: 0.003768, val loss: 0.006998, val mae: 0.059814, val r2: 0.825792\n",
      "epoch: 33, train loss: 0.003759, val loss: 0.007004, val mae: 0.059828, val r2: 0.825611\n",
      "epoch: 34, train loss: 0.003773, val loss: 0.006901, val mae: 0.059353, val r2: 0.828174\n",
      "epoch: 35, train loss: 0.003801, val loss: 0.006741, val mae: 0.058474, val r2: 0.832050\n",
      "epoch: 36, train loss: 0.003828, val loss: 0.006457, val mae: 0.056928, val r2: 0.838910\n",
      "epoch: 37, train loss: 0.003866, val loss: 0.006179, val mae: 0.055380, val r2: 0.845513\n",
      "epoch: 38, train loss: 0.003860, val loss: 0.006084, val mae: 0.054642, val r2: 0.847685\n",
      "epoch: 39, train loss: 0.003850, val loss: 0.006087, val mae: 0.054433, val r2: 0.847434\n",
      "epoch: 40, train loss: 0.003752, val loss: 0.006104, val mae: 0.054438, val r2: 0.846893\n",
      "epoch: 41, train loss: 0.003631, val loss: 0.006138, val mae: 0.054472, val r2: 0.846064\n",
      "epoch: 42, train loss: 0.003521, val loss: 0.006050, val mae: 0.054119, val r2: 0.848286\n",
      "epoch: 43, train loss: 0.003469, val loss: 0.006023, val mae: 0.053977, val r2: 0.848928\n",
      "epoch: 44, train loss: 0.003443, val loss: 0.005983, val mae: 0.053787, val r2: 0.849984\n",
      "epoch: 45, train loss: 0.003456, val loss: 0.005979, val mae: 0.053728, val r2: 0.850046\n",
      "epoch: 46, train loss: 0.003464, val loss: 0.005957, val mae: 0.053625, val r2: 0.850575\n",
      "epoch: 47, train loss: 0.003473, val loss: 0.005948, val mae: 0.053624, val r2: 0.850748\n",
      "epoch: 48, train loss: 0.003490, val loss: 0.005937, val mae: 0.053559, val r2: 0.851102\n",
      "epoch: 49, train loss: 0.003498, val loss: 0.005939, val mae: 0.053559, val r2: 0.851050\n",
      "epoch: 50, train loss: 0.003500, val loss: 0.005925, val mae: 0.053582, val r2: 0.851532\n",
      "epoch: 51, train loss: 0.003492, val loss: 0.005941, val mae: 0.053701, val r2: 0.851286\n",
      "epoch: 52, train loss: 0.003454, val loss: 0.005980, val mae: 0.053948, val r2: 0.850420\n",
      "epoch: 53, train loss: 0.003416, val loss: 0.006041, val mae: 0.054297, val r2: 0.849203\n",
      "epoch: 54, train loss: 0.003353, val loss: 0.006079, val mae: 0.054575, val r2: 0.848334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55, train loss: 0.003293, val loss: 0.006098, val mae: 0.054631, val r2: 0.847991\n",
      "epoch: 56, train loss: 0.003239, val loss: 0.006116, val mae: 0.054744, val r2: 0.847680\n",
      "epoch: 57, train loss: 0.003217, val loss: 0.006075, val mae: 0.054557, val r2: 0.848700\n",
      "epoch: 58, train loss: 0.003224, val loss: 0.006085, val mae: 0.054615, val r2: 0.848516\n",
      "epoch: 59, train loss: 0.003254, val loss: 0.006062, val mae: 0.054551, val r2: 0.849027\n",
      "epoch: 60, train loss: 0.003292, val loss: 0.005991, val mae: 0.054198, val r2: 0.850670\n",
      "epoch: 61, train loss: 0.003330, val loss: 0.005874, val mae: 0.053469, val r2: 0.853400\n",
      "epoch: 62, train loss: 0.003369, val loss: 0.005768, val mae: 0.052755, val r2: 0.855716\n",
      "epoch: 63, train loss: 0.003354, val loss: 0.005814, val mae: 0.052691, val r2: 0.854303\n",
      "epoch: 64, train loss: 0.003302, val loss: 0.005904, val mae: 0.053040, val r2: 0.851982\n",
      "epoch: 65, train loss: 0.003193, val loss: 0.005943, val mae: 0.053235, val r2: 0.850882\n",
      "epoch: 66, train loss: 0.003103, val loss: 0.005844, val mae: 0.052788, val r2: 0.853451\n",
      "epoch: 67, train loss: 0.003065, val loss: 0.005798, val mae: 0.052563, val r2: 0.854474\n",
      "epoch: 68, train loss: 0.003072, val loss: 0.005814, val mae: 0.052615, val r2: 0.854066\n",
      "epoch: 69, train loss: 0.003092, val loss: 0.005806, val mae: 0.052599, val r2: 0.854144\n",
      "epoch: 70, train loss: 0.003111, val loss: 0.005792, val mae: 0.052503, val r2: 0.854586\n",
      "epoch: 71, train loss: 0.003104, val loss: 0.005731, val mae: 0.052332, val r2: 0.856149\n",
      "epoch: 72, train loss: 0.003096, val loss: 0.005756, val mae: 0.052443, val r2: 0.855578\n",
      "epoch: 73, train loss: 0.003064, val loss: 0.005744, val mae: 0.052475, val r2: 0.856060\n",
      "epoch: 74, train loss: 0.003026, val loss: 0.005730, val mae: 0.052466, val r2: 0.856544\n",
      "epoch: 75, train loss: 0.003002, val loss: 0.005740, val mae: 0.052502, val r2: 0.856488\n",
      "epoch: 76, train loss: 0.002976, val loss: 0.005742, val mae: 0.052541, val r2: 0.856510\n",
      "epoch: 77, train loss: 0.002967, val loss: 0.005739, val mae: 0.052612, val r2: 0.856714\n",
      "epoch: 78, train loss: 0.002988, val loss: 0.005717, val mae: 0.052534, val r2: 0.857341\n",
      "epoch: 79, train loss: 0.003036, val loss: 0.005703, val mae: 0.052448, val r2: 0.857748\n",
      "epoch: 80, train loss: 0.003084, val loss: 0.005623, val mae: 0.051869, val r2: 0.859372\n",
      "epoch: 81, train loss: 0.003129, val loss: 0.005638, val mae: 0.051814, val r2: 0.858713\n",
      "epoch: 82, train loss: 0.003104, val loss: 0.005759, val mae: 0.052172, val r2: 0.855478\n",
      "epoch: 83, train loss: 0.003048, val loss: 0.005937, val mae: 0.053000, val r2: 0.850949\n",
      "epoch: 84, train loss: 0.002978, val loss: 0.005932, val mae: 0.053013, val r2: 0.851003\n",
      "epoch: 85, train loss: 0.002941, val loss: 0.005845, val mae: 0.052684, val r2: 0.853021\n",
      "epoch: 86, train loss: 0.002961, val loss: 0.005826, val mae: 0.052689, val r2: 0.853482\n",
      "epoch: 87, train loss: 0.002971, val loss: 0.005758, val mae: 0.052436, val r2: 0.855352\n",
      "epoch: 88, train loss: 0.002944, val loss: 0.005754, val mae: 0.052488, val r2: 0.855634\n",
      "epoch: 89, train loss: 0.002914, val loss: 0.005740, val mae: 0.052455, val r2: 0.856138\n",
      "epoch: 90, train loss: 0.002876, val loss: 0.005751, val mae: 0.052519, val r2: 0.855931\n",
      "epoch: 91, train loss: 0.002867, val loss: 0.005751, val mae: 0.052472, val r2: 0.856001\n",
      "epoch: 92, train loss: 0.002875, val loss: 0.005714, val mae: 0.052271, val r2: 0.856859\n",
      "epoch: 93, train loss: 0.002893, val loss: 0.005700, val mae: 0.052077, val r2: 0.857134\n",
      "epoch: 94, train loss: 0.002905, val loss: 0.005721, val mae: 0.052132, val r2: 0.856507\n",
      "epoch: 95, train loss: 0.002886, val loss: 0.005724, val mae: 0.052147, val r2: 0.856319\n",
      "epoch: 96, train loss: 0.002877, val loss: 0.005752, val mae: 0.052293, val r2: 0.855556\n",
      "epoch: 97, train loss: 0.002875, val loss: 0.005818, val mae: 0.052702, val r2: 0.853738\n",
      "epoch: 98, train loss: 0.002896, val loss: 0.005855, val mae: 0.052970, val r2: 0.852738\n",
      "epoch: 99, train loss: 0.002904, val loss: 0.005886, val mae: 0.053223, val r2: 0.851979\n",
      "epoch: 100, train loss: 0.002897, val loss: 0.005924, val mae: 0.053558, val r2: 0.851229\n",
      "epoch: 101, train loss: 0.002877, val loss: 0.006015, val mae: 0.054099, val r2: 0.848892\n",
      "epoch: 102, train loss: 0.002867, val loss: 0.006081, val mae: 0.054481, val r2: 0.847342\n",
      "epoch: 103, train loss: 0.002842, val loss: 0.006054, val mae: 0.054405, val r2: 0.848030\n",
      "epoch: 104, train loss: 0.002848, val loss: 0.006072, val mae: 0.054542, val r2: 0.847422\n",
      "epoch: 105, train loss: 0.002852, val loss: 0.006056, val mae: 0.054522, val r2: 0.847652\n",
      "epoch: 106, train loss: 0.002864, val loss: 0.006120, val mae: 0.054900, val r2: 0.845819\n",
      "epoch: 107, train loss: 0.002876, val loss: 0.006205, val mae: 0.055405, val r2: 0.843551\n",
      "epoch: 108, train loss: 0.002882, val loss: 0.006436, val mae: 0.056658, val r2: 0.837533\n",
      "epoch: 109, train loss: 0.002892, val loss: 0.006773, val mae: 0.058419, val r2: 0.828603\n",
      "epoch: 110, train loss: 0.002894, val loss: 0.007095, val mae: 0.060155, val r2: 0.820347\n",
      "epoch: 111, train loss: 0.002889, val loss: 0.007524, val mae: 0.062198, val r2: 0.808952\n",
      "epoch: 112, train loss: 0.002903, val loss: 0.007792, val mae: 0.063536, val r2: 0.801971\n",
      "epoch: 113, train loss: 0.002937, val loss: 0.007609, val mae: 0.062757, val r2: 0.806173\n",
      "epoch: 114, train loss: 0.003031, val loss: 0.007688, val mae: 0.063257, val r2: 0.803423\n",
      "epoch: 115, train loss: 0.003143, val loss: 0.008103, val mae: 0.065165, val r2: 0.792037\n",
      "epoch: 116, train loss: 0.003251, val loss: 0.008215, val mae: 0.065698, val r2: 0.788907\n",
      "epoch: 117, train loss: 0.003331, val loss: 0.007758, val mae: 0.063468, val r2: 0.800825\n",
      "epoch: 118, train loss: 0.003349, val loss: 0.006959, val mae: 0.059632, val r2: 0.822112\n",
      "epoch: 119, train loss: 0.003291, val loss: 0.006217, val mae: 0.055769, val r2: 0.841826\n",
      "epoch: 120, train loss: 0.003173, val loss: 0.005779, val mae: 0.053374, val r2: 0.853778\n",
      "epoch: 121, train loss: 0.003020, val loss: 0.005509, val mae: 0.051875, val r2: 0.861115\n",
      "epoch: 122, train loss: 0.002926, val loss: 0.005372, val mae: 0.051009, val r2: 0.864921\n",
      "epoch: 123, train loss: 0.002851, val loss: 0.005304, val mae: 0.050682, val r2: 0.866760\n",
      "epoch: 124, train loss: 0.002817, val loss: 0.005267, val mae: 0.050481, val r2: 0.867789\n",
      "epoch: 125, train loss: 0.002800, val loss: 0.005211, val mae: 0.050105, val r2: 0.869233\n",
      "epoch: 126, train loss: 0.002781, val loss: 0.005208, val mae: 0.050074, val r2: 0.869344\n",
      "epoch: 127, train loss: 0.002771, val loss: 0.005191, val mae: 0.049989, val r2: 0.869864\n",
      "epoch: 128, train loss: 0.002749, val loss: 0.005195, val mae: 0.050009, val r2: 0.869723\n",
      "epoch: 129, train loss: 0.002722, val loss: 0.005186, val mae: 0.049992, val r2: 0.869950\n",
      "epoch: 130, train loss: 0.002700, val loss: 0.005186, val mae: 0.049999, val r2: 0.870049\n",
      "epoch: 131, train loss: 0.002665, val loss: 0.005188, val mae: 0.050028, val r2: 0.869969\n",
      "epoch: 132, train loss: 0.002632, val loss: 0.005172, val mae: 0.049968, val r2: 0.870356\n",
      "epoch: 133, train loss: 0.002601, val loss: 0.005161, val mae: 0.049955, val r2: 0.870646\n",
      "epoch: 134, train loss: 0.002568, val loss: 0.005167, val mae: 0.049969, val r2: 0.870534\n",
      "epoch: 135, train loss: 0.002540, val loss: 0.005228, val mae: 0.050361, val r2: 0.869016\n",
      "epoch: 136, train loss: 0.002512, val loss: 0.005213, val mae: 0.050303, val r2: 0.869295\n",
      "epoch: 137, train loss: 0.002482, val loss: 0.005302, val mae: 0.050854, val r2: 0.866973\n",
      "epoch: 138, train loss: 0.002464, val loss: 0.005287, val mae: 0.050870, val r2: 0.867268\n",
      "epoch: 139, train loss: 0.002449, val loss: 0.005312, val mae: 0.051029, val r2: 0.866590\n",
      "epoch: 140, train loss: 0.002446, val loss: 0.005338, val mae: 0.051189, val r2: 0.865782\n",
      "epoch: 141, train loss: 0.002439, val loss: 0.005486, val mae: 0.052044, val r2: 0.861834\n",
      "epoch: 142, train loss: 0.002440, val loss: 0.005615, val mae: 0.052807, val r2: 0.858377\n",
      "epoch: 143, train loss: 0.002442, val loss: 0.005665, val mae: 0.053083, val r2: 0.856864\n",
      "epoch: 144, train loss: 0.002446, val loss: 0.005920, val mae: 0.054450, val r2: 0.850039\n",
      "epoch: 145, train loss: 0.002464, val loss: 0.006068, val mae: 0.055290, val r2: 0.846102\n",
      "epoch: 146, train loss: 0.002465, val loss: 0.006274, val mae: 0.056320, val r2: 0.840712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 147, train loss: 0.002486, val loss: 0.006579, val mae: 0.057843, val r2: 0.832318\n",
      "epoch: 148, train loss: 0.002499, val loss: 0.006645, val mae: 0.058165, val r2: 0.830369\n",
      "epoch: 149, train loss: 0.002510, val loss: 0.006927, val mae: 0.059475, val r2: 0.823146\n",
      "epoch: 150, train loss: 0.002520, val loss: 0.007175, val mae: 0.060664, val r2: 0.816349\n",
      "epoch: 151, train loss: 0.002534, val loss: 0.007265, val mae: 0.061019, val r2: 0.814058\n",
      "epoch: 152, train loss: 0.002534, val loss: 0.007403, val mae: 0.061569, val r2: 0.810294\n",
      "epoch: 153, train loss: 0.002528, val loss: 0.007551, val mae: 0.062258, val r2: 0.806399\n",
      "epoch: 154, train loss: 0.002524, val loss: 0.007560, val mae: 0.062232, val r2: 0.806229\n",
      "epoch: 155, train loss: 0.002520, val loss: 0.007163, val mae: 0.060385, val r2: 0.816524\n",
      "epoch: 156, train loss: 0.002512, val loss: 0.006732, val mae: 0.058456, val r2: 0.827661\n",
      "epoch: 157, train loss: 0.002517, val loss: 0.006427, val mae: 0.056972, val r2: 0.835884\n",
      "epoch: 158, train loss: 0.002493, val loss: 0.006123, val mae: 0.055476, val r2: 0.844280\n",
      "epoch: 159, train loss: 0.002482, val loss: 0.005751, val mae: 0.053515, val r2: 0.854353\n",
      "epoch: 160, train loss: 0.002470, val loss: 0.005386, val mae: 0.051395, val r2: 0.864305\n",
      "epoch: 161, train loss: 0.002469, val loss: 0.005189, val mae: 0.050135, val r2: 0.869678\n",
      "epoch: 162, train loss: 0.002469, val loss: 0.005100, val mae: 0.049420, val r2: 0.872032\n",
      "epoch: 163, train loss: 0.002455, val loss: 0.005072, val mae: 0.049152, val r2: 0.872686\n",
      "epoch: 164, train loss: 0.002441, val loss: 0.005078, val mae: 0.049113, val r2: 0.872481\n",
      "epoch: 165, train loss: 0.002433, val loss: 0.005084, val mae: 0.049185, val r2: 0.872309\n",
      "epoch: 166, train loss: 0.002428, val loss: 0.005090, val mae: 0.049264, val r2: 0.872162\n",
      "epoch: 167, train loss: 0.002423, val loss: 0.005088, val mae: 0.049294, val r2: 0.872306\n",
      "epoch: 168, train loss: 0.002415, val loss: 0.005081, val mae: 0.049266, val r2: 0.872471\n",
      "epoch: 169, train loss: 0.002397, val loss: 0.005083, val mae: 0.049238, val r2: 0.872464\n",
      "epoch: 170, train loss: 0.002381, val loss: 0.005068, val mae: 0.049197, val r2: 0.872886\n",
      "epoch: 171, train loss: 0.002370, val loss: 0.005067, val mae: 0.049182, val r2: 0.872880\n",
      "epoch: 172, train loss: 0.002347, val loss: 0.005080, val mae: 0.049425, val r2: 0.872558\n",
      "epoch: 173, train loss: 0.002327, val loss: 0.005106, val mae: 0.049563, val r2: 0.871897\n",
      "epoch: 174, train loss: 0.002300, val loss: 0.005114, val mae: 0.049674, val r2: 0.871660\n",
      "epoch: 175, train loss: 0.002271, val loss: 0.005177, val mae: 0.050097, val r2: 0.870037\n",
      "epoch: 176, train loss: 0.002256, val loss: 0.005262, val mae: 0.050632, val r2: 0.867777\n",
      "epoch: 177, train loss: 0.002244, val loss: 0.005330, val mae: 0.051048, val r2: 0.865899\n",
      "epoch: 178, train loss: 0.002244, val loss: 0.005450, val mae: 0.051769, val r2: 0.862709\n",
      "epoch: 179, train loss: 0.002249, val loss: 0.005602, val mae: 0.052712, val r2: 0.858508\n",
      "epoch: 180, train loss: 0.002264, val loss: 0.005706, val mae: 0.053179, val r2: 0.855505\n",
      "epoch: 181, train loss: 0.002263, val loss: 0.005694, val mae: 0.053101, val r2: 0.855612\n",
      "epoch: 182, train loss: 0.002278, val loss: 0.005863, val mae: 0.054038, val r2: 0.851012\n",
      "epoch: 183, train loss: 0.002282, val loss: 0.006004, val mae: 0.054806, val r2: 0.847144\n",
      "epoch: 184, train loss: 0.002292, val loss: 0.006116, val mae: 0.055324, val r2: 0.844061\n",
      "epoch: 185, train loss: 0.002299, val loss: 0.006490, val mae: 0.057155, val r2: 0.834082\n",
      "epoch: 186, train loss: 0.002297, val loss: 0.006860, val mae: 0.058925, val r2: 0.824231\n",
      "epoch: 187, train loss: 0.002284, val loss: 0.007154, val mae: 0.060393, val r2: 0.816673\n",
      "epoch: 188, train loss: 0.002285, val loss: 0.006841, val mae: 0.058843, val r2: 0.825381\n",
      "epoch: 189, train loss: 0.002306, val loss: 0.005930, val mae: 0.054313, val r2: 0.849611\n",
      "epoch: 190, train loss: 0.002313, val loss: 0.005275, val mae: 0.050824, val r2: 0.867229\n",
      "epoch: 191, train loss: 0.002281, val loss: 0.005100, val mae: 0.049436, val r2: 0.872061\n",
      "epoch: 192, train loss: 0.002268, val loss: 0.005118, val mae: 0.049369, val r2: 0.871574\n",
      "epoch: 193, train loss: 0.002278, val loss: 0.005146, val mae: 0.049593, val r2: 0.870761\n",
      "epoch: 194, train loss: 0.002256, val loss: 0.005148, val mae: 0.049698, val r2: 0.870810\n",
      "epoch: 195, train loss: 0.002217, val loss: 0.005077, val mae: 0.049268, val r2: 0.872648\n",
      "epoch: 196, train loss: 0.002191, val loss: 0.005046, val mae: 0.048977, val r2: 0.873423\n",
      "epoch: 197, train loss: 0.002179, val loss: 0.005024, val mae: 0.048868, val r2: 0.874053\n",
      "epoch: 198, train loss: 0.002162, val loss: 0.005045, val mae: 0.048951, val r2: 0.873505\n",
      "epoch: 199, train loss: 0.002156, val loss: 0.005048, val mae: 0.049063, val r2: 0.873364\n",
      "epoch: 200, train loss: 0.002139, val loss: 0.005107, val mae: 0.049515, val r2: 0.871798\n",
      "epoch: 201, train loss: 0.002124, val loss: 0.005212, val mae: 0.050348, val r2: 0.869060\n",
      "epoch: 202, train loss: 0.002096, val loss: 0.005500, val mae: 0.052113, val r2: 0.861493\n",
      "epoch: 203, train loss: 0.002090, val loss: 0.005749, val mae: 0.053523, val r2: 0.854587\n",
      "epoch: 204, train loss: 0.002103, val loss: 0.005851, val mae: 0.053963, val r2: 0.851579\n",
      "epoch: 205, train loss: 0.002119, val loss: 0.005833, val mae: 0.053873, val r2: 0.851899\n",
      "epoch: 206, train loss: 0.002131, val loss: 0.005751, val mae: 0.053520, val r2: 0.853896\n",
      "epoch: 207, train loss: 0.002131, val loss: 0.005623, val mae: 0.052871, val r2: 0.857326\n",
      "epoch: 208, train loss: 0.002116, val loss: 0.005608, val mae: 0.052734, val r2: 0.857853\n",
      "epoch: 209, train loss: 0.002110, val loss: 0.005631, val mae: 0.052900, val r2: 0.857194\n",
      "epoch: 210, train loss: 0.002113, val loss: 0.005691, val mae: 0.053192, val r2: 0.855674\n",
      "epoch: 211, train loss: 0.002117, val loss: 0.005725, val mae: 0.053408, val r2: 0.854950\n",
      "epoch: 212, train loss: 0.002110, val loss: 0.005453, val mae: 0.051874, val r2: 0.862620\n",
      "epoch: 213, train loss: 0.002120, val loss: 0.005186, val mae: 0.050145, val r2: 0.869708\n",
      "epoch: 214, train loss: 0.002117, val loss: 0.005088, val mae: 0.049324, val r2: 0.872413\n",
      "epoch: 215, train loss: 0.002091, val loss: 0.005209, val mae: 0.049582, val r2: 0.869330\n",
      "epoch: 216, train loss: 0.002080, val loss: 0.005258, val mae: 0.049798, val r2: 0.867995\n",
      "epoch: 217, train loss: 0.002106, val loss: 0.005223, val mae: 0.049765, val r2: 0.868774\n",
      "epoch: 218, train loss: 0.002146, val loss: 0.005165, val mae: 0.049695, val r2: 0.870302\n",
      "epoch: 219, train loss: 0.002113, val loss: 0.005311, val mae: 0.050824, val r2: 0.866681\n",
      "epoch: 220, train loss: 0.002050, val loss: 0.005379, val mae: 0.051363, val r2: 0.864954\n",
      "epoch: 221, train loss: 0.002047, val loss: 0.005320, val mae: 0.051047, val r2: 0.866094\n",
      "epoch: 222, train loss: 0.002074, val loss: 0.005140, val mae: 0.049954, val r2: 0.870865\n",
      "epoch: 223, train loss: 0.002036, val loss: 0.005143, val mae: 0.049768, val r2: 0.870928\n",
      "epoch: 224, train loss: 0.001999, val loss: 0.005229, val mae: 0.050302, val r2: 0.868462\n",
      "epoch: 225, train loss: 0.002009, val loss: 0.005384, val mae: 0.051311, val r2: 0.864299\n",
      "epoch: 226, train loss: 0.001994, val loss: 0.005659, val mae: 0.052937, val r2: 0.857031\n",
      "epoch: 227, train loss: 0.001982, val loss: 0.005757, val mae: 0.053628, val r2: 0.854234\n",
      "epoch: 228, train loss: 0.002012, val loss: 0.005514, val mae: 0.052159, val r2: 0.860656\n",
      "epoch: 229, train loss: 0.002033, val loss: 0.005243, val mae: 0.050630, val r2: 0.868014\n",
      "epoch: 230, train loss: 0.001994, val loss: 0.005171, val mae: 0.049983, val r2: 0.870348\n",
      "epoch: 231, train loss: 0.001959, val loss: 0.005194, val mae: 0.049910, val r2: 0.869692\n",
      "epoch: 232, train loss: 0.001971, val loss: 0.005200, val mae: 0.050137, val r2: 0.869300\n",
      "epoch: 233, train loss: 0.001974, val loss: 0.005256, val mae: 0.050629, val r2: 0.867859\n",
      "epoch: 234, train loss: 0.001953, val loss: 0.005199, val mae: 0.050260, val r2: 0.869455\n",
      "epoch: 235, train loss: 0.001952, val loss: 0.005177, val mae: 0.050055, val r2: 0.870036\n",
      "epoch: 236, train loss: 0.001954, val loss: 0.005081, val mae: 0.049399, val r2: 0.872544\n",
      "epoch: 237, train loss: 0.001935, val loss: 0.005143, val mae: 0.049435, val r2: 0.871143\n",
      "epoch: 238, train loss: 0.001932, val loss: 0.005169, val mae: 0.049494, val r2: 0.870257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 239, train loss: 0.001949, val loss: 0.005159, val mae: 0.049706, val r2: 0.870396\n",
      "epoch: 240, train loss: 0.001936, val loss: 0.005231, val mae: 0.050312, val r2: 0.868488\n",
      "epoch: 241, train loss: 0.001919, val loss: 0.005386, val mae: 0.051385, val r2: 0.864499\n",
      "epoch: 242, train loss: 0.001913, val loss: 0.005508, val mae: 0.052109, val r2: 0.861113\n",
      "epoch: 243, train loss: 0.001933, val loss: 0.005439, val mae: 0.051603, val r2: 0.862734\n",
      "epoch: 244, train loss: 0.001944, val loss: 0.005303, val mae: 0.050830, val r2: 0.866378\n",
      "epoch: 245, train loss: 0.001917, val loss: 0.005236, val mae: 0.050384, val r2: 0.868404\n",
      "epoch: 246, train loss: 0.001901, val loss: 0.005164, val mae: 0.049932, val r2: 0.870205\n",
      "epoch: 247, train loss: 0.001904, val loss: 0.005240, val mae: 0.050581, val r2: 0.868156\n",
      "epoch: 248, train loss: 0.001894, val loss: 0.005345, val mae: 0.051180, val r2: 0.865328\n",
      "epoch: 249, train loss: 0.001889, val loss: 0.005444, val mae: 0.051794, val r2: 0.862680\n",
      "epoch: 250, train loss: 0.001895, val loss: 0.005525, val mae: 0.052208, val r2: 0.860827\n",
      "epoch: 251, train loss: 0.001893, val loss: 0.005422, val mae: 0.051562, val r2: 0.863523\n",
      "epoch: 252, train loss: 0.001903, val loss: 0.005183, val mae: 0.050151, val r2: 0.869787\n",
      "epoch: 253, train loss: 0.001890, val loss: 0.005148, val mae: 0.049699, val r2: 0.870927\n",
      "epoch: 254, train loss: 0.001869, val loss: 0.005197, val mae: 0.049973, val r2: 0.869648\n",
      "epoch: 255, train loss: 0.001870, val loss: 0.005201, val mae: 0.050073, val r2: 0.869372\n",
      "epoch: 256, train loss: 0.001869, val loss: 0.005291, val mae: 0.050752, val r2: 0.867043\n",
      "epoch: 257, train loss: 0.001861, val loss: 0.005429, val mae: 0.051531, val r2: 0.863645\n",
      "epoch: 258, train loss: 0.001855, val loss: 0.005391, val mae: 0.051338, val r2: 0.864623\n",
      "epoch: 259, train loss: 0.001878, val loss: 0.005243, val mae: 0.050576, val r2: 0.868221\n",
      "epoch: 260, train loss: 0.001870, val loss: 0.005137, val mae: 0.049883, val r2: 0.871081\n",
      "epoch: 261, train loss: 0.001839, val loss: 0.005174, val mae: 0.049777, val r2: 0.870199\n",
      "epoch: 262, train loss: 0.001833, val loss: 0.005216, val mae: 0.050072, val r2: 0.868957\n",
      "epoch: 263, train loss: 0.001846, val loss: 0.005330, val mae: 0.050832, val r2: 0.866087\n",
      "epoch: 264, train loss: 0.001819, val loss: 0.005470, val mae: 0.051632, val r2: 0.862559\n",
      "epoch: 265, train loss: 0.001805, val loss: 0.005548, val mae: 0.052205, val r2: 0.860318\n",
      "epoch: 266, train loss: 0.001809, val loss: 0.005557, val mae: 0.052517, val r2: 0.859995\n",
      "epoch: 267, train loss: 0.001809, val loss: 0.005417, val mae: 0.051682, val r2: 0.863444\n",
      "epoch: 268, train loss: 0.001801, val loss: 0.005293, val mae: 0.050848, val r2: 0.866700\n",
      "epoch: 269, train loss: 0.001791, val loss: 0.005264, val mae: 0.050587, val r2: 0.867473\n",
      "epoch: 270, train loss: 0.001791, val loss: 0.005347, val mae: 0.051041, val r2: 0.865386\n",
      "epoch: 271, train loss: 0.001785, val loss: 0.005508, val mae: 0.052018, val r2: 0.861120\n",
      "epoch: 272, train loss: 0.001786, val loss: 0.005593, val mae: 0.052569, val r2: 0.858932\n",
      "epoch: 273, train loss: 0.001791, val loss: 0.005693, val mae: 0.053219, val r2: 0.856435\n",
      "epoch: 274, train loss: 0.001803, val loss: 0.005558, val mae: 0.052571, val r2: 0.860022\n",
      "epoch: 275, train loss: 0.001809, val loss: 0.005385, val mae: 0.051546, val r2: 0.864457\n",
      "epoch: 276, train loss: 0.001804, val loss: 0.005285, val mae: 0.050806, val r2: 0.867039\n",
      "epoch: 277, train loss: 0.001788, val loss: 0.005353, val mae: 0.051137, val r2: 0.865290\n",
      "epoch: 278, train loss: 0.001783, val loss: 0.005409, val mae: 0.051264, val r2: 0.863956\n",
      "epoch: 279, train loss: 0.001785, val loss: 0.005541, val mae: 0.052050, val r2: 0.860609\n",
      "epoch: 280, train loss: 0.001780, val loss: 0.005674, val mae: 0.052741, val r2: 0.857383\n",
      "epoch: 281, train loss: 0.001788, val loss: 0.005755, val mae: 0.053340, val r2: 0.855406\n",
      "epoch: 282, train loss: 0.001810, val loss: 0.005633, val mae: 0.052750, val r2: 0.858192\n",
      "epoch: 283, train loss: 0.001817, val loss: 0.005459, val mae: 0.051743, val r2: 0.862463\n",
      "epoch: 284, train loss: 0.001802, val loss: 0.005464, val mae: 0.051404, val r2: 0.862339\n",
      "epoch: 285, train loss: 0.001798, val loss: 0.005594, val mae: 0.052267, val r2: 0.859040\n",
      "epoch: 286, train loss: 0.001799, val loss: 0.006011, val mae: 0.054379, val r2: 0.848529\n",
      "epoch: 287, train loss: 0.001754, val loss: 0.006370, val mae: 0.056365, val r2: 0.839574\n",
      "epoch: 288, train loss: 0.001736, val loss: 0.006401, val mae: 0.056550, val r2: 0.838134\n",
      "epoch: 289, train loss: 0.001757, val loss: 0.006087, val mae: 0.055050, val r2: 0.845707\n",
      "epoch: 290, train loss: 0.001765, val loss: 0.005860, val mae: 0.053917, val r2: 0.851569\n",
      "epoch: 291, train loss: 0.001752, val loss: 0.005954, val mae: 0.054403, val r2: 0.849073\n",
      "epoch: 292, train loss: 0.001744, val loss: 0.006193, val mae: 0.055812, val r2: 0.842973\n",
      "epoch: 293, train loss: 0.001734, val loss: 0.006397, val mae: 0.056816, val r2: 0.837669\n",
      "epoch: 294, train loss: 0.001734, val loss: 0.006048, val mae: 0.054922, val r2: 0.846998\n",
      "epoch: 295, train loss: 0.001740, val loss: 0.005660, val mae: 0.052718, val r2: 0.857116\n",
      "epoch: 296, train loss: 0.001737, val loss: 0.005387, val mae: 0.051098, val r2: 0.864326\n",
      "epoch: 297, train loss: 0.001750, val loss: 0.005325, val mae: 0.050687, val r2: 0.866218\n",
      "epoch: 298, train loss: 0.001766, val loss: 0.005478, val mae: 0.051522, val r2: 0.862523\n",
      "epoch: 299, train loss: 0.001770, val loss: 0.005567, val mae: 0.051893, val r2: 0.860527\n",
      "epoch: 300, train loss: 0.001781, val loss: 0.005613, val mae: 0.052022, val r2: 0.859589\n",
      "epoch: 301, train loss: 0.001791, val loss: 0.005521, val mae: 0.051493, val r2: 0.861643\n",
      "epoch: 302, train loss: 0.001778, val loss: 0.005362, val mae: 0.050621, val r2: 0.865524\n",
      "epoch: 303, train loss: 0.001758, val loss: 0.005369, val mae: 0.050734, val r2: 0.865224\n",
      "epoch: 304, train loss: 0.001756, val loss: 0.005666, val mae: 0.052585, val r2: 0.857762\n",
      "epoch: 305, train loss: 0.001753, val loss: 0.006146, val mae: 0.054895, val r2: 0.845726\n",
      "epoch: 306, train loss: 0.001729, val loss: 0.006531, val mae: 0.056824, val r2: 0.835572\n",
      "epoch: 307, train loss: 0.001722, val loss: 0.006677, val mae: 0.057508, val r2: 0.831049\n",
      "epoch: 308, train loss: 0.001728, val loss: 0.006576, val mae: 0.057078, val r2: 0.833253\n",
      "epoch: 309, train loss: 0.001721, val loss: 0.006356, val mae: 0.055967, val r2: 0.838420\n",
      "epoch: 310, train loss: 0.001720, val loss: 0.006284, val mae: 0.055653, val r2: 0.840455\n",
      "epoch: 311, train loss: 0.001712, val loss: 0.006243, val mae: 0.055394, val r2: 0.841603\n",
      "epoch: 312, train loss: 0.001721, val loss: 0.006154, val mae: 0.054907, val r2: 0.844045\n",
      "epoch: 313, train loss: 0.001717, val loss: 0.005911, val mae: 0.053543, val r2: 0.850513\n",
      "epoch: 314, train loss: 0.001715, val loss: 0.005715, val mae: 0.052317, val r2: 0.855802\n",
      "epoch: 315, train loss: 0.001713, val loss: 0.005648, val mae: 0.051946, val r2: 0.857573\n",
      "epoch: 316, train loss: 0.001707, val loss: 0.005591, val mae: 0.051677, val r2: 0.859354\n",
      "epoch: 317, train loss: 0.001703, val loss: 0.005622, val mae: 0.051829, val r2: 0.858715\n",
      "epoch: 318, train loss: 0.001694, val loss: 0.005613, val mae: 0.051786, val r2: 0.859160\n",
      "epoch: 319, train loss: 0.001686, val loss: 0.005609, val mae: 0.051837, val r2: 0.859381\n",
      "epoch: 320, train loss: 0.001679, val loss: 0.005720, val mae: 0.052541, val r2: 0.856490\n",
      "epoch: 321, train loss: 0.001690, val loss: 0.005723, val mae: 0.052605, val r2: 0.856294\n",
      "epoch: 322, train loss: 0.001694, val loss: 0.005817, val mae: 0.053274, val r2: 0.853652\n",
      "epoch: 323, train loss: 0.001694, val loss: 0.006054, val mae: 0.054640, val r2: 0.847450\n",
      "epoch: 324, train loss: 0.001685, val loss: 0.006293, val mae: 0.055944, val r2: 0.841236\n",
      "epoch: 325, train loss: 0.001676, val loss: 0.006511, val mae: 0.056956, val r2: 0.835545\n",
      "epoch: 326, train loss: 0.001675, val loss: 0.006298, val mae: 0.055808, val r2: 0.840536\n",
      "epoch: 327, train loss: 0.001672, val loss: 0.005852, val mae: 0.053377, val r2: 0.852298\n",
      "epoch: 328, train loss: 0.001663, val loss: 0.005580, val mae: 0.051767, val r2: 0.859532\n",
      "epoch: 329, train loss: 0.001658, val loss: 0.005476, val mae: 0.051126, val r2: 0.862309\n",
      "epoch: 330, train loss: 0.001663, val loss: 0.005543, val mae: 0.051496, val r2: 0.860655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 331, train loss: 0.001672, val loss: 0.005796, val mae: 0.052772, val r2: 0.854295\n",
      "epoch: 332, train loss: 0.001659, val loss: 0.006080, val mae: 0.054240, val r2: 0.847087\n",
      "epoch: 333, train loss: 0.001653, val loss: 0.006248, val mae: 0.055078, val r2: 0.842530\n",
      "epoch: 334, train loss: 0.001665, val loss: 0.005938, val mae: 0.053482, val r2: 0.850135\n",
      "epoch: 335, train loss: 0.001660, val loss: 0.005663, val mae: 0.052076, val r2: 0.857308\n",
      "epoch: 336, train loss: 0.001670, val loss: 0.005602, val mae: 0.051792, val r2: 0.858981\n",
      "epoch: 337, train loss: 0.001702, val loss: 0.005898, val mae: 0.053415, val r2: 0.851441\n",
      "epoch: 338, train loss: 0.001681, val loss: 0.006229, val mae: 0.055190, val r2: 0.842935\n",
      "epoch: 339, train loss: 0.001655, val loss: 0.005877, val mae: 0.053549, val r2: 0.851737\n",
      "epoch: 340, train loss: 0.001671, val loss: 0.005516, val mae: 0.051582, val r2: 0.861209\n",
      "epoch: 341, train loss: 0.001658, val loss: 0.005464, val mae: 0.051305, val r2: 0.862798\n",
      "epoch: 342, train loss: 0.001661, val loss: 0.005517, val mae: 0.051617, val r2: 0.861413\n",
      "epoch: 343, train loss: 0.001665, val loss: 0.005557, val mae: 0.052039, val r2: 0.860440\n",
      "epoch: 344, train loss: 0.001641, val loss: 0.005463, val mae: 0.051495, val r2: 0.862886\n",
      "epoch: 345, train loss: 0.001631, val loss: 0.005358, val mae: 0.050648, val r2: 0.865625\n",
      "epoch: 346, train loss: 0.001620, val loss: 0.005346, val mae: 0.050628, val r2: 0.865894\n",
      "epoch: 347, train loss: 0.001615, val loss: 0.005411, val mae: 0.051146, val r2: 0.864250\n",
      "epoch: 348, train loss: 0.001604, val loss: 0.005485, val mae: 0.051605, val r2: 0.862369\n",
      "epoch: 349, train loss: 0.001587, val loss: 0.005581, val mae: 0.052259, val r2: 0.859679\n"
     ]
    }
   ],
   "source": [
    "# *\n",
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 32,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 1,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 6, \n",
    "                      'epoches': 350,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52af9682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps!\n",
      "Transformer(\n",
      "  (element_embedding_temporal): Linear(in_features=294, out_features=192, bias=True)\n",
      "  (element_embedding_spatial): Linear(in_features=2695, out_features=1760, bias=True)\n",
      "  (pos_embedding): Embedding(6, 32)\n",
      "  (variable_embedding): Embedding(55, 32)\n",
      "  (temporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatial): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatiotemporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (keys): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (queries): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatter): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=11220, out_features=5280, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=5280, out_features=2640, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.01)\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=2640, out_features=330, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch: 0, train loss: 0.016915, val loss: 0.012978, val mae: 0.084049, val r2: 0.674465\n",
      "epoch: 1, train loss: 0.009433, val loss: 0.011204, val mae: 0.077217, val r2: 0.717430\n",
      "epoch: 2, train loss: 0.008549, val loss: 0.010632, val mae: 0.075091, val r2: 0.730900\n",
      "epoch: 3, train loss: 0.008084, val loss: 0.010265, val mae: 0.073685, val r2: 0.740055\n",
      "epoch: 4, train loss: 0.007697, val loss: 0.009870, val mae: 0.072106, val r2: 0.750133\n",
      "epoch: 5, train loss: 0.007394, val loss: 0.009499, val mae: 0.070864, val r2: 0.759569\n",
      "epoch: 6, train loss: 0.007232, val loss: 0.009312, val mae: 0.070364, val r2: 0.764348\n",
      "epoch: 7, train loss: 0.007170, val loss: 0.009595, val mae: 0.072068, val r2: 0.757158\n",
      "epoch: 8, train loss: 0.007191, val loss: 0.010509, val mae: 0.076172, val r2: 0.734072\n",
      "epoch: 9, train loss: 0.007126, val loss: 0.010796, val mae: 0.077250, val r2: 0.727511\n",
      "epoch: 10, train loss: 0.006854, val loss: 0.010520, val mae: 0.075875, val r2: 0.734744\n",
      "epoch: 11, train loss: 0.006466, val loss: 0.010251, val mae: 0.074620, val r2: 0.741929\n",
      "epoch: 12, train loss: 0.006049, val loss: 0.009709, val mae: 0.072181, val r2: 0.755841\n",
      "epoch: 13, train loss: 0.005655, val loss: 0.009340, val mae: 0.070418, val r2: 0.765393\n",
      "epoch: 14, train loss: 0.005335, val loss: 0.008900, val mae: 0.068415, val r2: 0.776769\n",
      "epoch: 15, train loss: 0.005077, val loss: 0.008706, val mae: 0.067437, val r2: 0.781805\n",
      "epoch: 16, train loss: 0.004876, val loss: 0.008446, val mae: 0.066202, val r2: 0.788409\n",
      "epoch: 17, train loss: 0.004692, val loss: 0.008189, val mae: 0.065009, val r2: 0.795138\n",
      "epoch: 18, train loss: 0.004542, val loss: 0.008033, val mae: 0.064226, val r2: 0.799058\n",
      "epoch: 19, train loss: 0.004423, val loss: 0.008007, val mae: 0.064091, val r2: 0.799857\n",
      "epoch: 20, train loss: 0.004320, val loss: 0.007787, val mae: 0.063078, val r2: 0.805359\n",
      "epoch: 21, train loss: 0.004236, val loss: 0.007716, val mae: 0.062674, val r2: 0.807272\n",
      "epoch: 22, train loss: 0.004155, val loss: 0.007735, val mae: 0.062709, val r2: 0.806918\n",
      "epoch: 23, train loss: 0.004088, val loss: 0.007671, val mae: 0.062439, val r2: 0.808509\n",
      "epoch: 24, train loss: 0.004024, val loss: 0.007559, val mae: 0.061902, val r2: 0.811327\n",
      "epoch: 25, train loss: 0.003968, val loss: 0.007519, val mae: 0.061728, val r2: 0.812359\n",
      "epoch: 26, train loss: 0.003912, val loss: 0.007433, val mae: 0.061263, val r2: 0.814522\n",
      "epoch: 27, train loss: 0.003861, val loss: 0.007481, val mae: 0.061495, val r2: 0.813400\n",
      "epoch: 28, train loss: 0.003813, val loss: 0.007365, val mae: 0.060900, val r2: 0.816337\n",
      "epoch: 29, train loss: 0.003772, val loss: 0.007264, val mae: 0.060405, val r2: 0.818825\n",
      "epoch: 30, train loss: 0.003733, val loss: 0.007180, val mae: 0.059979, val r2: 0.820860\n",
      "epoch: 31, train loss: 0.003697, val loss: 0.007185, val mae: 0.059992, val r2: 0.820908\n",
      "epoch: 32, train loss: 0.003665, val loss: 0.007155, val mae: 0.059861, val r2: 0.821551\n",
      "epoch: 33, train loss: 0.003636, val loss: 0.006981, val mae: 0.058938, val r2: 0.825894\n",
      "epoch: 34, train loss: 0.003591, val loss: 0.007149, val mae: 0.059760, val r2: 0.821847\n",
      "epoch: 35, train loss: 0.003582, val loss: 0.006926, val mae: 0.058675, val r2: 0.827351\n",
      "epoch: 36, train loss: 0.003552, val loss: 0.006958, val mae: 0.058827, val r2: 0.826567\n",
      "epoch: 37, train loss: 0.003515, val loss: 0.006874, val mae: 0.058445, val r2: 0.828662\n",
      "epoch: 38, train loss: 0.003490, val loss: 0.006848, val mae: 0.058304, val r2: 0.829340\n",
      "epoch: 39, train loss: 0.003457, val loss: 0.006886, val mae: 0.058549, val r2: 0.828579\n",
      "epoch: 40, train loss: 0.003434, val loss: 0.006869, val mae: 0.058487, val r2: 0.828879\n",
      "epoch: 41, train loss: 0.003415, val loss: 0.006714, val mae: 0.057675, val r2: 0.832759\n",
      "epoch: 42, train loss: 0.003374, val loss: 0.006824, val mae: 0.058366, val r2: 0.830151\n",
      "epoch: 43, train loss: 0.003362, val loss: 0.006905, val mae: 0.058705, val r2: 0.828265\n",
      "epoch: 44, train loss: 0.003319, val loss: 0.006937, val mae: 0.058884, val r2: 0.827439\n",
      "epoch: 45, train loss: 0.003303, val loss: 0.006803, val mae: 0.058195, val r2: 0.830778\n",
      "epoch: 46, train loss: 0.003306, val loss: 0.006737, val mae: 0.057873, val r2: 0.832405\n",
      "epoch: 47, train loss: 0.003285, val loss: 0.006700, val mae: 0.057798, val r2: 0.833183\n",
      "epoch: 48, train loss: 0.003249, val loss: 0.006654, val mae: 0.057470, val r2: 0.834381\n",
      "epoch: 49, train loss: 0.003256, val loss: 0.007049, val mae: 0.059638, val r2: 0.824701\n",
      "epoch: 50, train loss: 0.003292, val loss: 0.006882, val mae: 0.058735, val r2: 0.828871\n",
      "epoch: 51, train loss: 0.003315, val loss: 0.007004, val mae: 0.059454, val r2: 0.825601\n",
      "epoch: 52, train loss: 0.003351, val loss: 0.007240, val mae: 0.060702, val r2: 0.819820\n",
      "epoch: 53, train loss: 0.003397, val loss: 0.007173, val mae: 0.060438, val r2: 0.821186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 54, train loss: 0.003427, val loss: 0.007077, val mae: 0.059921, val r2: 0.823587\n",
      "epoch: 55, train loss: 0.003416, val loss: 0.007359, val mae: 0.061116, val r2: 0.816803\n",
      "epoch: 56, train loss: 0.003323, val loss: 0.007471, val mae: 0.061419, val r2: 0.814141\n",
      "epoch: 57, train loss: 0.003242, val loss: 0.007171, val mae: 0.060048, val r2: 0.821452\n",
      "epoch: 58, train loss: 0.003187, val loss: 0.007385, val mae: 0.061065, val r2: 0.816085\n",
      "epoch: 59, train loss: 0.003185, val loss: 0.007224, val mae: 0.060155, val r2: 0.820131\n",
      "epoch: 60, train loss: 0.003181, val loss: 0.006968, val mae: 0.058872, val r2: 0.826406\n",
      "epoch: 61, train loss: 0.003160, val loss: 0.006862, val mae: 0.058340, val r2: 0.829005\n",
      "epoch: 62, train loss: 0.003119, val loss: 0.007034, val mae: 0.059268, val r2: 0.824922\n",
      "epoch: 63, train loss: 0.003107, val loss: 0.007149, val mae: 0.059709, val r2: 0.822286\n",
      "epoch: 64, train loss: 0.003083, val loss: 0.007054, val mae: 0.059502, val r2: 0.824430\n",
      "epoch: 65, train loss: 0.003066, val loss: 0.007076, val mae: 0.059655, val r2: 0.824083\n",
      "epoch: 66, train loss: 0.003035, val loss: 0.006940, val mae: 0.059104, val r2: 0.827462\n",
      "epoch: 67, train loss: 0.003010, val loss: 0.007201, val mae: 0.060422, val r2: 0.821095\n",
      "epoch: 68, train loss: 0.002997, val loss: 0.007204, val mae: 0.060475, val r2: 0.820840\n",
      "epoch: 69, train loss: 0.002990, val loss: 0.007355, val mae: 0.061268, val r2: 0.817187\n",
      "epoch: 70, train loss: 0.002990, val loss: 0.007155, val mae: 0.060345, val r2: 0.821756\n",
      "epoch: 71, train loss: 0.002985, val loss: 0.007209, val mae: 0.060656, val r2: 0.820384\n",
      "epoch: 72, train loss: 0.002991, val loss: 0.007161, val mae: 0.060412, val r2: 0.821738\n",
      "epoch: 73, train loss: 0.002970, val loss: 0.007721, val mae: 0.063110, val r2: 0.807902\n",
      "epoch: 74, train loss: 0.002956, val loss: 0.008111, val mae: 0.064993, val r2: 0.798271\n",
      "epoch: 75, train loss: 0.002940, val loss: 0.008067, val mae: 0.064610, val r2: 0.799616\n",
      "epoch: 76, train loss: 0.002965, val loss: 0.007533, val mae: 0.062056, val r2: 0.812899\n",
      "epoch: 77, train loss: 0.002971, val loss: 0.007975, val mae: 0.064011, val r2: 0.802205\n",
      "epoch: 78, train loss: 0.002963, val loss: 0.008389, val mae: 0.065893, val r2: 0.792064\n",
      "epoch: 79, train loss: 0.002955, val loss: 0.008621, val mae: 0.067000, val r2: 0.786195\n",
      "epoch: 80, train loss: 0.002952, val loss: 0.009205, val mae: 0.069431, val r2: 0.771741\n",
      "epoch: 81, train loss: 0.002970, val loss: 0.010105, val mae: 0.073076, val r2: 0.748909\n",
      "epoch: 82, train loss: 0.003025, val loss: 0.009837, val mae: 0.071979, val r2: 0.755884\n",
      "epoch: 83, train loss: 0.003140, val loss: 0.008376, val mae: 0.065612, val r2: 0.791789\n",
      "epoch: 84, train loss: 0.003270, val loss: 0.006350, val mae: 0.056212, val r2: 0.841398\n",
      "epoch: 85, train loss: 0.003340, val loss: 0.005735, val mae: 0.052532, val r2: 0.856373\n",
      "epoch: 86, train loss: 0.003235, val loss: 0.005917, val mae: 0.052996, val r2: 0.851710\n",
      "epoch: 87, train loss: 0.003171, val loss: 0.006229, val mae: 0.054312, val r2: 0.843914\n",
      "epoch: 88, train loss: 0.003085, val loss: 0.006020, val mae: 0.053262, val r2: 0.848850\n",
      "epoch: 89, train loss: 0.003016, val loss: 0.005826, val mae: 0.052344, val r2: 0.853422\n",
      "epoch: 90, train loss: 0.003045, val loss: 0.005784, val mae: 0.052196, val r2: 0.854363\n",
      "epoch: 91, train loss: 0.003017, val loss: 0.005770, val mae: 0.052450, val r2: 0.855157\n",
      "epoch: 92, train loss: 0.002931, val loss: 0.005768, val mae: 0.052546, val r2: 0.855281\n",
      "epoch: 93, train loss: 0.002868, val loss: 0.005711, val mae: 0.052407, val r2: 0.856776\n",
      "epoch: 94, train loss: 0.002867, val loss: 0.005600, val mae: 0.051774, val r2: 0.859476\n",
      "epoch: 95, train loss: 0.002890, val loss: 0.005518, val mae: 0.051367, val r2: 0.861454\n",
      "epoch: 96, train loss: 0.002874, val loss: 0.005572, val mae: 0.051372, val r2: 0.860041\n",
      "epoch: 97, train loss: 0.002821, val loss: 0.005706, val mae: 0.051872, val r2: 0.856745\n",
      "epoch: 98, train loss: 0.002772, val loss: 0.005773, val mae: 0.052118, val r2: 0.854802\n",
      "epoch: 99, train loss: 0.002735, val loss: 0.005817, val mae: 0.052375, val r2: 0.853580\n",
      "epoch: 100, train loss: 0.002715, val loss: 0.005866, val mae: 0.052604, val r2: 0.852205\n",
      "epoch: 101, train loss: 0.002710, val loss: 0.005921, val mae: 0.052899, val r2: 0.850657\n",
      "epoch: 102, train loss: 0.002685, val loss: 0.005838, val mae: 0.052564, val r2: 0.852956\n",
      "epoch: 103, train loss: 0.002669, val loss: 0.005808, val mae: 0.052549, val r2: 0.853863\n",
      "epoch: 104, train loss: 0.002651, val loss: 0.005766, val mae: 0.052510, val r2: 0.854972\n",
      "epoch: 105, train loss: 0.002633, val loss: 0.005697, val mae: 0.052302, val r2: 0.856785\n",
      "epoch: 106, train loss: 0.002632, val loss: 0.005690, val mae: 0.052407, val r2: 0.856950\n",
      "epoch: 107, train loss: 0.002623, val loss: 0.005653, val mae: 0.052217, val r2: 0.857854\n",
      "epoch: 108, train loss: 0.002635, val loss: 0.005669, val mae: 0.052243, val r2: 0.857395\n",
      "epoch: 109, train loss: 0.002634, val loss: 0.005642, val mae: 0.052094, val r2: 0.858101\n",
      "epoch: 110, train loss: 0.002636, val loss: 0.005666, val mae: 0.051956, val r2: 0.857409\n",
      "epoch: 111, train loss: 0.002628, val loss: 0.005827, val mae: 0.052582, val r2: 0.853394\n",
      "epoch: 112, train loss: 0.002600, val loss: 0.005869, val mae: 0.052590, val r2: 0.852380\n",
      "epoch: 113, train loss: 0.002627, val loss: 0.006162, val mae: 0.053775, val r2: 0.844537\n",
      "epoch: 114, train loss: 0.002627, val loss: 0.006235, val mae: 0.053947, val r2: 0.842573\n",
      "epoch: 115, train loss: 0.002648, val loss: 0.006223, val mae: 0.053909, val r2: 0.842845\n",
      "epoch: 116, train loss: 0.002637, val loss: 0.006105, val mae: 0.053490, val r2: 0.846242\n",
      "epoch: 117, train loss: 0.002616, val loss: 0.006220, val mae: 0.054377, val r2: 0.843584\n",
      "epoch: 118, train loss: 0.002570, val loss: 0.006322, val mae: 0.055207, val r2: 0.841030\n",
      "epoch: 119, train loss: 0.002560, val loss: 0.006575, val mae: 0.056646, val r2: 0.834966\n",
      "epoch: 120, train loss: 0.002552, val loss: 0.006497, val mae: 0.056650, val r2: 0.836539\n",
      "epoch: 121, train loss: 0.002566, val loss: 0.006187, val mae: 0.055214, val r2: 0.844134\n",
      "epoch: 122, train loss: 0.002585, val loss: 0.006042, val mae: 0.054490, val r2: 0.847400\n",
      "epoch: 123, train loss: 0.002583, val loss: 0.005863, val mae: 0.053453, val r2: 0.851816\n",
      "epoch: 124, train loss: 0.002579, val loss: 0.005911, val mae: 0.053503, val r2: 0.850840\n",
      "epoch: 125, train loss: 0.002598, val loss: 0.006125, val mae: 0.054361, val r2: 0.845539\n",
      "epoch: 126, train loss: 0.002632, val loss: 0.006489, val mae: 0.056127, val r2: 0.836509\n",
      "epoch: 127, train loss: 0.002571, val loss: 0.007036, val mae: 0.059185, val r2: 0.822604\n",
      "epoch: 128, train loss: 0.002531, val loss: 0.007411, val mae: 0.061336, val r2: 0.811831\n",
      "epoch: 129, train loss: 0.002489, val loss: 0.007234, val mae: 0.060491, val r2: 0.815586\n",
      "epoch: 130, train loss: 0.002489, val loss: 0.006862, val mae: 0.058821, val r2: 0.824941\n",
      "epoch: 131, train loss: 0.002478, val loss: 0.006634, val mae: 0.057624, val r2: 0.830759\n",
      "epoch: 132, train loss: 0.002454, val loss: 0.006604, val mae: 0.057426, val r2: 0.831720\n",
      "epoch: 133, train loss: 0.002432, val loss: 0.006706, val mae: 0.057858, val r2: 0.829260\n",
      "epoch: 134, train loss: 0.002410, val loss: 0.007038, val mae: 0.059426, val r2: 0.821050\n",
      "epoch: 135, train loss: 0.002393, val loss: 0.007254, val mae: 0.060514, val r2: 0.815476\n",
      "epoch: 136, train loss: 0.002377, val loss: 0.007202, val mae: 0.060294, val r2: 0.816792\n",
      "epoch: 137, train loss: 0.002360, val loss: 0.007208, val mae: 0.060295, val r2: 0.816351\n",
      "epoch: 138, train loss: 0.002348, val loss: 0.006761, val mae: 0.058231, val r2: 0.827899\n",
      "epoch: 139, train loss: 0.002338, val loss: 0.006422, val mae: 0.056524, val r2: 0.836817\n",
      "epoch: 140, train loss: 0.002323, val loss: 0.006384, val mae: 0.056222, val r2: 0.837823\n",
      "epoch: 141, train loss: 0.002311, val loss: 0.006332, val mae: 0.055824, val r2: 0.839294\n",
      "epoch: 142, train loss: 0.002309, val loss: 0.006547, val mae: 0.056746, val r2: 0.833943\n",
      "epoch: 143, train loss: 0.002300, val loss: 0.006610, val mae: 0.057141, val r2: 0.832390\n",
      "epoch: 144, train loss: 0.002297, val loss: 0.006885, val mae: 0.058630, val r2: 0.825158\n",
      "epoch: 145, train loss: 0.002286, val loss: 0.007224, val mae: 0.060260, val r2: 0.816507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 146, train loss: 0.002273, val loss: 0.007165, val mae: 0.059944, val r2: 0.818123\n",
      "epoch: 147, train loss: 0.002258, val loss: 0.007060, val mae: 0.059372, val r2: 0.820870\n",
      "epoch: 148, train loss: 0.002244, val loss: 0.006512, val mae: 0.056755, val r2: 0.834895\n",
      "epoch: 149, train loss: 0.002236, val loss: 0.006134, val mae: 0.054867, val r2: 0.845063\n",
      "epoch: 150, train loss: 0.002233, val loss: 0.005922, val mae: 0.053633, val r2: 0.850649\n",
      "epoch: 151, train loss: 0.002217, val loss: 0.005820, val mae: 0.052916, val r2: 0.853403\n",
      "epoch: 152, train loss: 0.002216, val loss: 0.005812, val mae: 0.052883, val r2: 0.853714\n",
      "epoch: 153, train loss: 0.002221, val loss: 0.005831, val mae: 0.052843, val r2: 0.853218\n",
      "epoch: 154, train loss: 0.002210, val loss: 0.005754, val mae: 0.052495, val r2: 0.855326\n",
      "epoch: 155, train loss: 0.002230, val loss: 0.005979, val mae: 0.053600, val r2: 0.849653\n",
      "epoch: 156, train loss: 0.002214, val loss: 0.006086, val mae: 0.054175, val r2: 0.846862\n",
      "epoch: 157, train loss: 0.002213, val loss: 0.006223, val mae: 0.055057, val r2: 0.843514\n",
      "epoch: 158, train loss: 0.002215, val loss: 0.006652, val mae: 0.057021, val r2: 0.832711\n",
      "epoch: 159, train loss: 0.002203, val loss: 0.006717, val mae: 0.057583, val r2: 0.830878\n",
      "epoch: 160, train loss: 0.002230, val loss: 0.006867, val mae: 0.058101, val r2: 0.826676\n",
      "epoch: 161, train loss: 0.002219, val loss: 0.006464, val mae: 0.056071, val r2: 0.838043\n",
      "epoch: 162, train loss: 0.002222, val loss: 0.005880, val mae: 0.053187, val r2: 0.852766\n",
      "epoch: 163, train loss: 0.002200, val loss: 0.005547, val mae: 0.051200, val r2: 0.861138\n",
      "epoch: 164, train loss: 0.002190, val loss: 0.005519, val mae: 0.050854, val r2: 0.861594\n",
      "epoch: 165, train loss: 0.002181, val loss: 0.005652, val mae: 0.051274, val r2: 0.858091\n",
      "epoch: 166, train loss: 0.002162, val loss: 0.005637, val mae: 0.051232, val r2: 0.858507\n",
      "epoch: 167, train loss: 0.002145, val loss: 0.005628, val mae: 0.051546, val r2: 0.858666\n",
      "epoch: 168, train loss: 0.002152, val loss: 0.005592, val mae: 0.051638, val r2: 0.859885\n",
      "epoch: 169, train loss: 0.002142, val loss: 0.005654, val mae: 0.052031, val r2: 0.858341\n",
      "epoch: 170, train loss: 0.002117, val loss: 0.005663, val mae: 0.051933, val r2: 0.858219\n",
      "epoch: 171, train loss: 0.002115, val loss: 0.005557, val mae: 0.051230, val r2: 0.860759\n",
      "epoch: 172, train loss: 0.002122, val loss: 0.005588, val mae: 0.051200, val r2: 0.860043\n",
      "epoch: 173, train loss: 0.002112, val loss: 0.005537, val mae: 0.050754, val r2: 0.861327\n",
      "epoch: 174, train loss: 0.002107, val loss: 0.005724, val mae: 0.051320, val r2: 0.856649\n",
      "epoch: 175, train loss: 0.002094, val loss: 0.005702, val mae: 0.050968, val r2: 0.857115\n",
      "epoch: 176, train loss: 0.002153, val loss: 0.005954, val mae: 0.052074, val r2: 0.850280\n",
      "epoch: 177, train loss: 0.002114, val loss: 0.005783, val mae: 0.051890, val r2: 0.855027\n",
      "epoch: 178, train loss: 0.002115, val loss: 0.005793, val mae: 0.052488, val r2: 0.854590\n",
      "epoch: 179, train loss: 0.002105, val loss: 0.005979, val mae: 0.053286, val r2: 0.849887\n",
      "epoch: 180, train loss: 0.002085, val loss: 0.005962, val mae: 0.053179, val r2: 0.850236\n",
      "epoch: 181, train loss: 0.002070, val loss: 0.005672, val mae: 0.051609, val r2: 0.857336\n",
      "epoch: 182, train loss: 0.002064, val loss: 0.005622, val mae: 0.050916, val r2: 0.858955\n",
      "epoch: 183, train loss: 0.002037, val loss: 0.005703, val mae: 0.051122, val r2: 0.856915\n",
      "epoch: 184, train loss: 0.002079, val loss: 0.005935, val mae: 0.052221, val r2: 0.851099\n",
      "epoch: 185, train loss: 0.002044, val loss: 0.006144, val mae: 0.054257, val r2: 0.845298\n",
      "epoch: 186, train loss: 0.002040, val loss: 0.006343, val mae: 0.055528, val r2: 0.839898\n",
      "epoch: 187, train loss: 0.002030, val loss: 0.006103, val mae: 0.054039, val r2: 0.845908\n",
      "epoch: 188, train loss: 0.002013, val loss: 0.005618, val mae: 0.051146, val r2: 0.858719\n",
      "epoch: 189, train loss: 0.001990, val loss: 0.005572, val mae: 0.050721, val r2: 0.860188\n",
      "epoch: 190, train loss: 0.002004, val loss: 0.005864, val mae: 0.052032, val r2: 0.852798\n",
      "epoch: 191, train loss: 0.002016, val loss: 0.006294, val mae: 0.054662, val r2: 0.841858\n",
      "epoch: 192, train loss: 0.002015, val loss: 0.006286, val mae: 0.055146, val r2: 0.841156\n",
      "epoch: 193, train loss: 0.002016, val loss: 0.006210, val mae: 0.054665, val r2: 0.843405\n",
      "epoch: 194, train loss: 0.001983, val loss: 0.005618, val mae: 0.051322, val r2: 0.858812\n",
      "epoch: 195, train loss: 0.001975, val loss: 0.005512, val mae: 0.050477, val r2: 0.861887\n",
      "epoch: 196, train loss: 0.001962, val loss: 0.005696, val mae: 0.051327, val r2: 0.857020\n",
      "epoch: 197, train loss: 0.001981, val loss: 0.006075, val mae: 0.053306, val r2: 0.847593\n",
      "epoch: 198, train loss: 0.001970, val loss: 0.006113, val mae: 0.054236, val r2: 0.845907\n",
      "epoch: 199, train loss: 0.001962, val loss: 0.006026, val mae: 0.053715, val r2: 0.848567\n",
      "epoch: 200, train loss: 0.001957, val loss: 0.005688, val mae: 0.051598, val r2: 0.857198\n",
      "epoch: 201, train loss: 0.001950, val loss: 0.005561, val mae: 0.050495, val r2: 0.860717\n",
      "epoch: 202, train loss: 0.001930, val loss: 0.005576, val mae: 0.050426, val r2: 0.860468\n",
      "epoch: 203, train loss: 0.001949, val loss: 0.005910, val mae: 0.051786, val r2: 0.851776\n",
      "epoch: 204, train loss: 0.001935, val loss: 0.006347, val mae: 0.054870, val r2: 0.840650\n",
      "epoch: 205, train loss: 0.001931, val loss: 0.006072, val mae: 0.053748, val r2: 0.847372\n",
      "epoch: 206, train loss: 0.001955, val loss: 0.005782, val mae: 0.052239, val r2: 0.854654\n",
      "epoch: 207, train loss: 0.001938, val loss: 0.005614, val mae: 0.050839, val r2: 0.859210\n",
      "epoch: 208, train loss: 0.001928, val loss: 0.005681, val mae: 0.050775, val r2: 0.857825\n",
      "epoch: 209, train loss: 0.001919, val loss: 0.005851, val mae: 0.051403, val r2: 0.853247\n",
      "epoch: 210, train loss: 0.001953, val loss: 0.006515, val mae: 0.055186, val r2: 0.836636\n",
      "epoch: 211, train loss: 0.001933, val loss: 0.005910, val mae: 0.053314, val r2: 0.851265\n",
      "epoch: 212, train loss: 0.001942, val loss: 0.005736, val mae: 0.052018, val r2: 0.855884\n",
      "epoch: 213, train loss: 0.001906, val loss: 0.005582, val mae: 0.050816, val r2: 0.860053\n",
      "epoch: 214, train loss: 0.001899, val loss: 0.005737, val mae: 0.051340, val r2: 0.856097\n",
      "epoch: 215, train loss: 0.001944, val loss: 0.006435, val mae: 0.054981, val r2: 0.838388\n",
      "epoch: 216, train loss: 0.001918, val loss: 0.006232, val mae: 0.055170, val r2: 0.842797\n",
      "epoch: 217, train loss: 0.001925, val loss: 0.005862, val mae: 0.052899, val r2: 0.852308\n",
      "epoch: 218, train loss: 0.001881, val loss: 0.005620, val mae: 0.051068, val r2: 0.859184\n",
      "epoch: 219, train loss: 0.001887, val loss: 0.005975, val mae: 0.052571, val r2: 0.849871\n",
      "epoch: 220, train loss: 0.001902, val loss: 0.006524, val mae: 0.056027, val r2: 0.836092\n",
      "epoch: 221, train loss: 0.001899, val loss: 0.006164, val mae: 0.054831, val r2: 0.844302\n",
      "epoch: 222, train loss: 0.001910, val loss: 0.005783, val mae: 0.052205, val r2: 0.854539\n",
      "epoch: 223, train loss: 0.001860, val loss: 0.005616, val mae: 0.050855, val r2: 0.859374\n",
      "epoch: 224, train loss: 0.001902, val loss: 0.006146, val mae: 0.053332, val r2: 0.846002\n",
      "epoch: 225, train loss: 0.001870, val loss: 0.006178, val mae: 0.054636, val r2: 0.844827\n",
      "epoch: 226, train loss: 0.001899, val loss: 0.005749, val mae: 0.052259, val r2: 0.855452\n",
      "epoch: 227, train loss: 0.001875, val loss: 0.005555, val mae: 0.050587, val r2: 0.860876\n",
      "epoch: 228, train loss: 0.001857, val loss: 0.005815, val mae: 0.051776, val r2: 0.854254\n",
      "epoch: 229, train loss: 0.001861, val loss: 0.006107, val mae: 0.053621, val r2: 0.846992\n",
      "epoch: 230, train loss: 0.001857, val loss: 0.005616, val mae: 0.051542, val r2: 0.859139\n",
      "epoch: 231, train loss: 0.001861, val loss: 0.005519, val mae: 0.050477, val r2: 0.861854\n",
      "epoch: 232, train loss: 0.001821, val loss: 0.005673, val mae: 0.050912, val r2: 0.857919\n",
      "epoch: 233, train loss: 0.001879, val loss: 0.006166, val mae: 0.053492, val r2: 0.845482\n",
      "epoch: 234, train loss: 0.001831, val loss: 0.005916, val mae: 0.053204, val r2: 0.851622\n",
      "epoch: 235, train loss: 0.001847, val loss: 0.005583, val mae: 0.051197, val r2: 0.859931\n",
      "epoch: 236, train loss: 0.001813, val loss: 0.005740, val mae: 0.051183, val r2: 0.856547\n",
      "epoch: 237, train loss: 0.001865, val loss: 0.006026, val mae: 0.052591, val r2: 0.848961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 238, train loss: 0.001806, val loss: 0.006509, val mae: 0.056232, val r2: 0.836793\n",
      "epoch: 239, train loss: 0.001846, val loss: 0.005651, val mae: 0.051852, val r2: 0.857995\n",
      "epoch: 240, train loss: 0.001814, val loss: 0.005775, val mae: 0.051442, val r2: 0.855457\n",
      "epoch: 241, train loss: 0.001824, val loss: 0.006022, val mae: 0.052637, val r2: 0.848972\n",
      "epoch: 242, train loss: 0.001801, val loss: 0.006579, val mae: 0.056221, val r2: 0.835152\n",
      "epoch: 243, train loss: 0.001824, val loss: 0.005782, val mae: 0.052463, val r2: 0.854703\n",
      "epoch: 244, train loss: 0.001811, val loss: 0.005728, val mae: 0.051222, val r2: 0.856587\n",
      "epoch: 245, train loss: 0.001782, val loss: 0.005923, val mae: 0.052071, val r2: 0.851484\n",
      "epoch: 246, train loss: 0.001788, val loss: 0.006357, val mae: 0.054772, val r2: 0.840782\n",
      "epoch: 247, train loss: 0.001780, val loss: 0.006030, val mae: 0.053687, val r2: 0.848524\n",
      "epoch: 248, train loss: 0.001813, val loss: 0.005681, val mae: 0.051060, val r2: 0.857607\n",
      "epoch: 249, train loss: 0.001751, val loss: 0.005843, val mae: 0.051480, val r2: 0.853640\n",
      "epoch: 250, train loss: 0.001795, val loss: 0.006583, val mae: 0.055673, val r2: 0.835142\n",
      "epoch: 251, train loss: 0.001756, val loss: 0.005997, val mae: 0.053412, val r2: 0.849218\n",
      "epoch: 252, train loss: 0.001794, val loss: 0.005761, val mae: 0.051354, val r2: 0.855469\n",
      "epoch: 253, train loss: 0.001740, val loss: 0.005896, val mae: 0.051615, val r2: 0.852055\n",
      "epoch: 254, train loss: 0.001791, val loss: 0.006518, val mae: 0.055463, val r2: 0.836641\n",
      "epoch: 255, train loss: 0.001739, val loss: 0.006155, val mae: 0.054409, val r2: 0.845115\n",
      "epoch: 256, train loss: 0.001761, val loss: 0.005737, val mae: 0.051462, val r2: 0.855910\n",
      "epoch: 257, train loss: 0.001727, val loss: 0.005934, val mae: 0.051940, val r2: 0.850959\n",
      "epoch: 258, train loss: 0.001800, val loss: 0.006754, val mae: 0.056761, val r2: 0.830422\n",
      "epoch: 259, train loss: 0.001740, val loss: 0.006113, val mae: 0.054402, val r2: 0.846201\n",
      "epoch: 260, train loss: 0.001762, val loss: 0.005729, val mae: 0.051380, val r2: 0.856327\n",
      "epoch: 261, train loss: 0.001717, val loss: 0.005822, val mae: 0.051681, val r2: 0.853909\n",
      "epoch: 262, train loss: 0.001778, val loss: 0.006670, val mae: 0.056674, val r2: 0.832657\n",
      "epoch: 263, train loss: 0.001728, val loss: 0.006022, val mae: 0.054047, val r2: 0.848455\n",
      "epoch: 264, train loss: 0.001756, val loss: 0.005656, val mae: 0.051024, val r2: 0.858214\n",
      "epoch: 265, train loss: 0.001707, val loss: 0.005777, val mae: 0.051298, val r2: 0.855168\n",
      "epoch: 266, train loss: 0.001771, val loss: 0.006717, val mae: 0.056949, val r2: 0.831814\n",
      "epoch: 267, train loss: 0.001726, val loss: 0.005819, val mae: 0.052771, val r2: 0.853675\n",
      "epoch: 268, train loss: 0.001753, val loss: 0.005651, val mae: 0.050694, val r2: 0.858487\n",
      "epoch: 269, train loss: 0.001709, val loss: 0.005935, val mae: 0.052255, val r2: 0.851079\n",
      "epoch: 270, train loss: 0.001709, val loss: 0.006074, val mae: 0.053630, val r2: 0.847885\n",
      "epoch: 271, train loss: 0.001725, val loss: 0.005485, val mae: 0.050564, val r2: 0.862437\n",
      "epoch: 272, train loss: 0.001696, val loss: 0.005750, val mae: 0.050811, val r2: 0.855895\n",
      "epoch: 273, train loss: 0.001706, val loss: 0.005927, val mae: 0.052109, val r2: 0.851589\n",
      "epoch: 274, train loss: 0.001672, val loss: 0.006088, val mae: 0.053849, val r2: 0.847625\n",
      "epoch: 275, train loss: 0.001727, val loss: 0.005526, val mae: 0.050554, val r2: 0.861523\n",
      "epoch: 276, train loss: 0.001683, val loss: 0.005913, val mae: 0.051504, val r2: 0.852225\n",
      "epoch: 277, train loss: 0.001748, val loss: 0.006391, val mae: 0.054781, val r2: 0.840401\n",
      "epoch: 278, train loss: 0.001674, val loss: 0.005646, val mae: 0.051755, val r2: 0.858379\n",
      "epoch: 279, train loss: 0.001701, val loss: 0.005765, val mae: 0.051150, val r2: 0.856086\n",
      "epoch: 280, train loss: 0.001664, val loss: 0.005958, val mae: 0.052048, val r2: 0.850561\n",
      "epoch: 281, train loss: 0.001678, val loss: 0.006612, val mae: 0.056706, val r2: 0.834529\n",
      "epoch: 282, train loss: 0.001699, val loss: 0.005615, val mae: 0.051043, val r2: 0.859089\n",
      "epoch: 283, train loss: 0.001668, val loss: 0.005935, val mae: 0.051625, val r2: 0.851262\n",
      "epoch: 284, train loss: 0.001735, val loss: 0.007038, val mae: 0.058346, val r2: 0.823905\n",
      "epoch: 285, train loss: 0.001656, val loss: 0.005855, val mae: 0.052997, val r2: 0.852829\n",
      "epoch: 286, train loss: 0.001694, val loss: 0.005810, val mae: 0.051360, val r2: 0.854789\n",
      "epoch: 287, train loss: 0.001661, val loss: 0.006384, val mae: 0.054443, val r2: 0.839838\n",
      "epoch: 288, train loss: 0.001634, val loss: 0.006679, val mae: 0.057231, val r2: 0.832213\n",
      "epoch: 289, train loss: 0.001696, val loss: 0.005679, val mae: 0.051295, val r2: 0.858054\n",
      "epoch: 290, train loss: 0.001665, val loss: 0.005965, val mae: 0.052213, val r2: 0.850146\n",
      "epoch: 291, train loss: 0.001720, val loss: 0.007126, val mae: 0.059207, val r2: 0.821449\n",
      "epoch: 292, train loss: 0.001683, val loss: 0.005557, val mae: 0.051117, val r2: 0.860395\n",
      "epoch: 293, train loss: 0.001667, val loss: 0.005860, val mae: 0.051417, val r2: 0.853041\n",
      "epoch: 294, train loss: 0.001711, val loss: 0.006852, val mae: 0.057689, val r2: 0.828542\n",
      "epoch: 295, train loss: 0.001628, val loss: 0.005573, val mae: 0.051146, val r2: 0.860128\n",
      "epoch: 296, train loss: 0.001637, val loss: 0.005791, val mae: 0.051334, val r2: 0.855053\n",
      "epoch: 297, train loss: 0.001674, val loss: 0.006783, val mae: 0.057061, val r2: 0.830381\n",
      "epoch: 298, train loss: 0.001627, val loss: 0.005567, val mae: 0.051166, val r2: 0.860433\n",
      "epoch: 299, train loss: 0.001632, val loss: 0.005775, val mae: 0.051216, val r2: 0.855564\n",
      "epoch: 300, train loss: 0.001675, val loss: 0.006926, val mae: 0.057752, val r2: 0.826658\n",
      "epoch: 301, train loss: 0.001624, val loss: 0.005553, val mae: 0.051107, val r2: 0.860834\n",
      "epoch: 302, train loss: 0.001619, val loss: 0.005756, val mae: 0.051227, val r2: 0.855833\n",
      "epoch: 303, train loss: 0.001672, val loss: 0.007296, val mae: 0.059682, val r2: 0.817511\n",
      "epoch: 304, train loss: 0.001635, val loss: 0.005582, val mae: 0.051202, val r2: 0.860257\n",
      "epoch: 305, train loss: 0.001627, val loss: 0.005747, val mae: 0.051243, val r2: 0.855827\n",
      "epoch: 306, train loss: 0.001654, val loss: 0.006832, val mae: 0.057545, val r2: 0.829124\n",
      "epoch: 307, train loss: 0.001630, val loss: 0.005605, val mae: 0.051312, val r2: 0.859742\n",
      "epoch: 308, train loss: 0.001615, val loss: 0.005762, val mae: 0.051373, val r2: 0.855523\n",
      "epoch: 309, train loss: 0.001671, val loss: 0.007252, val mae: 0.059653, val r2: 0.819094\n",
      "epoch: 310, train loss: 0.001647, val loss: 0.005664, val mae: 0.051501, val r2: 0.858409\n",
      "epoch: 311, train loss: 0.001615, val loss: 0.005790, val mae: 0.051335, val r2: 0.854858\n",
      "epoch: 312, train loss: 0.001646, val loss: 0.006795, val mae: 0.057416, val r2: 0.830279\n",
      "epoch: 313, train loss: 0.001612, val loss: 0.005737, val mae: 0.051709, val r2: 0.856732\n",
      "epoch: 314, train loss: 0.001595, val loss: 0.005855, val mae: 0.051737, val r2: 0.853106\n",
      "epoch: 315, train loss: 0.001625, val loss: 0.007069, val mae: 0.059002, val r2: 0.823070\n",
      "epoch: 316, train loss: 0.001608, val loss: 0.005717, val mae: 0.051595, val r2: 0.857112\n",
      "epoch: 317, train loss: 0.001603, val loss: 0.005837, val mae: 0.051751, val r2: 0.853498\n",
      "epoch: 318, train loss: 0.001628, val loss: 0.006953, val mae: 0.058498, val r2: 0.825858\n",
      "epoch: 319, train loss: 0.001598, val loss: 0.005712, val mae: 0.051711, val r2: 0.857321\n",
      "epoch: 320, train loss: 0.001579, val loss: 0.005856, val mae: 0.051954, val r2: 0.853076\n",
      "epoch: 321, train loss: 0.001607, val loss: 0.006824, val mae: 0.057971, val r2: 0.828844\n",
      "epoch: 322, train loss: 0.001591, val loss: 0.005672, val mae: 0.051452, val r2: 0.858268\n",
      "epoch: 323, train loss: 0.001562, val loss: 0.005758, val mae: 0.051378, val r2: 0.855657\n",
      "epoch: 324, train loss: 0.001601, val loss: 0.006597, val mae: 0.056683, val r2: 0.834702\n",
      "epoch: 325, train loss: 0.001581, val loss: 0.005700, val mae: 0.051583, val r2: 0.857703\n",
      "epoch: 326, train loss: 0.001548, val loss: 0.005816, val mae: 0.051843, val r2: 0.854090\n",
      "epoch: 327, train loss: 0.001591, val loss: 0.006414, val mae: 0.055592, val r2: 0.839190\n",
      "epoch: 328, train loss: 0.001582, val loss: 0.005811, val mae: 0.052004, val r2: 0.855204\n",
      "epoch: 329, train loss: 0.001551, val loss: 0.005890, val mae: 0.052080, val r2: 0.852313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 330, train loss: 0.001573, val loss: 0.005942, val mae: 0.053173, val r2: 0.851143\n",
      "epoch: 331, train loss: 0.001562, val loss: 0.005925, val mae: 0.052357, val r2: 0.852277\n",
      "epoch: 332, train loss: 0.001511, val loss: 0.005848, val mae: 0.051830, val r2: 0.853357\n",
      "epoch: 333, train loss: 0.001542, val loss: 0.006012, val mae: 0.053504, val r2: 0.849329\n",
      "epoch: 334, train loss: 0.001554, val loss: 0.006047, val mae: 0.052790, val r2: 0.849439\n",
      "epoch: 335, train loss: 0.001509, val loss: 0.005940, val mae: 0.052253, val r2: 0.851034\n",
      "epoch: 336, train loss: 0.001536, val loss: 0.006480, val mae: 0.056146, val r2: 0.837590\n",
      "epoch: 337, train loss: 0.001574, val loss: 0.005987, val mae: 0.052559, val r2: 0.850737\n",
      "epoch: 338, train loss: 0.001523, val loss: 0.006122, val mae: 0.053172, val r2: 0.846400\n",
      "epoch: 339, train loss: 0.001528, val loss: 0.006297, val mae: 0.055085, val r2: 0.842080\n",
      "epoch: 340, train loss: 0.001551, val loss: 0.006018, val mae: 0.052625, val r2: 0.849935\n",
      "epoch: 341, train loss: 0.001507, val loss: 0.006156, val mae: 0.053599, val r2: 0.845583\n",
      "epoch: 342, train loss: 0.001497, val loss: 0.006144, val mae: 0.054404, val r2: 0.845736\n",
      "epoch: 343, train loss: 0.001520, val loss: 0.005964, val mae: 0.052384, val r2: 0.851101\n",
      "epoch: 344, train loss: 0.001484, val loss: 0.006074, val mae: 0.053141, val r2: 0.847387\n",
      "epoch: 345, train loss: 0.001497, val loss: 0.006118, val mae: 0.054232, val r2: 0.846326\n",
      "epoch: 346, train loss: 0.001503, val loss: 0.006004, val mae: 0.052591, val r2: 0.850330\n",
      "epoch: 347, train loss: 0.001464, val loss: 0.005995, val mae: 0.052719, val r2: 0.849575\n",
      "epoch: 348, train loss: 0.001489, val loss: 0.006198, val mae: 0.054564, val r2: 0.844544\n",
      "epoch: 349, train loss: 0.001521, val loss: 0.006082, val mae: 0.052998, val r2: 0.848671\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [16, 8], # bad\n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 32,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 1,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 6, \n",
    "                      'epoches': 350,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc378c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps!\n",
      "Transformer(\n",
      "  (element_embedding_temporal): Linear(in_features=294, out_features=192, bias=True)\n",
      "  (element_embedding_spatial): Linear(in_features=2695, out_features=1760, bias=True)\n",
      "  (pos_embedding): Embedding(6, 32)\n",
      "  (variable_embedding): Embedding(55, 32)\n",
      "  (temporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatial): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatiotemporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (keys): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (queries): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatter): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=11220, out_features=2640, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=2640, out_features=330, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch: 0, train loss: 0.021395, val loss: 0.016851, val mae: 0.103443, val r2: 0.573950\n",
      "epoch: 1, train loss: 0.012694, val loss: 0.013307, val mae: 0.088025, val r2: 0.663711\n",
      "epoch: 2, train loss: 0.010291, val loss: 0.011840, val mae: 0.081848, val r2: 0.700420\n",
      "epoch: 3, train loss: 0.009142, val loss: 0.010896, val mae: 0.077924, val r2: 0.724129\n",
      "epoch: 4, train loss: 0.008358, val loss: 0.010260, val mae: 0.075356, val r2: 0.740266\n",
      "epoch: 5, train loss: 0.007802, val loss: 0.009804, val mae: 0.073465, val r2: 0.751960\n",
      "epoch: 6, train loss: 0.007358, val loss: 0.009377, val mae: 0.071603, val r2: 0.763068\n",
      "epoch: 7, train loss: 0.006987, val loss: 0.009036, val mae: 0.070124, val r2: 0.771826\n",
      "epoch: 8, train loss: 0.006665, val loss: 0.008737, val mae: 0.068739, val r2: 0.779686\n",
      "epoch: 9, train loss: 0.006382, val loss: 0.008500, val mae: 0.067585, val r2: 0.785846\n",
      "epoch: 10, train loss: 0.006135, val loss: 0.008321, val mae: 0.066705, val r2: 0.790562\n",
      "epoch: 11, train loss: 0.005909, val loss: 0.008128, val mae: 0.065761, val r2: 0.795593\n",
      "epoch: 12, train loss: 0.005695, val loss: 0.007971, val mae: 0.064943, val r2: 0.799600\n",
      "epoch: 13, train loss: 0.005515, val loss: 0.007783, val mae: 0.063995, val r2: 0.804466\n",
      "epoch: 14, train loss: 0.005339, val loss: 0.007666, val mae: 0.063357, val r2: 0.807504\n",
      "epoch: 15, train loss: 0.005202, val loss: 0.007578, val mae: 0.062922, val r2: 0.809797\n",
      "epoch: 16, train loss: 0.005074, val loss: 0.007500, val mae: 0.062499, val r2: 0.811810\n",
      "epoch: 17, train loss: 0.004956, val loss: 0.007412, val mae: 0.062093, val r2: 0.813996\n",
      "epoch: 18, train loss: 0.004846, val loss: 0.007294, val mae: 0.061497, val r2: 0.817183\n",
      "epoch: 19, train loss: 0.004764, val loss: 0.007277, val mae: 0.061422, val r2: 0.817632\n",
      "epoch: 20, train loss: 0.004683, val loss: 0.007192, val mae: 0.061059, val r2: 0.819789\n",
      "epoch: 21, train loss: 0.004593, val loss: 0.007140, val mae: 0.060792, val r2: 0.821109\n",
      "epoch: 22, train loss: 0.004540, val loss: 0.007079, val mae: 0.060503, val r2: 0.822617\n",
      "epoch: 23, train loss: 0.004466, val loss: 0.007071, val mae: 0.060427, val r2: 0.822907\n",
      "epoch: 24, train loss: 0.004414, val loss: 0.007003, val mae: 0.060130, val r2: 0.824559\n",
      "epoch: 25, train loss: 0.004350, val loss: 0.006989, val mae: 0.060058, val r2: 0.824951\n",
      "epoch: 26, train loss: 0.004306, val loss: 0.007000, val mae: 0.060080, val r2: 0.824776\n",
      "epoch: 27, train loss: 0.004257, val loss: 0.006952, val mae: 0.059819, val r2: 0.825995\n",
      "epoch: 28, train loss: 0.004203, val loss: 0.006942, val mae: 0.059754, val r2: 0.826272\n",
      "epoch: 29, train loss: 0.004166, val loss: 0.006953, val mae: 0.059860, val r2: 0.825994\n",
      "epoch: 30, train loss: 0.004133, val loss: 0.006883, val mae: 0.059541, val r2: 0.827727\n",
      "epoch: 31, train loss: 0.004085, val loss: 0.006872, val mae: 0.059417, val r2: 0.828084\n",
      "epoch: 32, train loss: 0.004047, val loss: 0.006878, val mae: 0.059516, val r2: 0.827935\n",
      "epoch: 33, train loss: 0.004008, val loss: 0.006863, val mae: 0.059426, val r2: 0.828280\n",
      "epoch: 34, train loss: 0.003958, val loss: 0.006793, val mae: 0.059081, val r2: 0.830020\n",
      "epoch: 35, train loss: 0.003936, val loss: 0.006890, val mae: 0.059630, val r2: 0.827622\n",
      "epoch: 36, train loss: 0.003907, val loss: 0.006846, val mae: 0.059357, val r2: 0.828744\n",
      "epoch: 37, train loss: 0.003873, val loss: 0.006774, val mae: 0.058987, val r2: 0.830560\n",
      "epoch: 38, train loss: 0.003844, val loss: 0.006805, val mae: 0.059146, val r2: 0.829769\n",
      "epoch: 39, train loss: 0.003812, val loss: 0.006820, val mae: 0.059276, val r2: 0.829325\n",
      "epoch: 40, train loss: 0.003778, val loss: 0.006757, val mae: 0.058855, val r2: 0.830959\n",
      "epoch: 41, train loss: 0.003764, val loss: 0.006748, val mae: 0.058787, val r2: 0.831239\n",
      "epoch: 42, train loss: 0.003736, val loss: 0.006754, val mae: 0.058860, val r2: 0.831061\n",
      "epoch: 43, train loss: 0.003717, val loss: 0.006787, val mae: 0.059091, val r2: 0.830216\n",
      "epoch: 44, train loss: 0.003688, val loss: 0.006760, val mae: 0.058893, val r2: 0.830927\n",
      "epoch: 45, train loss: 0.003676, val loss: 0.006734, val mae: 0.058718, val r2: 0.831611\n",
      "epoch: 46, train loss: 0.003660, val loss: 0.006739, val mae: 0.058779, val r2: 0.831396\n",
      "epoch: 47, train loss: 0.003654, val loss: 0.006748, val mae: 0.058836, val r2: 0.831156\n",
      "epoch: 48, train loss: 0.003618, val loss: 0.006625, val mae: 0.058222, val r2: 0.834300\n",
      "epoch: 49, train loss: 0.003619, val loss: 0.006634, val mae: 0.058193, val r2: 0.834127\n",
      "epoch: 50, train loss: 0.003603, val loss: 0.006629, val mae: 0.058195, val r2: 0.834031\n",
      "epoch: 51, train loss: 0.003588, val loss: 0.006530, val mae: 0.057625, val r2: 0.836703\n",
      "epoch: 52, train loss: 0.003592, val loss: 0.006515, val mae: 0.057507, val r2: 0.837132\n",
      "epoch: 53, train loss: 0.003594, val loss: 0.006502, val mae: 0.057402, val r2: 0.837436\n",
      "epoch: 54, train loss: 0.003580, val loss: 0.006398, val mae: 0.056782, val r2: 0.840138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55, train loss: 0.003623, val loss: 0.006221, val mae: 0.055808, val r2: 0.844616\n",
      "epoch: 56, train loss: 0.003599, val loss: 0.006080, val mae: 0.054943, val r2: 0.848160\n",
      "epoch: 57, train loss: 0.003623, val loss: 0.005946, val mae: 0.054071, val r2: 0.851501\n",
      "epoch: 58, train loss: 0.003627, val loss: 0.005862, val mae: 0.053343, val r2: 0.853676\n",
      "epoch: 59, train loss: 0.003628, val loss: 0.005947, val mae: 0.053403, val r2: 0.851536\n",
      "epoch: 60, train loss: 0.003597, val loss: 0.006191, val mae: 0.054339, val r2: 0.845449\n",
      "epoch: 61, train loss: 0.003574, val loss: 0.006497, val mae: 0.055609, val r2: 0.837774\n",
      "epoch: 62, train loss: 0.003589, val loss: 0.006729, val mae: 0.056667, val r2: 0.832001\n",
      "epoch: 63, train loss: 0.003689, val loss: 0.006602, val mae: 0.056053, val r2: 0.834958\n",
      "epoch: 64, train loss: 0.003954, val loss: 0.006115, val mae: 0.053967, val r2: 0.847089\n",
      "epoch: 65, train loss: 0.004343, val loss: 0.005922, val mae: 0.053939, val r2: 0.851509\n",
      "epoch: 66, train loss: 0.004623, val loss: 0.006875, val mae: 0.059314, val r2: 0.827320\n",
      "epoch: 67, train loss: 0.004264, val loss: 0.006626, val mae: 0.057858, val r2: 0.833842\n",
      "epoch: 68, train loss: 0.003662, val loss: 0.006104, val mae: 0.054867, val r2: 0.847155\n",
      "epoch: 69, train loss: 0.003414, val loss: 0.005949, val mae: 0.053817, val r2: 0.851170\n",
      "epoch: 70, train loss: 0.003342, val loss: 0.005888, val mae: 0.053372, val r2: 0.852841\n",
      "epoch: 71, train loss: 0.003308, val loss: 0.005825, val mae: 0.052988, val r2: 0.854365\n",
      "epoch: 72, train loss: 0.003285, val loss: 0.005822, val mae: 0.052862, val r2: 0.854438\n",
      "epoch: 73, train loss: 0.003269, val loss: 0.005798, val mae: 0.052754, val r2: 0.854977\n",
      "epoch: 74, train loss: 0.003249, val loss: 0.005794, val mae: 0.052711, val r2: 0.855063\n",
      "epoch: 75, train loss: 0.003242, val loss: 0.005781, val mae: 0.052625, val r2: 0.855403\n",
      "epoch: 76, train loss: 0.003224, val loss: 0.005771, val mae: 0.052579, val r2: 0.855605\n",
      "epoch: 77, train loss: 0.003221, val loss: 0.005788, val mae: 0.052638, val r2: 0.855158\n",
      "epoch: 78, train loss: 0.003194, val loss: 0.005768, val mae: 0.052540, val r2: 0.855657\n",
      "epoch: 79, train loss: 0.003170, val loss: 0.005753, val mae: 0.052418, val r2: 0.856082\n",
      "epoch: 80, train loss: 0.003155, val loss: 0.005728, val mae: 0.052333, val r2: 0.856655\n",
      "epoch: 81, train loss: 0.003131, val loss: 0.005744, val mae: 0.052345, val r2: 0.856230\n",
      "epoch: 82, train loss: 0.003113, val loss: 0.005737, val mae: 0.052348, val r2: 0.856412\n",
      "epoch: 83, train loss: 0.003093, val loss: 0.005723, val mae: 0.052253, val r2: 0.856831\n",
      "epoch: 84, train loss: 0.003076, val loss: 0.005739, val mae: 0.052333, val r2: 0.856337\n",
      "epoch: 85, train loss: 0.003060, val loss: 0.005715, val mae: 0.052278, val r2: 0.856967\n",
      "epoch: 86, train loss: 0.003046, val loss: 0.005730, val mae: 0.052305, val r2: 0.856543\n",
      "epoch: 87, train loss: 0.003028, val loss: 0.005684, val mae: 0.052095, val r2: 0.857781\n",
      "epoch: 88, train loss: 0.003010, val loss: 0.005681, val mae: 0.052155, val r2: 0.857847\n",
      "epoch: 89, train loss: 0.003007, val loss: 0.005675, val mae: 0.052046, val r2: 0.857970\n",
      "epoch: 90, train loss: 0.002993, val loss: 0.005697, val mae: 0.052228, val r2: 0.857440\n",
      "epoch: 91, train loss: 0.002979, val loss: 0.005685, val mae: 0.052167, val r2: 0.857731\n",
      "epoch: 92, train loss: 0.002961, val loss: 0.005680, val mae: 0.052138, val r2: 0.857853\n",
      "epoch: 93, train loss: 0.002955, val loss: 0.005647, val mae: 0.052007, val r2: 0.858704\n",
      "epoch: 94, train loss: 0.002939, val loss: 0.005663, val mae: 0.051979, val r2: 0.858227\n",
      "epoch: 95, train loss: 0.002927, val loss: 0.005672, val mae: 0.052024, val r2: 0.857969\n",
      "epoch: 96, train loss: 0.002911, val loss: 0.005680, val mae: 0.052085, val r2: 0.857752\n",
      "epoch: 97, train loss: 0.002901, val loss: 0.005665, val mae: 0.052030, val r2: 0.858057\n",
      "epoch: 98, train loss: 0.002888, val loss: 0.005685, val mae: 0.052078, val r2: 0.857586\n",
      "epoch: 99, train loss: 0.002876, val loss: 0.005701, val mae: 0.052161, val r2: 0.857169\n",
      "epoch: 100, train loss: 0.002870, val loss: 0.005711, val mae: 0.052217, val r2: 0.856928\n",
      "epoch: 101, train loss: 0.002856, val loss: 0.005692, val mae: 0.052101, val r2: 0.857427\n",
      "epoch: 102, train loss: 0.002845, val loss: 0.005698, val mae: 0.052125, val r2: 0.857229\n",
      "epoch: 103, train loss: 0.002839, val loss: 0.005704, val mae: 0.052141, val r2: 0.857016\n",
      "epoch: 104, train loss: 0.002827, val loss: 0.005727, val mae: 0.052230, val r2: 0.856471\n",
      "epoch: 105, train loss: 0.002816, val loss: 0.005741, val mae: 0.052266, val r2: 0.856024\n",
      "epoch: 106, train loss: 0.002810, val loss: 0.005721, val mae: 0.052162, val r2: 0.856491\n",
      "epoch: 107, train loss: 0.002798, val loss: 0.005742, val mae: 0.052239, val r2: 0.855942\n",
      "epoch: 108, train loss: 0.002791, val loss: 0.005796, val mae: 0.052516, val r2: 0.854463\n",
      "epoch: 109, train loss: 0.002776, val loss: 0.005879, val mae: 0.052837, val r2: 0.852427\n",
      "epoch: 110, train loss: 0.002778, val loss: 0.005872, val mae: 0.052774, val r2: 0.852508\n",
      "epoch: 111, train loss: 0.002767, val loss: 0.005946, val mae: 0.053134, val r2: 0.850591\n",
      "epoch: 112, train loss: 0.002756, val loss: 0.005997, val mae: 0.053331, val r2: 0.849244\n",
      "epoch: 113, train loss: 0.002759, val loss: 0.006176, val mae: 0.054074, val r2: 0.844635\n",
      "epoch: 114, train loss: 0.002753, val loss: 0.006186, val mae: 0.054159, val r2: 0.844371\n",
      "epoch: 115, train loss: 0.002754, val loss: 0.006232, val mae: 0.054333, val r2: 0.843218\n",
      "epoch: 116, train loss: 0.002750, val loss: 0.006395, val mae: 0.055022, val r2: 0.839109\n",
      "epoch: 117, train loss: 0.002760, val loss: 0.006548, val mae: 0.055788, val r2: 0.835226\n",
      "epoch: 118, train loss: 0.002765, val loss: 0.006787, val mae: 0.056825, val r2: 0.829179\n",
      "epoch: 119, train loss: 0.002794, val loss: 0.007121, val mae: 0.058382, val r2: 0.820741\n",
      "epoch: 120, train loss: 0.002845, val loss: 0.007411, val mae: 0.059645, val r2: 0.813339\n",
      "epoch: 121, train loss: 0.002968, val loss: 0.007613, val mae: 0.060623, val r2: 0.808320\n",
      "epoch: 122, train loss: 0.003211, val loss: 0.007090, val mae: 0.058361, val r2: 0.821133\n",
      "epoch: 123, train loss: 0.003675, val loss: 0.006395, val mae: 0.055426, val r2: 0.837962\n",
      "epoch: 124, train loss: 0.004193, val loss: 0.008232, val mae: 0.064529, val r2: 0.791188\n",
      "epoch: 125, train loss: 0.003990, val loss: 0.006610, val mae: 0.057460, val r2: 0.832595\n",
      "epoch: 126, train loss: 0.003555, val loss: 0.006142, val mae: 0.055102, val r2: 0.844686\n",
      "epoch: 127, train loss: 0.003442, val loss: 0.006401, val mae: 0.056532, val r2: 0.837590\n",
      "epoch: 128, train loss: 0.003404, val loss: 0.006743, val mae: 0.058319, val r2: 0.828399\n",
      "epoch: 129, train loss: 0.003337, val loss: 0.006770, val mae: 0.058469, val r2: 0.827539\n",
      "epoch: 130, train loss: 0.003289, val loss: 0.006803, val mae: 0.058611, val r2: 0.826436\n",
      "epoch: 131, train loss: 0.003285, val loss: 0.006797, val mae: 0.058572, val r2: 0.826477\n",
      "epoch: 132, train loss: 0.003280, val loss: 0.006736, val mae: 0.058258, val r2: 0.827974\n",
      "epoch: 133, train loss: 0.003273, val loss: 0.006711, val mae: 0.058122, val r2: 0.828597\n",
      "epoch: 134, train loss: 0.003242, val loss: 0.006559, val mae: 0.057322, val r2: 0.832681\n",
      "epoch: 135, train loss: 0.003203, val loss: 0.006403, val mae: 0.056489, val r2: 0.836928\n",
      "epoch: 136, train loss: 0.003146, val loss: 0.006333, val mae: 0.056163, val r2: 0.838854\n",
      "epoch: 137, train loss: 0.003069, val loss: 0.006166, val mae: 0.055300, val r2: 0.843463\n",
      "epoch: 138, train loss: 0.003014, val loss: 0.006006, val mae: 0.054474, val r2: 0.847846\n",
      "epoch: 139, train loss: 0.002949, val loss: 0.005930, val mae: 0.054037, val r2: 0.850056\n",
      "epoch: 140, train loss: 0.002895, val loss: 0.005774, val mae: 0.053198, val r2: 0.854297\n",
      "epoch: 141, train loss: 0.002865, val loss: 0.005688, val mae: 0.052732, val r2: 0.856650\n",
      "epoch: 142, train loss: 0.002828, val loss: 0.005638, val mae: 0.052489, val r2: 0.858080\n",
      "epoch: 143, train loss: 0.002811, val loss: 0.005561, val mae: 0.052036, val r2: 0.860200\n",
      "epoch: 144, train loss: 0.002784, val loss: 0.005515, val mae: 0.051721, val r2: 0.861578\n",
      "epoch: 145, train loss: 0.002764, val loss: 0.005466, val mae: 0.051454, val r2: 0.862885\n",
      "epoch: 146, train loss: 0.002749, val loss: 0.005423, val mae: 0.051165, val r2: 0.864040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 147, train loss: 0.002726, val loss: 0.005392, val mae: 0.050971, val r2: 0.864854\n",
      "epoch: 148, train loss: 0.002705, val loss: 0.005401, val mae: 0.051033, val r2: 0.864671\n",
      "epoch: 149, train loss: 0.002680, val loss: 0.005401, val mae: 0.051035, val r2: 0.864666\n",
      "epoch: 150, train loss: 0.002652, val loss: 0.005374, val mae: 0.050863, val r2: 0.865400\n",
      "epoch: 151, train loss: 0.002623, val loss: 0.005344, val mae: 0.050642, val r2: 0.866245\n",
      "epoch: 152, train loss: 0.002606, val loss: 0.005347, val mae: 0.050625, val r2: 0.866130\n",
      "epoch: 153, train loss: 0.002584, val loss: 0.005344, val mae: 0.050636, val r2: 0.866178\n",
      "epoch: 154, train loss: 0.002567, val loss: 0.005355, val mae: 0.050695, val r2: 0.865870\n",
      "epoch: 155, train loss: 0.002554, val loss: 0.005362, val mae: 0.050802, val r2: 0.865693\n",
      "epoch: 156, train loss: 0.002539, val loss: 0.005380, val mae: 0.050880, val r2: 0.865213\n",
      "epoch: 157, train loss: 0.002524, val loss: 0.005378, val mae: 0.050916, val r2: 0.865210\n",
      "epoch: 158, train loss: 0.002513, val loss: 0.005383, val mae: 0.050908, val r2: 0.865124\n",
      "epoch: 159, train loss: 0.002498, val loss: 0.005384, val mae: 0.050942, val r2: 0.865053\n",
      "epoch: 160, train loss: 0.002493, val loss: 0.005409, val mae: 0.051099, val r2: 0.864431\n",
      "epoch: 161, train loss: 0.002482, val loss: 0.005378, val mae: 0.050989, val r2: 0.865074\n",
      "epoch: 162, train loss: 0.002474, val loss: 0.005380, val mae: 0.050962, val r2: 0.865059\n",
      "epoch: 163, train loss: 0.002466, val loss: 0.005411, val mae: 0.051184, val r2: 0.864116\n",
      "epoch: 164, train loss: 0.002464, val loss: 0.005454, val mae: 0.051374, val r2: 0.863020\n",
      "epoch: 165, train loss: 0.002460, val loss: 0.005507, val mae: 0.051826, val r2: 0.861486\n",
      "epoch: 166, train loss: 0.002458, val loss: 0.005546, val mae: 0.052017, val r2: 0.860413\n",
      "epoch: 167, train loss: 0.002462, val loss: 0.005616, val mae: 0.052454, val r2: 0.858452\n",
      "epoch: 168, train loss: 0.002467, val loss: 0.005724, val mae: 0.053030, val r2: 0.855498\n",
      "epoch: 169, train loss: 0.002465, val loss: 0.005837, val mae: 0.053714, val r2: 0.852343\n",
      "epoch: 170, train loss: 0.002469, val loss: 0.005941, val mae: 0.054304, val r2: 0.849528\n",
      "epoch: 171, train loss: 0.002470, val loss: 0.006135, val mae: 0.055389, val r2: 0.844247\n",
      "epoch: 172, train loss: 0.002473, val loss: 0.006298, val mae: 0.056261, val r2: 0.839713\n",
      "epoch: 173, train loss: 0.002478, val loss: 0.006538, val mae: 0.057485, val r2: 0.833221\n",
      "epoch: 174, train loss: 0.002487, val loss: 0.006742, val mae: 0.058485, val r2: 0.827451\n",
      "epoch: 175, train loss: 0.002494, val loss: 0.006913, val mae: 0.059329, val r2: 0.822672\n",
      "epoch: 176, train loss: 0.002515, val loss: 0.006999, val mae: 0.059722, val r2: 0.820172\n",
      "epoch: 177, train loss: 0.002525, val loss: 0.007238, val mae: 0.060831, val r2: 0.813540\n",
      "epoch: 178, train loss: 0.002549, val loss: 0.007312, val mae: 0.061097, val r2: 0.811320\n",
      "epoch: 179, train loss: 0.002570, val loss: 0.007784, val mae: 0.063154, val r2: 0.798467\n",
      "epoch: 180, train loss: 0.002592, val loss: 0.007904, val mae: 0.063640, val r2: 0.795304\n",
      "epoch: 181, train loss: 0.002628, val loss: 0.008237, val mae: 0.064940, val r2: 0.786311\n",
      "epoch: 182, train loss: 0.002659, val loss: 0.008787, val mae: 0.067220, val r2: 0.771445\n",
      "epoch: 183, train loss: 0.002684, val loss: 0.008748, val mae: 0.067106, val r2: 0.772933\n",
      "epoch: 184, train loss: 0.002721, val loss: 0.007875, val mae: 0.063361, val r2: 0.796336\n",
      "epoch: 185, train loss: 0.002767, val loss: 0.007088, val mae: 0.059955, val r2: 0.817685\n",
      "epoch: 186, train loss: 0.002830, val loss: 0.006491, val mae: 0.057322, val r2: 0.834000\n",
      "epoch: 187, train loss: 0.002891, val loss: 0.006110, val mae: 0.055384, val r2: 0.844793\n",
      "epoch: 188, train loss: 0.002915, val loss: 0.005539, val mae: 0.051957, val r2: 0.860727\n",
      "epoch: 189, train loss: 0.002925, val loss: 0.005441, val mae: 0.050774, val r2: 0.864002\n",
      "epoch: 190, train loss: 0.002931, val loss: 0.005577, val mae: 0.051212, val r2: 0.860407\n",
      "epoch: 191, train loss: 0.002882, val loss: 0.005511, val mae: 0.051028, val r2: 0.861981\n",
      "epoch: 192, train loss: 0.002878, val loss: 0.005367, val mae: 0.050561, val r2: 0.865653\n",
      "epoch: 193, train loss: 0.002898, val loss: 0.005333, val mae: 0.050567, val r2: 0.866480\n",
      "epoch: 194, train loss: 0.002832, val loss: 0.005296, val mae: 0.050432, val r2: 0.867404\n",
      "epoch: 195, train loss: 0.002712, val loss: 0.005274, val mae: 0.050387, val r2: 0.867988\n",
      "epoch: 196, train loss: 0.002599, val loss: 0.005293, val mae: 0.050479, val r2: 0.867456\n",
      "epoch: 197, train loss: 0.002521, val loss: 0.005305, val mae: 0.050670, val r2: 0.866978\n",
      "epoch: 198, train loss: 0.002477, val loss: 0.005358, val mae: 0.051020, val r2: 0.865472\n",
      "epoch: 199, train loss: 0.002444, val loss: 0.005395, val mae: 0.051307, val r2: 0.864323\n",
      "epoch: 200, train loss: 0.002442, val loss: 0.005409, val mae: 0.051431, val r2: 0.863839\n",
      "epoch: 201, train loss: 0.002428, val loss: 0.005487, val mae: 0.051918, val r2: 0.861611\n",
      "epoch: 202, train loss: 0.002404, val loss: 0.005563, val mae: 0.052364, val r2: 0.859507\n",
      "epoch: 203, train loss: 0.002384, val loss: 0.005587, val mae: 0.052490, val r2: 0.858827\n",
      "epoch: 204, train loss: 0.002367, val loss: 0.005605, val mae: 0.052595, val r2: 0.858376\n",
      "epoch: 205, train loss: 0.002350, val loss: 0.005597, val mae: 0.052526, val r2: 0.858656\n",
      "epoch: 206, train loss: 0.002337, val loss: 0.005539, val mae: 0.052199, val r2: 0.860311\n",
      "epoch: 207, train loss: 0.002329, val loss: 0.005441, val mae: 0.051537, val r2: 0.863117\n",
      "epoch: 208, train loss: 0.002324, val loss: 0.005342, val mae: 0.050878, val r2: 0.865966\n",
      "epoch: 209, train loss: 0.002325, val loss: 0.005315, val mae: 0.050641, val r2: 0.866823\n",
      "epoch: 210, train loss: 0.002336, val loss: 0.005295, val mae: 0.050345, val r2: 0.867504\n",
      "epoch: 211, train loss: 0.002336, val loss: 0.005313, val mae: 0.050304, val r2: 0.867017\n",
      "epoch: 212, train loss: 0.002329, val loss: 0.005309, val mae: 0.050298, val r2: 0.867108\n",
      "epoch: 213, train loss: 0.002321, val loss: 0.005322, val mae: 0.050335, val r2: 0.866866\n",
      "epoch: 214, train loss: 0.002307, val loss: 0.005310, val mae: 0.050290, val r2: 0.867166\n",
      "epoch: 215, train loss: 0.002289, val loss: 0.005321, val mae: 0.050296, val r2: 0.866905\n",
      "epoch: 216, train loss: 0.002276, val loss: 0.005351, val mae: 0.050537, val r2: 0.866068\n",
      "epoch: 217, train loss: 0.002251, val loss: 0.005336, val mae: 0.050535, val r2: 0.866312\n",
      "epoch: 218, train loss: 0.002233, val loss: 0.005321, val mae: 0.050543, val r2: 0.866719\n",
      "epoch: 219, train loss: 0.002219, val loss: 0.005351, val mae: 0.050837, val r2: 0.865686\n",
      "epoch: 220, train loss: 0.002206, val loss: 0.005404, val mae: 0.051174, val r2: 0.864138\n",
      "epoch: 221, train loss: 0.002198, val loss: 0.005484, val mae: 0.051766, val r2: 0.861830\n",
      "epoch: 222, train loss: 0.002194, val loss: 0.005573, val mae: 0.052341, val r2: 0.859323\n",
      "epoch: 223, train loss: 0.002194, val loss: 0.005732, val mae: 0.053318, val r2: 0.854860\n",
      "epoch: 224, train loss: 0.002190, val loss: 0.005829, val mae: 0.053868, val r2: 0.852132\n",
      "epoch: 225, train loss: 0.002190, val loss: 0.005946, val mae: 0.054547, val r2: 0.848825\n",
      "epoch: 226, train loss: 0.002192, val loss: 0.006003, val mae: 0.054824, val r2: 0.847329\n",
      "epoch: 227, train loss: 0.002192, val loss: 0.006075, val mae: 0.055222, val r2: 0.845457\n",
      "epoch: 228, train loss: 0.002193, val loss: 0.006054, val mae: 0.055105, val r2: 0.846070\n",
      "epoch: 229, train loss: 0.002191, val loss: 0.006085, val mae: 0.055216, val r2: 0.845426\n",
      "epoch: 230, train loss: 0.002189, val loss: 0.005908, val mae: 0.054297, val r2: 0.850412\n",
      "epoch: 231, train loss: 0.002208, val loss: 0.005758, val mae: 0.053505, val r2: 0.854639\n",
      "epoch: 232, train loss: 0.002233, val loss: 0.005518, val mae: 0.052108, val r2: 0.861267\n",
      "epoch: 233, train loss: 0.002263, val loss: 0.005329, val mae: 0.050807, val r2: 0.866446\n",
      "epoch: 234, train loss: 0.002262, val loss: 0.005372, val mae: 0.050636, val r2: 0.865661\n",
      "epoch: 235, train loss: 0.002255, val loss: 0.005582, val mae: 0.051415, val r2: 0.860227\n",
      "epoch: 236, train loss: 0.002238, val loss: 0.005746, val mae: 0.052064, val r2: 0.855951\n",
      "epoch: 237, train loss: 0.002234, val loss: 0.005752, val mae: 0.052143, val r2: 0.855961\n",
      "epoch: 238, train loss: 0.002239, val loss: 0.005676, val mae: 0.051816, val r2: 0.857852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 239, train loss: 0.002268, val loss: 0.005513, val mae: 0.051206, val r2: 0.862024\n",
      "epoch: 240, train loss: 0.002276, val loss: 0.005450, val mae: 0.051021, val r2: 0.863530\n",
      "epoch: 241, train loss: 0.002258, val loss: 0.005432, val mae: 0.051108, val r2: 0.863906\n",
      "epoch: 242, train loss: 0.002219, val loss: 0.005446, val mae: 0.051351, val r2: 0.863426\n",
      "epoch: 243, train loss: 0.002189, val loss: 0.005500, val mae: 0.051742, val r2: 0.861734\n",
      "epoch: 244, train loss: 0.002166, val loss: 0.005551, val mae: 0.052120, val r2: 0.860125\n",
      "epoch: 245, train loss: 0.002148, val loss: 0.005675, val mae: 0.052879, val r2: 0.856522\n",
      "epoch: 246, train loss: 0.002126, val loss: 0.005747, val mae: 0.053317, val r2: 0.854357\n",
      "epoch: 247, train loss: 0.002126, val loss: 0.005814, val mae: 0.053662, val r2: 0.852492\n",
      "epoch: 248, train loss: 0.002135, val loss: 0.005797, val mae: 0.053556, val r2: 0.853062\n",
      "epoch: 249, train loss: 0.002139, val loss: 0.005793, val mae: 0.053655, val r2: 0.853366\n",
      "epoch: 250, train loss: 0.002129, val loss: 0.005737, val mae: 0.053418, val r2: 0.855045\n",
      "epoch: 251, train loss: 0.002123, val loss: 0.005486, val mae: 0.051876, val r2: 0.862162\n",
      "epoch: 252, train loss: 0.002123, val loss: 0.005355, val mae: 0.050953, val r2: 0.865829\n",
      "epoch: 253, train loss: 0.002120, val loss: 0.005364, val mae: 0.050642, val r2: 0.865936\n",
      "epoch: 254, train loss: 0.002102, val loss: 0.005470, val mae: 0.050919, val r2: 0.863323\n",
      "epoch: 255, train loss: 0.002088, val loss: 0.005605, val mae: 0.051485, val r2: 0.860000\n",
      "epoch: 256, train loss: 0.002089, val loss: 0.005603, val mae: 0.051540, val r2: 0.859973\n",
      "epoch: 257, train loss: 0.002098, val loss: 0.005481, val mae: 0.051114, val r2: 0.862835\n",
      "epoch: 258, train loss: 0.002096, val loss: 0.005442, val mae: 0.051195, val r2: 0.863502\n",
      "epoch: 259, train loss: 0.002076, val loss: 0.005593, val mae: 0.052261, val r2: 0.859257\n",
      "epoch: 260, train loss: 0.002033, val loss: 0.005770, val mae: 0.053390, val r2: 0.854368\n",
      "epoch: 261, train loss: 0.002025, val loss: 0.005735, val mae: 0.053273, val r2: 0.855081\n",
      "epoch: 262, train loss: 0.002038, val loss: 0.005593, val mae: 0.052578, val r2: 0.858809\n",
      "epoch: 263, train loss: 0.002024, val loss: 0.005461, val mae: 0.051708, val r2: 0.862511\n",
      "epoch: 264, train loss: 0.001998, val loss: 0.005422, val mae: 0.051424, val r2: 0.863620\n",
      "epoch: 265, train loss: 0.001983, val loss: 0.005427, val mae: 0.051386, val r2: 0.863517\n",
      "epoch: 266, train loss: 0.001978, val loss: 0.005473, val mae: 0.051625, val r2: 0.862345\n",
      "epoch: 267, train loss: 0.001977, val loss: 0.005495, val mae: 0.051704, val r2: 0.861777\n",
      "epoch: 268, train loss: 0.001981, val loss: 0.005489, val mae: 0.051703, val r2: 0.862039\n",
      "epoch: 269, train loss: 0.001984, val loss: 0.005468, val mae: 0.051505, val r2: 0.862789\n",
      "epoch: 270, train loss: 0.001993, val loss: 0.005382, val mae: 0.051015, val r2: 0.865111\n",
      "epoch: 271, train loss: 0.002001, val loss: 0.005327, val mae: 0.050675, val r2: 0.866765\n",
      "epoch: 272, train loss: 0.001999, val loss: 0.005314, val mae: 0.050569, val r2: 0.867222\n",
      "epoch: 273, train loss: 0.001993, val loss: 0.005375, val mae: 0.050663, val r2: 0.865821\n",
      "epoch: 274, train loss: 0.001989, val loss: 0.005412, val mae: 0.050711, val r2: 0.864924\n",
      "epoch: 275, train loss: 0.002008, val loss: 0.005387, val mae: 0.050702, val r2: 0.865331\n",
      "epoch: 276, train loss: 0.002036, val loss: 0.005465, val mae: 0.051228, val r2: 0.862995\n",
      "epoch: 277, train loss: 0.002026, val loss: 0.005649, val mae: 0.052374, val r2: 0.857979\n",
      "epoch: 278, train loss: 0.001992, val loss: 0.005804, val mae: 0.053285, val r2: 0.853608\n",
      "epoch: 279, train loss: 0.001979, val loss: 0.005918, val mae: 0.054092, val r2: 0.850278\n",
      "epoch: 280, train loss: 0.001973, val loss: 0.006087, val mae: 0.055167, val r2: 0.845707\n",
      "epoch: 281, train loss: 0.001969, val loss: 0.006036, val mae: 0.054907, val r2: 0.846772\n",
      "epoch: 282, train loss: 0.001963, val loss: 0.005781, val mae: 0.053496, val r2: 0.853680\n",
      "epoch: 283, train loss: 0.001959, val loss: 0.005607, val mae: 0.052473, val r2: 0.858547\n",
      "epoch: 284, train loss: 0.001935, val loss: 0.005555, val mae: 0.052204, val r2: 0.860044\n",
      "epoch: 285, train loss: 0.001909, val loss: 0.005507, val mae: 0.051875, val r2: 0.861502\n",
      "epoch: 286, train loss: 0.001903, val loss: 0.005476, val mae: 0.051680, val r2: 0.862381\n",
      "epoch: 287, train loss: 0.001909, val loss: 0.005453, val mae: 0.051541, val r2: 0.863118\n",
      "epoch: 288, train loss: 0.001909, val loss: 0.005440, val mae: 0.051388, val r2: 0.863593\n",
      "epoch: 289, train loss: 0.001902, val loss: 0.005431, val mae: 0.051274, val r2: 0.863992\n",
      "epoch: 290, train loss: 0.001901, val loss: 0.005419, val mae: 0.051228, val r2: 0.864346\n",
      "epoch: 291, train loss: 0.001901, val loss: 0.005343, val mae: 0.050733, val r2: 0.866337\n",
      "epoch: 292, train loss: 0.001895, val loss: 0.005342, val mae: 0.050670, val r2: 0.866428\n",
      "epoch: 293, train loss: 0.001896, val loss: 0.005338, val mae: 0.050543, val r2: 0.866581\n",
      "epoch: 294, train loss: 0.001897, val loss: 0.005354, val mae: 0.050578, val r2: 0.866173\n",
      "epoch: 295, train loss: 0.001891, val loss: 0.005375, val mae: 0.050687, val r2: 0.865609\n",
      "epoch: 296, train loss: 0.001899, val loss: 0.005453, val mae: 0.051142, val r2: 0.863441\n",
      "epoch: 297, train loss: 0.001903, val loss: 0.005561, val mae: 0.051863, val r2: 0.860355\n",
      "epoch: 298, train loss: 0.001908, val loss: 0.005744, val mae: 0.052981, val r2: 0.855164\n",
      "epoch: 299, train loss: 0.001902, val loss: 0.006092, val mae: 0.054817, val r2: 0.845855\n",
      "epoch: 300, train loss: 0.001887, val loss: 0.006534, val mae: 0.057217, val r2: 0.833813\n",
      "epoch: 301, train loss: 0.001881, val loss: 0.006692, val mae: 0.058104, val r2: 0.829357\n",
      "epoch: 302, train loss: 0.001902, val loss: 0.006306, val mae: 0.056263, val r2: 0.839779\n",
      "epoch: 303, train loss: 0.001921, val loss: 0.005791, val mae: 0.053576, val r2: 0.853654\n",
      "epoch: 304, train loss: 0.001911, val loss: 0.005508, val mae: 0.051922, val r2: 0.861522\n",
      "epoch: 305, train loss: 0.001886, val loss: 0.005465, val mae: 0.051503, val r2: 0.862874\n",
      "epoch: 306, train loss: 0.001870, val loss: 0.005468, val mae: 0.051516, val r2: 0.862808\n",
      "epoch: 307, train loss: 0.001876, val loss: 0.005511, val mae: 0.051711, val r2: 0.861630\n",
      "epoch: 308, train loss: 0.001871, val loss: 0.005556, val mae: 0.051945, val r2: 0.860424\n",
      "epoch: 309, train loss: 0.001855, val loss: 0.005613, val mae: 0.052195, val r2: 0.859093\n",
      "epoch: 310, train loss: 0.001847, val loss: 0.005533, val mae: 0.051802, val r2: 0.861221\n",
      "epoch: 311, train loss: 0.001848, val loss: 0.005400, val mae: 0.051090, val r2: 0.864664\n",
      "epoch: 312, train loss: 0.001835, val loss: 0.005386, val mae: 0.050841, val r2: 0.865148\n",
      "epoch: 313, train loss: 0.001827, val loss: 0.005447, val mae: 0.051243, val r2: 0.863442\n",
      "epoch: 314, train loss: 0.001840, val loss: 0.005596, val mae: 0.052118, val r2: 0.859278\n",
      "epoch: 315, train loss: 0.001856, val loss: 0.006041, val mae: 0.054527, val r2: 0.847295\n",
      "epoch: 316, train loss: 0.001835, val loss: 0.006432, val mae: 0.056619, val r2: 0.836818\n",
      "epoch: 317, train loss: 0.001803, val loss: 0.006419, val mae: 0.056635, val r2: 0.836890\n",
      "epoch: 318, train loss: 0.001816, val loss: 0.006118, val mae: 0.055244, val r2: 0.844817\n",
      "epoch: 319, train loss: 0.001809, val loss: 0.005799, val mae: 0.053577, val r2: 0.853560\n",
      "epoch: 320, train loss: 0.001789, val loss: 0.005604, val mae: 0.052582, val r2: 0.858938\n",
      "epoch: 321, train loss: 0.001766, val loss: 0.005521, val mae: 0.051949, val r2: 0.861430\n",
      "epoch: 322, train loss: 0.001761, val loss: 0.005490, val mae: 0.051744, val r2: 0.862299\n",
      "epoch: 323, train loss: 0.001758, val loss: 0.005525, val mae: 0.051849, val r2: 0.861380\n",
      "epoch: 324, train loss: 0.001756, val loss: 0.005526, val mae: 0.051741, val r2: 0.861467\n",
      "epoch: 325, train loss: 0.001747, val loss: 0.005518, val mae: 0.051702, val r2: 0.861608\n",
      "epoch: 326, train loss: 0.001741, val loss: 0.005509, val mae: 0.051632, val r2: 0.861837\n",
      "epoch: 327, train loss: 0.001742, val loss: 0.005474, val mae: 0.051439, val r2: 0.862746\n",
      "epoch: 328, train loss: 0.001736, val loss: 0.005475, val mae: 0.051393, val r2: 0.862816\n",
      "epoch: 329, train loss: 0.001734, val loss: 0.005529, val mae: 0.051718, val r2: 0.861260\n",
      "epoch: 330, train loss: 0.001737, val loss: 0.005712, val mae: 0.052733, val r2: 0.856310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 331, train loss: 0.001740, val loss: 0.005946, val mae: 0.054011, val r2: 0.849896\n",
      "epoch: 332, train loss: 0.001738, val loss: 0.006202, val mae: 0.055277, val r2: 0.843132\n",
      "epoch: 333, train loss: 0.001730, val loss: 0.006368, val mae: 0.056197, val r2: 0.838480\n",
      "epoch: 334, train loss: 0.001736, val loss: 0.006466, val mae: 0.056839, val r2: 0.835844\n",
      "epoch: 335, train loss: 0.001750, val loss: 0.006129, val mae: 0.055115, val r2: 0.844777\n",
      "epoch: 336, train loss: 0.001763, val loss: 0.005836, val mae: 0.053742, val r2: 0.852629\n",
      "epoch: 337, train loss: 0.001758, val loss: 0.005581, val mae: 0.052218, val r2: 0.859756\n",
      "epoch: 338, train loss: 0.001755, val loss: 0.005515, val mae: 0.051793, val r2: 0.861629\n",
      "epoch: 339, train loss: 0.001742, val loss: 0.005548, val mae: 0.051842, val r2: 0.860809\n",
      "epoch: 340, train loss: 0.001742, val loss: 0.005508, val mae: 0.051502, val r2: 0.861929\n",
      "epoch: 341, train loss: 0.001743, val loss: 0.005521, val mae: 0.051591, val r2: 0.861519\n",
      "epoch: 342, train loss: 0.001741, val loss: 0.005561, val mae: 0.051756, val r2: 0.860621\n",
      "epoch: 343, train loss: 0.001737, val loss: 0.005623, val mae: 0.052246, val r2: 0.859093\n",
      "epoch: 344, train loss: 0.001733, val loss: 0.005749, val mae: 0.053018, val r2: 0.855834\n",
      "epoch: 345, train loss: 0.001739, val loss: 0.005883, val mae: 0.053784, val r2: 0.852355\n",
      "epoch: 346, train loss: 0.001753, val loss: 0.006126, val mae: 0.054993, val r2: 0.845877\n",
      "epoch: 347, train loss: 0.001779, val loss: 0.006263, val mae: 0.055627, val r2: 0.841839\n",
      "epoch: 348, train loss: 0.001786, val loss: 0.006215, val mae: 0.055380, val r2: 0.842833\n",
      "epoch: 349, train loss: 0.001764, val loss: 0.006092, val mae: 0.054931, val r2: 0.846022\n"
     ]
    }
   ],
   "source": [
    "# * v1\n",
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 32,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 1,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 6, \n",
    "                      'epoches': 350,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 9, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b55bbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps!\n",
      "Transformer(\n",
      "  (element_embedding_temporal): Linear(in_features=196, out_features=128, bias=True)\n",
      "  (element_embedding_spatial): Linear(in_features=2695, out_features=1760, bias=True)\n",
      "  (pos_embedding): Embedding(4, 32)\n",
      "  (variable_embedding): Embedding(55, 32)\n",
      "  (temporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatial): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatiotemporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (keys): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (queries): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatter): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=7700, out_features=2640, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=2640, out_features=330, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch: 0, train loss: 0.018427, val loss: 0.015085, val mae: 0.096555, val r2: 0.617647\n",
      "epoch: 1, train loss: 0.010463, val loss: 0.012439, val mae: 0.084828, val r2: 0.684778\n",
      "epoch: 2, train loss: 0.009047, val loss: 0.011430, val mae: 0.080483, val r2: 0.710391\n",
      "epoch: 3, train loss: 0.008284, val loss: 0.010682, val mae: 0.077394, val r2: 0.729447\n",
      "epoch: 4, train loss: 0.007690, val loss: 0.010052, val mae: 0.074769, val r2: 0.745555\n",
      "epoch: 5, train loss: 0.007227, val loss: 0.009602, val mae: 0.072970, val r2: 0.757149\n",
      "epoch: 6, train loss: 0.006846, val loss: 0.009205, val mae: 0.071285, val r2: 0.767559\n",
      "epoch: 7, train loss: 0.006500, val loss: 0.008912, val mae: 0.069952, val r2: 0.775120\n",
      "epoch: 8, train loss: 0.006210, val loss: 0.008694, val mae: 0.068934, val r2: 0.780810\n",
      "epoch: 9, train loss: 0.005944, val loss: 0.008426, val mae: 0.067710, val r2: 0.787834\n",
      "epoch: 10, train loss: 0.005699, val loss: 0.008253, val mae: 0.066841, val r2: 0.792279\n",
      "epoch: 11, train loss: 0.005486, val loss: 0.007988, val mae: 0.065486, val r2: 0.799079\n",
      "epoch: 12, train loss: 0.005298, val loss: 0.007808, val mae: 0.064612, val r2: 0.803750\n",
      "epoch: 13, train loss: 0.005134, val loss: 0.007694, val mae: 0.063970, val r2: 0.806653\n",
      "epoch: 14, train loss: 0.004981, val loss: 0.007512, val mae: 0.063032, val r2: 0.811326\n",
      "epoch: 15, train loss: 0.004849, val loss: 0.007457, val mae: 0.062655, val r2: 0.812861\n",
      "epoch: 16, train loss: 0.004733, val loss: 0.007327, val mae: 0.061988, val r2: 0.816152\n",
      "epoch: 17, train loss: 0.004618, val loss: 0.007212, val mae: 0.061309, val r2: 0.819146\n",
      "epoch: 18, train loss: 0.004511, val loss: 0.007154, val mae: 0.060980, val r2: 0.820711\n",
      "epoch: 19, train loss: 0.004426, val loss: 0.007010, val mae: 0.060191, val r2: 0.824282\n",
      "epoch: 20, train loss: 0.004341, val loss: 0.006959, val mae: 0.059861, val r2: 0.825583\n",
      "epoch: 21, train loss: 0.004263, val loss: 0.006927, val mae: 0.059714, val r2: 0.826553\n",
      "epoch: 22, train loss: 0.004196, val loss: 0.006843, val mae: 0.059257, val r2: 0.828785\n",
      "epoch: 23, train loss: 0.004138, val loss: 0.006825, val mae: 0.059204, val r2: 0.829260\n",
      "epoch: 24, train loss: 0.004080, val loss: 0.006754, val mae: 0.058785, val r2: 0.831085\n",
      "epoch: 25, train loss: 0.004033, val loss: 0.006763, val mae: 0.058759, val r2: 0.831013\n",
      "epoch: 26, train loss: 0.003985, val loss: 0.006739, val mae: 0.058643, val r2: 0.831634\n",
      "epoch: 27, train loss: 0.003928, val loss: 0.006718, val mae: 0.058532, val r2: 0.832276\n",
      "epoch: 28, train loss: 0.003895, val loss: 0.006739, val mae: 0.058661, val r2: 0.831855\n",
      "epoch: 29, train loss: 0.003857, val loss: 0.006696, val mae: 0.058394, val r2: 0.833011\n",
      "epoch: 30, train loss: 0.003822, val loss: 0.006674, val mae: 0.058317, val r2: 0.833558\n",
      "epoch: 31, train loss: 0.003792, val loss: 0.006736, val mae: 0.058559, val r2: 0.832088\n",
      "epoch: 32, train loss: 0.003760, val loss: 0.006663, val mae: 0.058286, val r2: 0.833945\n",
      "epoch: 33, train loss: 0.003732, val loss: 0.006613, val mae: 0.057988, val r2: 0.835202\n",
      "epoch: 34, train loss: 0.003708, val loss: 0.006621, val mae: 0.058072, val r2: 0.835042\n",
      "epoch: 35, train loss: 0.003686, val loss: 0.006641, val mae: 0.058188, val r2: 0.834533\n",
      "epoch: 36, train loss: 0.003667, val loss: 0.006669, val mae: 0.058295, val r2: 0.833874\n",
      "epoch: 37, train loss: 0.003653, val loss: 0.006681, val mae: 0.058328, val r2: 0.833604\n",
      "epoch: 38, train loss: 0.003626, val loss: 0.006606, val mae: 0.057928, val r2: 0.835479\n",
      "epoch: 39, train loss: 0.003619, val loss: 0.006588, val mae: 0.057895, val r2: 0.835874\n",
      "epoch: 40, train loss: 0.003621, val loss: 0.006499, val mae: 0.057406, val r2: 0.838059\n",
      "epoch: 41, train loss: 0.003622, val loss: 0.006361, val mae: 0.056695, val r2: 0.841448\n",
      "epoch: 42, train loss: 0.003619, val loss: 0.006307, val mae: 0.056431, val r2: 0.842802\n",
      "epoch: 43, train loss: 0.003633, val loss: 0.006144, val mae: 0.055573, val r2: 0.846698\n",
      "epoch: 44, train loss: 0.003643, val loss: 0.005929, val mae: 0.054302, val r2: 0.851910\n",
      "epoch: 45, train loss: 0.003656, val loss: 0.005768, val mae: 0.053393, val r2: 0.855762\n",
      "epoch: 46, train loss: 0.003654, val loss: 0.005616, val mae: 0.052375, val r2: 0.859371\n",
      "epoch: 47, train loss: 0.003620, val loss: 0.005598, val mae: 0.052187, val r2: 0.859753\n",
      "epoch: 48, train loss: 0.003568, val loss: 0.005614, val mae: 0.052091, val r2: 0.859248\n",
      "epoch: 49, train loss: 0.003502, val loss: 0.005627, val mae: 0.052130, val r2: 0.858966\n",
      "epoch: 50, train loss: 0.003430, val loss: 0.005608, val mae: 0.052012, val r2: 0.859346\n",
      "epoch: 51, train loss: 0.003377, val loss: 0.005586, val mae: 0.051919, val r2: 0.859941\n",
      "epoch: 52, train loss: 0.003342, val loss: 0.005590, val mae: 0.051906, val r2: 0.859851\n",
      "epoch: 53, train loss: 0.003327, val loss: 0.005590, val mae: 0.051873, val r2: 0.859817\n",
      "epoch: 54, train loss: 0.003316, val loss: 0.005575, val mae: 0.051789, val r2: 0.860192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55, train loss: 0.003318, val loss: 0.005575, val mae: 0.051741, val r2: 0.860228\n",
      "epoch: 56, train loss: 0.003324, val loss: 0.005596, val mae: 0.051836, val r2: 0.859653\n",
      "epoch: 57, train loss: 0.003323, val loss: 0.005598, val mae: 0.051802, val r2: 0.859590\n",
      "epoch: 58, train loss: 0.003340, val loss: 0.005597, val mae: 0.051795, val r2: 0.859634\n",
      "epoch: 59, train loss: 0.003363, val loss: 0.005570, val mae: 0.051713, val r2: 0.860355\n",
      "epoch: 60, train loss: 0.003382, val loss: 0.005586, val mae: 0.051842, val r2: 0.859954\n",
      "epoch: 61, train loss: 0.003398, val loss: 0.005630, val mae: 0.052168, val r2: 0.858955\n",
      "epoch: 62, train loss: 0.003400, val loss: 0.005665, val mae: 0.052421, val r2: 0.858327\n",
      "epoch: 63, train loss: 0.003376, val loss: 0.005742, val mae: 0.052883, val r2: 0.856599\n",
      "epoch: 64, train loss: 0.003317, val loss: 0.005820, val mae: 0.053341, val r2: 0.854779\n",
      "epoch: 65, train loss: 0.003263, val loss: 0.005827, val mae: 0.053384, val r2: 0.854733\n",
      "epoch: 66, train loss: 0.003197, val loss: 0.005788, val mae: 0.053127, val r2: 0.855751\n",
      "epoch: 67, train loss: 0.003152, val loss: 0.005745, val mae: 0.052903, val r2: 0.856854\n",
      "epoch: 68, train loss: 0.003143, val loss: 0.005736, val mae: 0.052927, val r2: 0.857133\n",
      "epoch: 69, train loss: 0.003162, val loss: 0.005723, val mae: 0.052883, val r2: 0.857476\n",
      "epoch: 70, train loss: 0.003188, val loss: 0.005634, val mae: 0.052437, val r2: 0.859569\n",
      "epoch: 71, train loss: 0.003234, val loss: 0.005560, val mae: 0.052059, val r2: 0.861251\n",
      "epoch: 72, train loss: 0.003279, val loss: 0.005415, val mae: 0.051125, val r2: 0.864600\n",
      "epoch: 73, train loss: 0.003283, val loss: 0.005397, val mae: 0.050846, val r2: 0.864851\n",
      "epoch: 74, train loss: 0.003255, val loss: 0.005441, val mae: 0.050938, val r2: 0.863597\n",
      "epoch: 75, train loss: 0.003192, val loss: 0.005513, val mae: 0.051204, val r2: 0.861717\n",
      "epoch: 76, train loss: 0.003101, val loss: 0.005493, val mae: 0.051119, val r2: 0.862196\n",
      "epoch: 77, train loss: 0.003037, val loss: 0.005458, val mae: 0.050973, val r2: 0.863063\n",
      "epoch: 78, train loss: 0.002998, val loss: 0.005436, val mae: 0.050868, val r2: 0.863599\n",
      "epoch: 79, train loss: 0.002973, val loss: 0.005411, val mae: 0.050765, val r2: 0.864243\n",
      "epoch: 80, train loss: 0.002961, val loss: 0.005406, val mae: 0.050740, val r2: 0.864344\n",
      "epoch: 81, train loss: 0.002950, val loss: 0.005378, val mae: 0.050614, val r2: 0.865095\n",
      "epoch: 82, train loss: 0.002937, val loss: 0.005411, val mae: 0.050794, val r2: 0.864157\n",
      "epoch: 83, train loss: 0.002930, val loss: 0.005420, val mae: 0.050833, val r2: 0.863988\n",
      "epoch: 84, train loss: 0.002914, val loss: 0.005381, val mae: 0.050671, val r2: 0.865049\n",
      "epoch: 85, train loss: 0.002905, val loss: 0.005379, val mae: 0.050682, val r2: 0.865077\n",
      "epoch: 86, train loss: 0.002885, val loss: 0.005361, val mae: 0.050580, val r2: 0.865517\n",
      "epoch: 87, train loss: 0.002874, val loss: 0.005393, val mae: 0.050709, val r2: 0.864714\n",
      "epoch: 88, train loss: 0.002858, val loss: 0.005367, val mae: 0.050643, val r2: 0.865495\n",
      "epoch: 89, train loss: 0.002845, val loss: 0.005340, val mae: 0.050477, val r2: 0.866185\n",
      "epoch: 90, train loss: 0.002836, val loss: 0.005327, val mae: 0.050530, val r2: 0.866529\n",
      "epoch: 91, train loss: 0.002825, val loss: 0.005302, val mae: 0.050423, val r2: 0.867268\n",
      "epoch: 92, train loss: 0.002815, val loss: 0.005313, val mae: 0.050474, val r2: 0.866909\n",
      "epoch: 93, train loss: 0.002799, val loss: 0.005286, val mae: 0.050320, val r2: 0.867647\n",
      "epoch: 94, train loss: 0.002792, val loss: 0.005295, val mae: 0.050404, val r2: 0.867431\n",
      "epoch: 95, train loss: 0.002777, val loss: 0.005296, val mae: 0.050382, val r2: 0.867348\n",
      "epoch: 96, train loss: 0.002773, val loss: 0.005302, val mae: 0.050490, val r2: 0.867320\n",
      "epoch: 97, train loss: 0.002763, val loss: 0.005301, val mae: 0.050509, val r2: 0.867373\n",
      "epoch: 98, train loss: 0.002758, val loss: 0.005286, val mae: 0.050436, val r2: 0.867761\n",
      "epoch: 99, train loss: 0.002749, val loss: 0.005318, val mae: 0.050643, val r2: 0.866988\n",
      "epoch: 100, train loss: 0.002735, val loss: 0.005294, val mae: 0.050599, val r2: 0.867666\n",
      "epoch: 101, train loss: 0.002732, val loss: 0.005309, val mae: 0.050626, val r2: 0.867257\n",
      "epoch: 102, train loss: 0.002730, val loss: 0.005361, val mae: 0.050939, val r2: 0.866038\n",
      "epoch: 103, train loss: 0.002728, val loss: 0.005334, val mae: 0.050858, val r2: 0.866733\n",
      "epoch: 104, train loss: 0.002723, val loss: 0.005406, val mae: 0.051271, val r2: 0.864987\n",
      "epoch: 105, train loss: 0.002735, val loss: 0.005432, val mae: 0.051439, val r2: 0.864311\n",
      "epoch: 106, train loss: 0.002732, val loss: 0.005502, val mae: 0.051815, val r2: 0.862678\n",
      "epoch: 107, train loss: 0.002751, val loss: 0.005567, val mae: 0.052124, val r2: 0.861072\n",
      "epoch: 108, train loss: 0.002793, val loss: 0.005487, val mae: 0.051691, val r2: 0.862946\n",
      "epoch: 109, train loss: 0.002823, val loss: 0.005420, val mae: 0.051274, val r2: 0.864425\n",
      "epoch: 110, train loss: 0.002896, val loss: 0.005301, val mae: 0.050420, val r2: 0.866978\n",
      "epoch: 111, train loss: 0.002997, val loss: 0.005444, val mae: 0.050792, val r2: 0.863136\n",
      "epoch: 112, train loss: 0.003052, val loss: 0.006041, val mae: 0.053507, val r2: 0.848097\n",
      "epoch: 113, train loss: 0.003026, val loss: 0.006163, val mae: 0.054090, val r2: 0.845012\n",
      "epoch: 114, train loss: 0.003006, val loss: 0.005762, val mae: 0.052291, val r2: 0.854599\n",
      "epoch: 115, train loss: 0.003089, val loss: 0.005586, val mae: 0.051610, val r2: 0.859007\n",
      "epoch: 116, train loss: 0.003114, val loss: 0.005651, val mae: 0.052149, val r2: 0.857835\n",
      "epoch: 117, train loss: 0.003023, val loss: 0.005722, val mae: 0.052549, val r2: 0.856286\n",
      "epoch: 118, train loss: 0.002892, val loss: 0.005618, val mae: 0.052055, val r2: 0.858833\n",
      "epoch: 119, train loss: 0.002802, val loss: 0.005523, val mae: 0.051495, val r2: 0.861297\n",
      "epoch: 120, train loss: 0.002772, val loss: 0.005466, val mae: 0.051242, val r2: 0.862673\n",
      "epoch: 121, train loss: 0.002762, val loss: 0.005487, val mae: 0.051337, val r2: 0.862113\n",
      "epoch: 122, train loss: 0.002754, val loss: 0.005504, val mae: 0.051470, val r2: 0.861598\n",
      "epoch: 123, train loss: 0.002758, val loss: 0.005553, val mae: 0.051734, val r2: 0.860345\n",
      "epoch: 124, train loss: 0.002749, val loss: 0.005626, val mae: 0.052177, val r2: 0.858500\n",
      "epoch: 125, train loss: 0.002744, val loss: 0.005669, val mae: 0.052466, val r2: 0.857432\n",
      "epoch: 126, train loss: 0.002742, val loss: 0.005653, val mae: 0.052446, val r2: 0.857677\n",
      "epoch: 127, train loss: 0.002746, val loss: 0.005768, val mae: 0.053080, val r2: 0.854803\n",
      "epoch: 128, train loss: 0.002739, val loss: 0.005841, val mae: 0.053576, val r2: 0.852791\n",
      "epoch: 129, train loss: 0.002747, val loss: 0.005884, val mae: 0.053827, val r2: 0.851734\n",
      "epoch: 130, train loss: 0.002741, val loss: 0.005981, val mae: 0.054389, val r2: 0.849142\n",
      "epoch: 131, train loss: 0.002745, val loss: 0.006092, val mae: 0.054997, val r2: 0.846213\n",
      "epoch: 132, train loss: 0.002745, val loss: 0.006240, val mae: 0.055895, val r2: 0.842177\n",
      "epoch: 133, train loss: 0.002764, val loss: 0.006352, val mae: 0.056501, val r2: 0.839219\n",
      "epoch: 134, train loss: 0.002767, val loss: 0.006536, val mae: 0.057472, val r2: 0.834308\n",
      "epoch: 135, train loss: 0.002784, val loss: 0.006607, val mae: 0.057882, val r2: 0.832136\n",
      "epoch: 136, train loss: 0.002796, val loss: 0.006766, val mae: 0.058699, val r2: 0.827891\n",
      "epoch: 137, train loss: 0.002814, val loss: 0.006870, val mae: 0.059235, val r2: 0.824932\n",
      "epoch: 138, train loss: 0.002823, val loss: 0.006960, val mae: 0.059694, val r2: 0.822201\n",
      "epoch: 139, train loss: 0.002846, val loss: 0.006967, val mae: 0.059797, val r2: 0.821913\n",
      "epoch: 140, train loss: 0.002843, val loss: 0.006903, val mae: 0.059436, val r2: 0.823741\n",
      "epoch: 141, train loss: 0.002843, val loss: 0.006733, val mae: 0.058598, val r2: 0.827908\n",
      "epoch: 142, train loss: 0.002837, val loss: 0.006593, val mae: 0.057879, val r2: 0.831526\n",
      "epoch: 143, train loss: 0.002827, val loss: 0.006512, val mae: 0.057446, val r2: 0.833744\n",
      "epoch: 144, train loss: 0.002799, val loss: 0.006157, val mae: 0.055641, val r2: 0.843133\n",
      "epoch: 145, train loss: 0.002774, val loss: 0.005939, val mae: 0.054486, val r2: 0.848978\n",
      "epoch: 146, train loss: 0.002746, val loss: 0.005812, val mae: 0.053828, val r2: 0.852425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 147, train loss: 0.002709, val loss: 0.005629, val mae: 0.052849, val r2: 0.857366\n",
      "epoch: 148, train loss: 0.002690, val loss: 0.005449, val mae: 0.051800, val r2: 0.862257\n",
      "epoch: 149, train loss: 0.002650, val loss: 0.005339, val mae: 0.051180, val r2: 0.865327\n",
      "epoch: 150, train loss: 0.002625, val loss: 0.005173, val mae: 0.050246, val r2: 0.869721\n",
      "epoch: 151, train loss: 0.002606, val loss: 0.005098, val mae: 0.049757, val r2: 0.871740\n",
      "epoch: 152, train loss: 0.002583, val loss: 0.005066, val mae: 0.049528, val r2: 0.872632\n",
      "epoch: 153, train loss: 0.002580, val loss: 0.004976, val mae: 0.048986, val r2: 0.875021\n",
      "epoch: 154, train loss: 0.002561, val loss: 0.004944, val mae: 0.048716, val r2: 0.875838\n",
      "epoch: 155, train loss: 0.002545, val loss: 0.004906, val mae: 0.048487, val r2: 0.876857\n",
      "epoch: 156, train loss: 0.002536, val loss: 0.004907, val mae: 0.048453, val r2: 0.876831\n",
      "epoch: 157, train loss: 0.002530, val loss: 0.004898, val mae: 0.048376, val r2: 0.877039\n",
      "epoch: 158, train loss: 0.002526, val loss: 0.004919, val mae: 0.048468, val r2: 0.876496\n",
      "epoch: 159, train loss: 0.002512, val loss: 0.004924, val mae: 0.048445, val r2: 0.876401\n",
      "epoch: 160, train loss: 0.002504, val loss: 0.004932, val mae: 0.048501, val r2: 0.876087\n",
      "epoch: 161, train loss: 0.002494, val loss: 0.004928, val mae: 0.048468, val r2: 0.876170\n",
      "epoch: 162, train loss: 0.002485, val loss: 0.004923, val mae: 0.048429, val r2: 0.876319\n",
      "epoch: 163, train loss: 0.002471, val loss: 0.004949, val mae: 0.048566, val r2: 0.875636\n",
      "epoch: 164, train loss: 0.002465, val loss: 0.004969, val mae: 0.048653, val r2: 0.875121\n",
      "epoch: 165, train loss: 0.002457, val loss: 0.004974, val mae: 0.048645, val r2: 0.875008\n",
      "epoch: 166, train loss: 0.002443, val loss: 0.004993, val mae: 0.048760, val r2: 0.874517\n",
      "epoch: 167, train loss: 0.002433, val loss: 0.005004, val mae: 0.048819, val r2: 0.874219\n",
      "epoch: 168, train loss: 0.002425, val loss: 0.005008, val mae: 0.048914, val r2: 0.874141\n",
      "epoch: 169, train loss: 0.002405, val loss: 0.004981, val mae: 0.048745, val r2: 0.874877\n",
      "epoch: 170, train loss: 0.002400, val loss: 0.005015, val mae: 0.048924, val r2: 0.873980\n",
      "epoch: 171, train loss: 0.002381, val loss: 0.004994, val mae: 0.048884, val r2: 0.874609\n",
      "epoch: 172, train loss: 0.002371, val loss: 0.005015, val mae: 0.049033, val r2: 0.874066\n",
      "epoch: 173, train loss: 0.002362, val loss: 0.005006, val mae: 0.048951, val r2: 0.874290\n",
      "epoch: 174, train loss: 0.002355, val loss: 0.004997, val mae: 0.048939, val r2: 0.874572\n",
      "epoch: 175, train loss: 0.002349, val loss: 0.005039, val mae: 0.049177, val r2: 0.873514\n",
      "epoch: 176, train loss: 0.002349, val loss: 0.005019, val mae: 0.049118, val r2: 0.874004\n",
      "epoch: 177, train loss: 0.002344, val loss: 0.005046, val mae: 0.049125, val r2: 0.873261\n",
      "epoch: 178, train loss: 0.002340, val loss: 0.005026, val mae: 0.049012, val r2: 0.873768\n",
      "epoch: 179, train loss: 0.002338, val loss: 0.005054, val mae: 0.049143, val r2: 0.873133\n",
      "epoch: 180, train loss: 0.002344, val loss: 0.005101, val mae: 0.049287, val r2: 0.871903\n",
      "epoch: 181, train loss: 0.002349, val loss: 0.005157, val mae: 0.049497, val r2: 0.870410\n",
      "epoch: 182, train loss: 0.002384, val loss: 0.005228, val mae: 0.049859, val r2: 0.868513\n",
      "epoch: 183, train loss: 0.002440, val loss: 0.005192, val mae: 0.049783, val r2: 0.869267\n",
      "epoch: 184, train loss: 0.002485, val loss: 0.005397, val mae: 0.051188, val r2: 0.863877\n",
      "epoch: 185, train loss: 0.002486, val loss: 0.006063, val mae: 0.054929, val r2: 0.846904\n",
      "epoch: 186, train loss: 0.002451, val loss: 0.006657, val mae: 0.057907, val r2: 0.831764\n",
      "epoch: 187, train loss: 0.002410, val loss: 0.006757, val mae: 0.058409, val r2: 0.828751\n",
      "epoch: 188, train loss: 0.002418, val loss: 0.006400, val mae: 0.056751, val r2: 0.837575\n",
      "epoch: 189, train loss: 0.002455, val loss: 0.005956, val mae: 0.054531, val r2: 0.848932\n",
      "epoch: 190, train loss: 0.002465, val loss: 0.005635, val mae: 0.052838, val r2: 0.857145\n",
      "epoch: 191, train loss: 0.002456, val loss: 0.005684, val mae: 0.053135, val r2: 0.855767\n",
      "epoch: 192, train loss: 0.002443, val loss: 0.005833, val mae: 0.053887, val r2: 0.851728\n",
      "epoch: 193, train loss: 0.002445, val loss: 0.006120, val mae: 0.055289, val r2: 0.844125\n",
      "epoch: 194, train loss: 0.002441, val loss: 0.006462, val mae: 0.057064, val r2: 0.835034\n",
      "epoch: 195, train loss: 0.002448, val loss: 0.006474, val mae: 0.057163, val r2: 0.834789\n",
      "epoch: 196, train loss: 0.002436, val loss: 0.006637, val mae: 0.058043, val r2: 0.830467\n",
      "epoch: 197, train loss: 0.002423, val loss: 0.006275, val mae: 0.056267, val r2: 0.839847\n",
      "epoch: 198, train loss: 0.002436, val loss: 0.006001, val mae: 0.054868, val r2: 0.847154\n",
      "epoch: 199, train loss: 0.002423, val loss: 0.005958, val mae: 0.054658, val r2: 0.848249\n",
      "epoch: 200, train loss: 0.002423, val loss: 0.005686, val mae: 0.053263, val r2: 0.855537\n",
      "epoch: 201, train loss: 0.002420, val loss: 0.005549, val mae: 0.052492, val r2: 0.859352\n",
      "epoch: 202, train loss: 0.002421, val loss: 0.005266, val mae: 0.050901, val r2: 0.867084\n",
      "epoch: 203, train loss: 0.002412, val loss: 0.005048, val mae: 0.049543, val r2: 0.873032\n",
      "epoch: 204, train loss: 0.002411, val loss: 0.004939, val mae: 0.048674, val r2: 0.875933\n",
      "epoch: 205, train loss: 0.002418, val loss: 0.004934, val mae: 0.048383, val r2: 0.876075\n",
      "epoch: 206, train loss: 0.002420, val loss: 0.004991, val mae: 0.048613, val r2: 0.874508\n",
      "epoch: 207, train loss: 0.002422, val loss: 0.005110, val mae: 0.049054, val r2: 0.871476\n",
      "epoch: 208, train loss: 0.002420, val loss: 0.005139, val mae: 0.049183, val r2: 0.870705\n",
      "epoch: 209, train loss: 0.002426, val loss: 0.005160, val mae: 0.049205, val r2: 0.870131\n",
      "epoch: 210, train loss: 0.002443, val loss: 0.005123, val mae: 0.049108, val r2: 0.871110\n",
      "epoch: 211, train loss: 0.002467, val loss: 0.005072, val mae: 0.048913, val r2: 0.872426\n",
      "epoch: 212, train loss: 0.002474, val loss: 0.005036, val mae: 0.048799, val r2: 0.873381\n",
      "epoch: 213, train loss: 0.002481, val loss: 0.005036, val mae: 0.048766, val r2: 0.873437\n",
      "epoch: 214, train loss: 0.002473, val loss: 0.004991, val mae: 0.048613, val r2: 0.874685\n",
      "epoch: 215, train loss: 0.002453, val loss: 0.004965, val mae: 0.048544, val r2: 0.875347\n",
      "epoch: 216, train loss: 0.002435, val loss: 0.004961, val mae: 0.048601, val r2: 0.875483\n",
      "epoch: 217, train loss: 0.002391, val loss: 0.004953, val mae: 0.048610, val r2: 0.875747\n",
      "epoch: 218, train loss: 0.002348, val loss: 0.004957, val mae: 0.048679, val r2: 0.875638\n",
      "epoch: 219, train loss: 0.002300, val loss: 0.004980, val mae: 0.048870, val r2: 0.875067\n",
      "epoch: 220, train loss: 0.002263, val loss: 0.005011, val mae: 0.049150, val r2: 0.874141\n",
      "epoch: 221, train loss: 0.002241, val loss: 0.005043, val mae: 0.049402, val r2: 0.873314\n",
      "epoch: 222, train loss: 0.002226, val loss: 0.005144, val mae: 0.050001, val r2: 0.870563\n",
      "epoch: 223, train loss: 0.002213, val loss: 0.005251, val mae: 0.050677, val r2: 0.867780\n",
      "epoch: 224, train loss: 0.002217, val loss: 0.005423, val mae: 0.051660, val r2: 0.863061\n",
      "epoch: 225, train loss: 0.002221, val loss: 0.005477, val mae: 0.051947, val r2: 0.861617\n",
      "epoch: 226, train loss: 0.002222, val loss: 0.005429, val mae: 0.051707, val r2: 0.862655\n",
      "epoch: 227, train loss: 0.002219, val loss: 0.005462, val mae: 0.051905, val r2: 0.861667\n",
      "epoch: 228, train loss: 0.002209, val loss: 0.005456, val mae: 0.051847, val r2: 0.861811\n",
      "epoch: 229, train loss: 0.002211, val loss: 0.005443, val mae: 0.051779, val r2: 0.862137\n",
      "epoch: 230, train loss: 0.002198, val loss: 0.005421, val mae: 0.051689, val r2: 0.862724\n",
      "epoch: 231, train loss: 0.002197, val loss: 0.005300, val mae: 0.050965, val r2: 0.866068\n",
      "epoch: 232, train loss: 0.002187, val loss: 0.005270, val mae: 0.050800, val r2: 0.866894\n",
      "epoch: 233, train loss: 0.002178, val loss: 0.005136, val mae: 0.049993, val r2: 0.870589\n",
      "epoch: 234, train loss: 0.002173, val loss: 0.005022, val mae: 0.049253, val r2: 0.873694\n",
      "epoch: 235, train loss: 0.002164, val loss: 0.004971, val mae: 0.048911, val r2: 0.875080\n",
      "epoch: 236, train loss: 0.002161, val loss: 0.004964, val mae: 0.048721, val r2: 0.875302\n",
      "epoch: 237, train loss: 0.002161, val loss: 0.004925, val mae: 0.048377, val r2: 0.876330\n",
      "epoch: 238, train loss: 0.002163, val loss: 0.004962, val mae: 0.048507, val r2: 0.875368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 239, train loss: 0.002165, val loss: 0.005083, val mae: 0.048977, val r2: 0.872073\n",
      "epoch: 240, train loss: 0.002166, val loss: 0.005060, val mae: 0.048867, val r2: 0.872834\n",
      "epoch: 241, train loss: 0.002164, val loss: 0.005250, val mae: 0.049739, val r2: 0.868088\n",
      "epoch: 242, train loss: 0.002161, val loss: 0.005493, val mae: 0.050777, val r2: 0.861922\n",
      "epoch: 243, train loss: 0.002153, val loss: 0.005569, val mae: 0.051057, val r2: 0.860251\n",
      "epoch: 244, train loss: 0.002195, val loss: 0.005418, val mae: 0.050371, val r2: 0.863899\n",
      "epoch: 245, train loss: 0.002247, val loss: 0.005072, val mae: 0.048918, val r2: 0.872559\n",
      "epoch: 246, train loss: 0.002238, val loss: 0.004989, val mae: 0.048763, val r2: 0.874827\n",
      "epoch: 247, train loss: 0.002195, val loss: 0.005093, val mae: 0.049544, val r2: 0.872190\n",
      "epoch: 248, train loss: 0.002152, val loss: 0.005116, val mae: 0.049800, val r2: 0.871575\n",
      "epoch: 249, train loss: 0.002138, val loss: 0.005062, val mae: 0.049553, val r2: 0.872839\n",
      "epoch: 250, train loss: 0.002136, val loss: 0.005012, val mae: 0.049256, val r2: 0.874114\n",
      "epoch: 251, train loss: 0.002118, val loss: 0.005004, val mae: 0.049087, val r2: 0.874384\n",
      "epoch: 252, train loss: 0.002095, val loss: 0.005016, val mae: 0.049195, val r2: 0.873978\n",
      "epoch: 253, train loss: 0.002089, val loss: 0.005103, val mae: 0.049703, val r2: 0.871654\n",
      "epoch: 254, train loss: 0.002086, val loss: 0.005211, val mae: 0.050386, val r2: 0.868739\n",
      "epoch: 255, train loss: 0.002074, val loss: 0.005418, val mae: 0.051630, val r2: 0.863210\n",
      "epoch: 256, train loss: 0.002062, val loss: 0.005441, val mae: 0.051778, val r2: 0.862491\n",
      "epoch: 257, train loss: 0.002064, val loss: 0.005362, val mae: 0.051402, val r2: 0.864499\n",
      "epoch: 258, train loss: 0.002076, val loss: 0.005247, val mae: 0.050733, val r2: 0.867477\n",
      "epoch: 259, train loss: 0.002081, val loss: 0.005108, val mae: 0.049873, val r2: 0.871397\n",
      "epoch: 260, train loss: 0.002071, val loss: 0.004996, val mae: 0.049130, val r2: 0.874674\n",
      "epoch: 261, train loss: 0.002057, val loss: 0.005036, val mae: 0.049111, val r2: 0.873956\n",
      "epoch: 262, train loss: 0.002040, val loss: 0.005151, val mae: 0.049385, val r2: 0.871225\n",
      "epoch: 263, train loss: 0.002043, val loss: 0.005337, val mae: 0.050049, val r2: 0.866653\n",
      "epoch: 264, train loss: 0.002058, val loss: 0.005374, val mae: 0.050203, val r2: 0.865449\n",
      "epoch: 265, train loss: 0.002076, val loss: 0.005184, val mae: 0.049407, val r2: 0.869966\n",
      "epoch: 266, train loss: 0.002078, val loss: 0.005026, val mae: 0.048842, val r2: 0.873883\n",
      "epoch: 267, train loss: 0.002060, val loss: 0.004950, val mae: 0.048677, val r2: 0.875854\n",
      "epoch: 268, train loss: 0.002062, val loss: 0.004920, val mae: 0.048610, val r2: 0.876610\n",
      "epoch: 269, train loss: 0.002066, val loss: 0.004932, val mae: 0.048526, val r2: 0.876484\n",
      "epoch: 270, train loss: 0.002048, val loss: 0.005105, val mae: 0.049187, val r2: 0.872330\n",
      "epoch: 271, train loss: 0.002020, val loss: 0.005223, val mae: 0.049471, val r2: 0.869355\n",
      "epoch: 272, train loss: 0.002014, val loss: 0.005159, val mae: 0.049227, val r2: 0.870794\n",
      "epoch: 273, train loss: 0.002026, val loss: 0.005027, val mae: 0.048761, val r2: 0.873905\n",
      "epoch: 274, train loss: 0.002018, val loss: 0.005016, val mae: 0.048990, val r2: 0.874117\n",
      "epoch: 275, train loss: 0.001993, val loss: 0.005024, val mae: 0.049152, val r2: 0.873842\n",
      "epoch: 276, train loss: 0.001980, val loss: 0.005090, val mae: 0.049640, val r2: 0.872172\n",
      "epoch: 277, train loss: 0.001986, val loss: 0.005066, val mae: 0.049608, val r2: 0.872647\n",
      "epoch: 278, train loss: 0.001984, val loss: 0.005007, val mae: 0.049219, val r2: 0.874210\n",
      "epoch: 279, train loss: 0.001973, val loss: 0.004993, val mae: 0.049023, val r2: 0.874748\n",
      "epoch: 280, train loss: 0.001960, val loss: 0.005016, val mae: 0.049096, val r2: 0.874216\n",
      "epoch: 281, train loss: 0.001952, val loss: 0.005036, val mae: 0.049115, val r2: 0.873787\n",
      "epoch: 282, train loss: 0.001953, val loss: 0.005039, val mae: 0.049087, val r2: 0.873657\n",
      "epoch: 283, train loss: 0.001950, val loss: 0.005068, val mae: 0.049290, val r2: 0.872844\n",
      "epoch: 284, train loss: 0.001950, val loss: 0.005065, val mae: 0.049318, val r2: 0.872871\n",
      "epoch: 285, train loss: 0.001950, val loss: 0.005125, val mae: 0.049782, val r2: 0.871228\n",
      "epoch: 286, train loss: 0.001952, val loss: 0.005153, val mae: 0.049995, val r2: 0.870487\n",
      "epoch: 287, train loss: 0.001960, val loss: 0.005157, val mae: 0.050149, val r2: 0.870177\n",
      "epoch: 288, train loss: 0.001982, val loss: 0.005186, val mae: 0.050434, val r2: 0.869574\n",
      "epoch: 289, train loss: 0.002007, val loss: 0.005063, val mae: 0.049694, val r2: 0.872908\n",
      "epoch: 290, train loss: 0.002005, val loss: 0.005136, val mae: 0.049754, val r2: 0.871481\n",
      "epoch: 291, train loss: 0.001973, val loss: 0.005323, val mae: 0.050281, val r2: 0.866942\n",
      "epoch: 292, train loss: 0.001955, val loss: 0.005357, val mae: 0.050160, val r2: 0.865756\n",
      "epoch: 293, train loss: 0.001959, val loss: 0.005203, val mae: 0.049615, val r2: 0.869373\n",
      "epoch: 294, train loss: 0.001963, val loss: 0.005130, val mae: 0.049358, val r2: 0.871177\n",
      "epoch: 295, train loss: 0.001938, val loss: 0.005081, val mae: 0.049307, val r2: 0.872462\n",
      "epoch: 296, train loss: 0.001921, val loss: 0.005078, val mae: 0.049410, val r2: 0.872434\n",
      "epoch: 297, train loss: 0.001911, val loss: 0.005130, val mae: 0.049803, val r2: 0.871134\n",
      "epoch: 298, train loss: 0.001911, val loss: 0.005079, val mae: 0.049590, val r2: 0.872351\n",
      "epoch: 299, train loss: 0.001909, val loss: 0.005076, val mae: 0.049570, val r2: 0.872551\n",
      "epoch: 300, train loss: 0.001901, val loss: 0.005060, val mae: 0.049261, val r2: 0.873104\n",
      "epoch: 301, train loss: 0.001893, val loss: 0.005073, val mae: 0.049216, val r2: 0.872810\n",
      "epoch: 302, train loss: 0.001900, val loss: 0.005093, val mae: 0.049256, val r2: 0.872187\n",
      "epoch: 303, train loss: 0.001899, val loss: 0.005081, val mae: 0.049312, val r2: 0.872373\n",
      "epoch: 304, train loss: 0.001884, val loss: 0.005128, val mae: 0.049655, val r2: 0.871112\n",
      "epoch: 305, train loss: 0.001879, val loss: 0.005162, val mae: 0.049930, val r2: 0.870185\n",
      "epoch: 306, train loss: 0.001884, val loss: 0.005200, val mae: 0.050197, val r2: 0.869235\n",
      "epoch: 307, train loss: 0.001884, val loss: 0.005192, val mae: 0.050334, val r2: 0.869435\n",
      "epoch: 308, train loss: 0.001887, val loss: 0.005169, val mae: 0.050232, val r2: 0.869971\n",
      "epoch: 309, train loss: 0.001898, val loss: 0.005071, val mae: 0.049630, val r2: 0.872652\n",
      "epoch: 310, train loss: 0.001886, val loss: 0.005060, val mae: 0.049274, val r2: 0.873100\n",
      "epoch: 311, train loss: 0.001872, val loss: 0.005169, val mae: 0.049510, val r2: 0.870462\n",
      "epoch: 312, train loss: 0.001866, val loss: 0.005161, val mae: 0.049529, val r2: 0.870467\n",
      "epoch: 313, train loss: 0.001863, val loss: 0.005168, val mae: 0.049633, val r2: 0.870177\n",
      "epoch: 314, train loss: 0.001854, val loss: 0.005156, val mae: 0.049769, val r2: 0.870481\n",
      "epoch: 315, train loss: 0.001844, val loss: 0.005201, val mae: 0.050123, val r2: 0.869328\n",
      "epoch: 316, train loss: 0.001843, val loss: 0.005245, val mae: 0.050398, val r2: 0.868246\n",
      "epoch: 317, train loss: 0.001848, val loss: 0.005322, val mae: 0.051021, val r2: 0.866066\n",
      "epoch: 318, train loss: 0.001858, val loss: 0.005329, val mae: 0.050988, val r2: 0.865781\n",
      "epoch: 319, train loss: 0.001867, val loss: 0.005189, val mae: 0.050196, val r2: 0.869428\n",
      "epoch: 320, train loss: 0.001855, val loss: 0.005129, val mae: 0.049773, val r2: 0.871068\n",
      "epoch: 321, train loss: 0.001848, val loss: 0.005108, val mae: 0.049541, val r2: 0.871730\n",
      "epoch: 322, train loss: 0.001857, val loss: 0.005164, val mae: 0.049795, val r2: 0.870195\n",
      "epoch: 323, train loss: 0.001868, val loss: 0.005347, val mae: 0.050850, val r2: 0.865385\n",
      "epoch: 324, train loss: 0.001859, val loss: 0.005627, val mae: 0.052572, val r2: 0.858244\n",
      "epoch: 325, train loss: 0.001837, val loss: 0.005973, val mae: 0.054352, val r2: 0.849239\n",
      "epoch: 326, train loss: 0.001827, val loss: 0.005971, val mae: 0.054368, val r2: 0.849278\n",
      "epoch: 327, train loss: 0.001835, val loss: 0.005835, val mae: 0.053770, val r2: 0.852569\n",
      "epoch: 328, train loss: 0.001841, val loss: 0.005467, val mae: 0.051935, val r2: 0.861885\n",
      "epoch: 329, train loss: 0.001817, val loss: 0.005232, val mae: 0.050459, val r2: 0.868181\n",
      "epoch: 330, train loss: 0.001803, val loss: 0.005253, val mae: 0.050423, val r2: 0.867821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 331, train loss: 0.001804, val loss: 0.005272, val mae: 0.050430, val r2: 0.867315\n",
      "epoch: 332, train loss: 0.001797, val loss: 0.005414, val mae: 0.051229, val r2: 0.863793\n",
      "epoch: 333, train loss: 0.001792, val loss: 0.005401, val mae: 0.051190, val r2: 0.864160\n",
      "epoch: 334, train loss: 0.001792, val loss: 0.005411, val mae: 0.051289, val r2: 0.863924\n",
      "epoch: 335, train loss: 0.001796, val loss: 0.005356, val mae: 0.051034, val r2: 0.865246\n",
      "epoch: 336, train loss: 0.001792, val loss: 0.005308, val mae: 0.050702, val r2: 0.866558\n",
      "epoch: 337, train loss: 0.001784, val loss: 0.005210, val mae: 0.050055, val r2: 0.869083\n",
      "epoch: 338, train loss: 0.001786, val loss: 0.005159, val mae: 0.049684, val r2: 0.870268\n",
      "epoch: 339, train loss: 0.001777, val loss: 0.005157, val mae: 0.049516, val r2: 0.870337\n",
      "epoch: 340, train loss: 0.001783, val loss: 0.005258, val mae: 0.050068, val r2: 0.867882\n",
      "epoch: 341, train loss: 0.001782, val loss: 0.005393, val mae: 0.050723, val r2: 0.864610\n",
      "epoch: 342, train loss: 0.001777, val loss: 0.005505, val mae: 0.051315, val r2: 0.861859\n",
      "epoch: 343, train loss: 0.001775, val loss: 0.005618, val mae: 0.051953, val r2: 0.859150\n",
      "epoch: 344, train loss: 0.001776, val loss: 0.005898, val mae: 0.053318, val r2: 0.852107\n",
      "epoch: 345, train loss: 0.001777, val loss: 0.005998, val mae: 0.053917, val r2: 0.849343\n",
      "epoch: 346, train loss: 0.001783, val loss: 0.006063, val mae: 0.054290, val r2: 0.847469\n",
      "epoch: 347, train loss: 0.001791, val loss: 0.006226, val mae: 0.055124, val r2: 0.843191\n",
      "epoch: 348, train loss: 0.001801, val loss: 0.006407, val mae: 0.056199, val r2: 0.838343\n",
      "epoch: 349, train loss: 0.001813, val loss: 0.006785, val mae: 0.057977, val r2: 0.828264\n"
     ]
    }
   ],
   "source": [
    "# * v2 !!\n",
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 32,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 1,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 4, \n",
    "                      'epoches': 350,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35d420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.2, \n",
    "                               'layers': [16, 8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 32,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 3,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 4, \n",
    "                      'epoches': 350,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef579c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 24,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 3,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 4, \n",
    "                      'epoches': 350,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 32,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 2,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 4, \n",
    "                      'epoches': 300,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61c767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps!\n",
      "Transformer(\n",
      "  (element_embedding_temporal): Linear(in_features=196, out_features=128, bias=True)\n",
      "  (element_embedding_spatial): Linear(in_features=2695, out_features=1760, bias=True)\n",
      "  (pos_embedding): Embedding(4, 32)\n",
      "  (variable_embedding): Embedding(55, 32)\n",
      "  (temporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatial): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (keys): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (queries): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spatiotemporal): Encoder(\n",
      "    (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x EncoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (keys): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (queries): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatter): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=7700, out_features=2640, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=2640, out_features=330, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch: 0, train loss: 0.018137, val loss: 0.014798, val mae: 0.095990, val r2: 0.624469\n",
      "epoch: 1, train loss: 0.010185, val loss: 0.012141, val mae: 0.083538, val r2: 0.692335\n",
      "epoch: 2, train loss: 0.008769, val loss: 0.011078, val mae: 0.078979, val r2: 0.719399\n",
      "epoch: 3, train loss: 0.008004, val loss: 0.010272, val mae: 0.075515, val r2: 0.739940\n",
      "epoch: 4, train loss: 0.007392, val loss: 0.009679, val mae: 0.073012, val r2: 0.755162\n",
      "epoch: 5, train loss: 0.006938, val loss: 0.009281, val mae: 0.071359, val r2: 0.765385\n",
      "epoch: 6, train loss: 0.006529, val loss: 0.008929, val mae: 0.069837, val r2: 0.774684\n",
      "epoch: 7, train loss: 0.006195, val loss: 0.008593, val mae: 0.068307, val r2: 0.783396\n",
      "epoch: 8, train loss: 0.005886, val loss: 0.008318, val mae: 0.067044, val r2: 0.790568\n",
      "epoch: 9, train loss: 0.005616, val loss: 0.008066, val mae: 0.065829, val r2: 0.797064\n",
      "epoch: 10, train loss: 0.005387, val loss: 0.007886, val mae: 0.064935, val r2: 0.801719\n",
      "epoch: 11, train loss: 0.005183, val loss: 0.007700, val mae: 0.063929, val r2: 0.806559\n",
      "epoch: 12, train loss: 0.004998, val loss: 0.007548, val mae: 0.063086, val r2: 0.810511\n",
      "epoch: 13, train loss: 0.004830, val loss: 0.007415, val mae: 0.062344, val r2: 0.814003\n",
      "epoch: 14, train loss: 0.004703, val loss: 0.007288, val mae: 0.061690, val r2: 0.817298\n",
      "epoch: 15, train loss: 0.004576, val loss: 0.007156, val mae: 0.060980, val r2: 0.820751\n",
      "epoch: 16, train loss: 0.004469, val loss: 0.007063, val mae: 0.060454, val r2: 0.823134\n",
      "epoch: 17, train loss: 0.004372, val loss: 0.006964, val mae: 0.059914, val r2: 0.825692\n",
      "epoch: 18, train loss: 0.004300, val loss: 0.006893, val mae: 0.059504, val r2: 0.827589\n",
      "epoch: 19, train loss: 0.004227, val loss: 0.006820, val mae: 0.059142, val r2: 0.829485\n",
      "epoch: 20, train loss: 0.004185, val loss: 0.006765, val mae: 0.058839, val r2: 0.830931\n",
      "epoch: 21, train loss: 0.004149, val loss: 0.006736, val mae: 0.058719, val r2: 0.831786\n",
      "epoch: 22, train loss: 0.004123, val loss: 0.006616, val mae: 0.058063, val r2: 0.834673\n",
      "epoch: 23, train loss: 0.004112, val loss: 0.006530, val mae: 0.057580, val r2: 0.836842\n",
      "epoch: 24, train loss: 0.004099, val loss: 0.006414, val mae: 0.056947, val r2: 0.839600\n",
      "epoch: 25, train loss: 0.004106, val loss: 0.006267, val mae: 0.056080, val r2: 0.843181\n",
      "epoch: 26, train loss: 0.004109, val loss: 0.006194, val mae: 0.055538, val r2: 0.844812\n",
      "epoch: 27, train loss: 0.004073, val loss: 0.006140, val mae: 0.055161, val r2: 0.846106\n",
      "epoch: 28, train loss: 0.004032, val loss: 0.006100, val mae: 0.054870, val r2: 0.847029\n",
      "epoch: 29, train loss: 0.003959, val loss: 0.006066, val mae: 0.054686, val r2: 0.847941\n",
      "epoch: 30, train loss: 0.003861, val loss: 0.006038, val mae: 0.054487, val r2: 0.848662\n",
      "epoch: 31, train loss: 0.003782, val loss: 0.006001, val mae: 0.054334, val r2: 0.849618\n",
      "epoch: 32, train loss: 0.003718, val loss: 0.005951, val mae: 0.054077, val r2: 0.850931\n",
      "epoch: 33, train loss: 0.003651, val loss: 0.005920, val mae: 0.053917, val r2: 0.851713\n",
      "epoch: 34, train loss: 0.003600, val loss: 0.005897, val mae: 0.053835, val r2: 0.852355\n",
      "epoch: 35, train loss: 0.003563, val loss: 0.005874, val mae: 0.053725, val r2: 0.852922\n",
      "epoch: 36, train loss: 0.003541, val loss: 0.005842, val mae: 0.053533, val r2: 0.853724\n",
      "epoch: 37, train loss: 0.003499, val loss: 0.005821, val mae: 0.053396, val r2: 0.854317\n",
      "epoch: 38, train loss: 0.003472, val loss: 0.005821, val mae: 0.053386, val r2: 0.854354\n",
      "epoch: 39, train loss: 0.003444, val loss: 0.005782, val mae: 0.053235, val r2: 0.855382\n",
      "epoch: 40, train loss: 0.003421, val loss: 0.005771, val mae: 0.053116, val r2: 0.855676\n",
      "epoch: 41, train loss: 0.003396, val loss: 0.005751, val mae: 0.053039, val r2: 0.856187\n",
      "epoch: 42, train loss: 0.003379, val loss: 0.005743, val mae: 0.052937, val r2: 0.856378\n",
      "epoch: 43, train loss: 0.003351, val loss: 0.005705, val mae: 0.052744, val r2: 0.857342\n",
      "epoch: 44, train loss: 0.003336, val loss: 0.005693, val mae: 0.052707, val r2: 0.857717\n",
      "epoch: 45, train loss: 0.003308, val loss: 0.005676, val mae: 0.052666, val r2: 0.858179\n",
      "epoch: 46, train loss: 0.003290, val loss: 0.005670, val mae: 0.052644, val r2: 0.858357\n",
      "epoch: 47, train loss: 0.003268, val loss: 0.005656, val mae: 0.052520, val r2: 0.858738\n",
      "epoch: 48, train loss: 0.003245, val loss: 0.005641, val mae: 0.052474, val r2: 0.859086\n",
      "epoch: 49, train loss: 0.003224, val loss: 0.005642, val mae: 0.052441, val r2: 0.859124\n",
      "epoch: 50, train loss: 0.003205, val loss: 0.005616, val mae: 0.052334, val r2: 0.859759\n",
      "epoch: 51, train loss: 0.003186, val loss: 0.005574, val mae: 0.052072, val r2: 0.860834\n",
      "epoch: 52, train loss: 0.003167, val loss: 0.005569, val mae: 0.052167, val r2: 0.860965\n",
      "epoch: 53, train loss: 0.003149, val loss: 0.005564, val mae: 0.052067, val r2: 0.861077\n",
      "epoch: 54, train loss: 0.003133, val loss: 0.005551, val mae: 0.052095, val r2: 0.861416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55, train loss: 0.003115, val loss: 0.005520, val mae: 0.051866, val r2: 0.862153\n",
      "epoch: 56, train loss: 0.003095, val loss: 0.005548, val mae: 0.052017, val r2: 0.861520\n",
      "epoch: 57, train loss: 0.003086, val loss: 0.005513, val mae: 0.051818, val r2: 0.862389\n",
      "epoch: 58, train loss: 0.003063, val loss: 0.005519, val mae: 0.051910, val r2: 0.862212\n",
      "epoch: 59, train loss: 0.003049, val loss: 0.005500, val mae: 0.051687, val r2: 0.862692\n",
      "epoch: 60, train loss: 0.003038, val loss: 0.005462, val mae: 0.051567, val r2: 0.863652\n",
      "epoch: 61, train loss: 0.003015, val loss: 0.005450, val mae: 0.051530, val r2: 0.863919\n",
      "epoch: 62, train loss: 0.003003, val loss: 0.005465, val mae: 0.051572, val r2: 0.863601\n",
      "epoch: 63, train loss: 0.002990, val loss: 0.005444, val mae: 0.051504, val r2: 0.864088\n",
      "epoch: 64, train loss: 0.002975, val loss: 0.005422, val mae: 0.051389, val r2: 0.864657\n",
      "epoch: 65, train loss: 0.002956, val loss: 0.005405, val mae: 0.051259, val r2: 0.864986\n",
      "epoch: 66, train loss: 0.002943, val loss: 0.005395, val mae: 0.051264, val r2: 0.865244\n",
      "epoch: 67, train loss: 0.002929, val loss: 0.005394, val mae: 0.051237, val r2: 0.865282\n",
      "epoch: 68, train loss: 0.002915, val loss: 0.005420, val mae: 0.051387, val r2: 0.864683\n",
      "epoch: 69, train loss: 0.002903, val loss: 0.005413, val mae: 0.051370, val r2: 0.864842\n",
      "epoch: 70, train loss: 0.002886, val loss: 0.005407, val mae: 0.051291, val r2: 0.865010\n",
      "epoch: 71, train loss: 0.002874, val loss: 0.005380, val mae: 0.051176, val r2: 0.865598\n",
      "epoch: 72, train loss: 0.002861, val loss: 0.005329, val mae: 0.050928, val r2: 0.866784\n",
      "epoch: 73, train loss: 0.002847, val loss: 0.005315, val mae: 0.050753, val r2: 0.867159\n",
      "epoch: 74, train loss: 0.002838, val loss: 0.005316, val mae: 0.050834, val r2: 0.867116\n",
      "epoch: 75, train loss: 0.002828, val loss: 0.005360, val mae: 0.051052, val r2: 0.866076\n",
      "epoch: 76, train loss: 0.002812, val loss: 0.005331, val mae: 0.050922, val r2: 0.866710\n",
      "epoch: 77, train loss: 0.002799, val loss: 0.005319, val mae: 0.050877, val r2: 0.867051\n",
      "epoch: 78, train loss: 0.002788, val loss: 0.005326, val mae: 0.050869, val r2: 0.866796\n",
      "epoch: 79, train loss: 0.002791, val loss: 0.005293, val mae: 0.050619, val r2: 0.867649\n",
      "epoch: 80, train loss: 0.002798, val loss: 0.005273, val mae: 0.050509, val r2: 0.868090\n",
      "epoch: 81, train loss: 0.002796, val loss: 0.005256, val mae: 0.050342, val r2: 0.868438\n",
      "epoch: 82, train loss: 0.002780, val loss: 0.005277, val mae: 0.050604, val r2: 0.868000\n",
      "epoch: 83, train loss: 0.002759, val loss: 0.005288, val mae: 0.050745, val r2: 0.867627\n",
      "epoch: 84, train loss: 0.002746, val loss: 0.005275, val mae: 0.050599, val r2: 0.867903\n",
      "epoch: 85, train loss: 0.002739, val loss: 0.005260, val mae: 0.050549, val r2: 0.868263\n",
      "epoch: 86, train loss: 0.002728, val loss: 0.005265, val mae: 0.050479, val r2: 0.868051\n",
      "epoch: 87, train loss: 0.002731, val loss: 0.005248, val mae: 0.050532, val r2: 0.868501\n",
      "epoch: 88, train loss: 0.002729, val loss: 0.005213, val mae: 0.050340, val r2: 0.869374\n",
      "epoch: 89, train loss: 0.002733, val loss: 0.005205, val mae: 0.050294, val r2: 0.869528\n",
      "epoch: 90, train loss: 0.002740, val loss: 0.005196, val mae: 0.050143, val r2: 0.869720\n",
      "epoch: 91, train loss: 0.002745, val loss: 0.005183, val mae: 0.049987, val r2: 0.870073\n",
      "epoch: 92, train loss: 0.002777, val loss: 0.005336, val mae: 0.050524, val r2: 0.866004\n",
      "epoch: 93, train loss: 0.002796, val loss: 0.005490, val mae: 0.051124, val r2: 0.861959\n",
      "epoch: 94, train loss: 0.002866, val loss: 0.005512, val mae: 0.051202, val r2: 0.861281\n",
      "epoch: 95, train loss: 0.002923, val loss: 0.005455, val mae: 0.051061, val r2: 0.863127\n",
      "epoch: 96, train loss: 0.002944, val loss: 0.005566, val mae: 0.051914, val r2: 0.861104\n",
      "epoch: 97, train loss: 0.002944, val loss: 0.005888, val mae: 0.053840, val r2: 0.853476\n",
      "epoch: 98, train loss: 0.003070, val loss: 0.005566, val mae: 0.052001, val r2: 0.860859\n",
      "epoch: 99, train loss: 0.003283, val loss: 0.005592, val mae: 0.051529, val r2: 0.859253\n",
      "epoch: 100, train loss: 0.003263, val loss: 0.006159, val mae: 0.054180, val r2: 0.845305\n",
      "epoch: 101, train loss: 0.003025, val loss: 0.005495, val mae: 0.051057, val r2: 0.861734\n",
      "epoch: 102, train loss: 0.002919, val loss: 0.005384, val mae: 0.050715, val r2: 0.864296\n",
      "epoch: 103, train loss: 0.003002, val loss: 0.005399, val mae: 0.050901, val r2: 0.864067\n",
      "epoch: 104, train loss: 0.002995, val loss: 0.005407, val mae: 0.050990, val r2: 0.864074\n",
      "epoch: 105, train loss: 0.002932, val loss: 0.005430, val mae: 0.051106, val r2: 0.863606\n",
      "epoch: 106, train loss: 0.002898, val loss: 0.005387, val mae: 0.050903, val r2: 0.864707\n",
      "epoch: 107, train loss: 0.002861, val loss: 0.005377, val mae: 0.050821, val r2: 0.864969\n",
      "epoch: 108, train loss: 0.002836, val loss: 0.005352, val mae: 0.050713, val r2: 0.865638\n",
      "epoch: 109, train loss: 0.002818, val loss: 0.005312, val mae: 0.050525, val r2: 0.866632\n",
      "epoch: 110, train loss: 0.002801, val loss: 0.005329, val mae: 0.050630, val r2: 0.866165\n",
      "epoch: 111, train loss: 0.002785, val loss: 0.005336, val mae: 0.050756, val r2: 0.866088\n",
      "epoch: 112, train loss: 0.002779, val loss: 0.005344, val mae: 0.050784, val r2: 0.865859\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "settings = {}\n",
    "settings['regression_head'] = {'heads': 1, \n",
    "                               'dropout_head': 0.1, \n",
    "                               'layers': [8], \n",
    "                               'add_features': 2,    \n",
    "                               'flatt_factor': 2} \n",
    "\n",
    "settings['data_setting'] = {'features': True, 'D': True, 'hours': True, 'weekday': True}\n",
    "\n",
    "settings['params'] = {'embed_size': 32,\n",
    "                      'heads': 4,\n",
    "                      'num_layers': 3,\n",
    "                      'dropout': 0.1, \n",
    "                      'forward_expansion': 1, \n",
    "                      'lr': 0.00001, \n",
    "                      'batch_size': 128, \n",
    "                      'seq_len': 4, \n",
    "                      'epoches': 250,\n",
    "                      'device': 'cpu',\n",
    "                     }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from sttre.sttre import train_val\n",
    "\n",
    "\n",
    "regression_head = settings['regression_head']\n",
    "data_setting = settings['data_setting']\n",
    "params = settings['params']\n",
    "\n",
    "period = {'train': [datetime.datetime(2020, 10, 1), datetime.datetime(2022, 4, 30)], \n",
    "          'val': [datetime.datetime(2022, 5, 1), datetime.datetime(2022, 7, 31)]}\n",
    "\n",
    "#path_preprocessed = './../data/preprocessed'\n",
    "path_preprocessed = './../_ynyt/data/preprocessed'\n",
    "horizon = 6\n",
    "\n",
    "model = train_val(period=period, path_preprocessed=path_preprocessed,\n",
    "                  regression_head=regression_head, data_setting=data_setting, \n",
    "                  verbose=True, horizon=horizon, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e9b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hard_ml",
   "language": "python",
   "name": "hard_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
